{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.basic_data import DataBunch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID      = 1\n",
    "VERSION      = 1\n",
    "MODEL_STRING = f'mol_transformer_parallel_v{VERSION}_fold{FOLD_ID}'\n",
    "\n",
    "TYPES              = np.array(['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'])\n",
    "TYPES_MAP          = {t: i for i, t in enumerate(TYPES)}\n",
    "SC_EDGE_FEATS      = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "SC_MOL_FEATS       = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', \n",
    "                      #'inv_dist', 'normed_inv_dist', \n",
    "                      'std_bond_length', 'ave_bond_length', #'total_bond_length',  \n",
    "                      #'ave_inv_bond_length', 'total_inv_bond_length', \n",
    "                      'ave_atom_weight'#, 'total_atom_weight'\n",
    "                     ]\n",
    "ATOM_FEATS         = ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', \n",
    "                      'degree_1', 'degree_2', 'degree_3', 'degree_4', 'degree_5', \n",
    "                      'SP', 'SP2', 'SP3', 'hybridization_unspecified', \n",
    "                      'aromatic', 'formal_charge', 'atomic_num',\n",
    "                      'donor', 'acceptor', \n",
    "                      'ave_bond_length', \n",
    "                      #'ave_inv_bond_length',\n",
    "                      'ave_neighbor_weight']\n",
    "EDGE_FEATS         = ['single', 'double', 'triple', 'aromatic', \n",
    "                      'conjugated', 'in_ring',\n",
    "                      'dist', 'normed_dist', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "TARGET_COL         = 'scalar_coupling_constant'\n",
    "CONTRIB_COLS       = ['fc', 'sd', 'pso', 'dso']\n",
    "MAX_N_ATOMS        = 29\n",
    "N_EDGE_FEATURES    = len(EDGE_FEATS)\n",
    "N_SC_EDGE_FEATURES = len(SC_EDGE_FEATS)\n",
    "N_SC_MOL_FEATURES  = len(SC_MOL_FEATS)\n",
    "N_ATOM_FEATURES    = len(ATOM_FEATS)\n",
    "N_TYPES            = len(TYPES)\n",
    "N_MOLS             = 130775\n",
    "SC_MEAN            = 16\n",
    "SC_STD             = 35\n",
    "BATCH_PAD_VAL      = -999\n",
    "\n",
    "SC_FEATS_TO_SCALE   = ['dist', 'dist_min_rad', 'dist_electro_neg_adj', 'num_atoms', 'num_C_atoms', \n",
    "                       'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', 'inv_dist', \n",
    "                       'ave_bond_length', 'std_bond_length', 'total_bond_length',  'ave_inv_bond_length', \n",
    "                       'total_inv_bond_length', 'ave_atom_weight', 'total_atom_weight']\n",
    "ATOM_FEATS_TO_SCALE = ['atomic_num', 'ave_bond_length', 'ave_inv_bond_length', 'ave_neighbor_weight']\n",
    "EDGE_FEATS_TO_SCALE = ['dist', 'inv_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = '../tmp/'\n",
    "CV_IDXS_PATH = PATH\n",
    "GRAPH_PATH = PATH\n",
    "# DATA_PATH = '../input/champs-scalar-coupling/'\n",
    "# PATH = '../input/champs-processed-data-3/'\n",
    "# CV_IDXS_PATH = '../input/champs-cv-8-fold-idxs/'\n",
    "# GRAPH_PATH = '../input/champs-graph-dists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp/: ['atomic_features.csv', 'angle_out_df.csv', 'train_proc_df.csv', 'graph_dist_df.csv', 'mask.csv', 'train_idxs_8_fold_cv.csv', 'edge_mask.csv', 'atom_df.csv', 'pairs_idx.csv', 'edge_df.csv', 'train_idxs_4_fold_cv.csv', 'edge_features.csv', 'dist_df.csv', 'angle_in_df.csv', 'angle_df.csv', 'val_idxs_8_fold_cv.csv', 'val_idxs_4_fold_cv.csv', 'test_proc_df.csv']\n",
      "../data/: ['scalar_coupling_contributions.csv', 'mulliken_charges.csv', 'structures.csv', 'test.csv', 'train.csv', 'magnetic_shielding_tensors.csv', 'dipole_moments.csv', 'sample_submission.csv', 'potential_energy.csv']\n"
     ]
    }
   ],
   "source": [
    "def show_csv_files(path):\n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if f.find('.csv') != -1]\n",
    "    print(f'{path}:', files)\n",
    "show_csv_files(PATH)\n",
    "show_csv_files(DATA_PATH)\n",
    "# show_csv_files(CV_IDXS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/python36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(PATH+'train_proc_df.csv', index_col=0)\n",
    "test_df  = pd.read_csv(PATH+'test_proc_df.csv', index_col=0)\n",
    "atom_df  = pd.read_csv(PATH+'atom_df.csv', index_col=0)\n",
    "edge_df  = pd.read_csv(PATH+'edge_df.csv', index_col=0)\n",
    "angle_in_df  = pd.read_csv(PATH+'angle_in_df.csv', index_col=0)\n",
    "angle_out_df = pd.read_csv(PATH+'angle_out_df.csv', index_col=0)\n",
    "graph_dist_df = pd.read_csv(GRAPH_PATH+'graph_dist_df.csv', index_col=0, dtype=np.int32)\n",
    "\n",
    "structures_df = pd.read_csv(DATA_PATH+'structures.csv')\n",
    "mol_id_map = {m_name: m_id for m_id, m_name in enumerate(sorted(structures_df['molecule_name'].unique()))}\n",
    "structures_df['molecule_id'] = structures_df['molecule_name'].map(mol_id_map)\n",
    "\n",
    "train_mol_ids = pd.read_csv(CV_IDXS_PATH+'train_idxs_8_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "val_mol_ids   = pd.read_csv(CV_IDXS_PATH+'val_idxs_8_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "test_mol_ids  = pd.Series(test_df['molecule_id'].unique())\n",
    "\n",
    "contribs_df = pd.read_csv(DATA_PATH+'scalar_coupling_contributions.csv')\n",
    "train_df = pd.concat((train_df, contribs_df[CONTRIB_COLS]), axis=1)\n",
    "del contribs_df\n",
    "gc.collect()\n",
    "\n",
    "train_df[[TARGET_COL, 'fc']] = (train_df[[TARGET_COL, 'fc']] - SC_MEAN) / SC_STD\n",
    "train_df[CONTRIB_COLS[1:]] = train_df[CONTRIB_COLS[1:]] / SC_STD\n",
    "\n",
    "train_df['num_atoms'] = train_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                  'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "test_df['num_atoms'] = test_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "train_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10\n",
    "test_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>2.593389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914926</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035961</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.333287</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>3.922863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772420</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>-0.098103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.816496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783147</td>\n",
       "      <td>3.922924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772357</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081672</td>\n",
       "      <td>-0.098111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.333347</td>\n",
       "      <td>0.816502</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>3.922945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772340</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>-0.098112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>2.593385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914920</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "0      H      C             1             0   0.000000    0.000000   \n",
       "1      H      H             1             2  -0.333287    0.816483   \n",
       "2      H      H             1             3  -0.333335    0.816498   \n",
       "3      H      H             1             4  -0.333347    0.816502   \n",
       "4      H      C             2             0   0.000000    0.000000   \n",
       "\n",
       "   cos_angle1  diangle      dist  dist_electro_neg_adj    ...     \\\n",
       "0   -0.333335      0.0  1.091953              2.593389    ...      \n",
       "1    0.816482      0.0  1.783120              3.922863    ...      \n",
       "2    0.816496      0.0  1.783147              3.922924    ...      \n",
       "3    0.816500      0.0  1.783157              3.922945    ...      \n",
       "4   -0.333352      0.0  1.091952              2.593385    ...      \n",
       "\n",
       "   std_bond_length  total_bond_length  ave_inv_bond_length  \\\n",
       "0         0.000003           4.367799             0.915793   \n",
       "1         0.000003           4.367799             0.915793   \n",
       "2         0.000003           4.367799             0.915793   \n",
       "3         0.000003           4.367799             0.915793   \n",
       "4         0.000003           4.367799             0.915793   \n",
       "\n",
       "   total_inv_bond_length  ave_atom_weight  total_atom_weight        fc  \\\n",
       "0               3.663173              0.2                1.0  1.914926   \n",
       "1               3.663173              0.2                1.0 -0.772420   \n",
       "2               3.663173              0.2                1.0 -0.772357   \n",
       "3               3.663173              0.2                1.0 -0.772340   \n",
       "4               3.663173              0.2                1.0  1.914920   \n",
       "\n",
       "         sd       pso       dso  \n",
       "0  0.007274  0.035961  0.007772  \n",
       "1  0.010085  0.081668 -0.098103  \n",
       "2  0.010084  0.081672 -0.098111  \n",
       "3  0.010084  0.081673 -0.098112  \n",
       "4  0.007274  0.035960  0.007772  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>type_7</th>\n",
       "      <th>inv_dist</th>\n",
       "      <th>normed_inv_dist</th>\n",
       "      <th>ave_bond_length</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>7.311210</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300908</td>\n",
       "      <td>-2.038452</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "4658147      H      C             2             0       -1.0         1.0   \n",
       "4658148      H      C             2             1        0.0         0.0   \n",
       "4658149      H      H             2             3        0.0         1.0   \n",
       "4658150      H      C             3             0        0.0         0.0   \n",
       "4658151      H      C             3             1       -1.0         1.0   \n",
       "\n",
       "         cos_angle1  diangle      dist  dist_electro_neg_adj  \\\n",
       "4658147        -1.0      0.0  2.261178              5.370298   \n",
       "4658148        -1.0      0.0  1.062099              2.522485   \n",
       "4658149         1.0      0.0  3.323277              7.311210   \n",
       "4658150        -1.0      0.0  1.062099              2.522485   \n",
       "4658151        -1.0      0.0  2.261178              5.370298   \n",
       "\n",
       "               ...          type_7  inv_dist  normed_inv_dist  \\\n",
       "4658147        ...               0  0.442247        -0.815269   \n",
       "4658148        ...               0  0.941532         4.621731   \n",
       "4658149        ...               0  0.300908        -2.038452   \n",
       "4658150        ...               0  0.941532         4.621731   \n",
       "4658151        ...               0  0.442247        -0.815269   \n",
       "\n",
       "         ave_bond_length  std_bond_length  total_bond_length  \\\n",
       "4658147         1.107759         0.064573           3.323277   \n",
       "4658148         1.107759         0.064573           3.323277   \n",
       "4658149         1.107759         0.064573           3.323277   \n",
       "4658150         1.107759         0.064573           3.323277   \n",
       "4658151         1.107759         0.064573           3.323277   \n",
       "\n",
       "         ave_inv_bond_length  total_inv_bond_length  ave_atom_weight  \\\n",
       "4658147             0.905679               2.717037             0.35   \n",
       "4658148             0.905679               2.717037             0.35   \n",
       "4658149             0.905679               2.717037             0.35   \n",
       "4658150             0.905679               2.717037             0.35   \n",
       "4658151             0.905679               2.717037             0.35   \n",
       "\n",
       "         total_atom_weight  \n",
       "4658147                1.4  \n",
       "4658148                1.4  \n",
       "4658149                1.4  \n",
       "4658150                1.4  \n",
       "4658151                1.4  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Molecule Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     6,
     16
    ]
   },
   "outputs": [],
   "source": [
    "def hidden_layer(n_in, n_out, batch_norm, dropout, layer_norm=False, \n",
    "                 act=None):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if act: layers.append(act)\n",
    "    if batch_norm: layers.append(nn.BatchNorm1d(n_out))\n",
    "    if layer_norm: layers.append(nn.LayerNorm(n_out))\n",
    "    if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "    return layers\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    def __init__(self, n_input, n_output=None, layers=[], \n",
    "                 act=nn.ReLU(True), dropout=[], batch_norm=False, \n",
    "                 out_act=None, final_bn=False, layer_norm=False, \n",
    "                 final_ln=False):\n",
    "        super().__init__()\n",
    "        sizes = [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        layers_ = []\n",
    "        for i, (n_in, n_out, dr) in enumerate(zip(sizes[:-1], \n",
    "                                                  sizes[1:], dropout)):\n",
    "            act_ = act if i < len(layers) else out_act\n",
    "            batch_norm_ = batch_norm if i < len(layers) else final_bn\n",
    "            layer_norm_ = layer_norm if i < len(layers) else final_ln\n",
    "            layers_ += hidden_layer(\n",
    "                n_in, n_out, batch_norm_, dr, layer_norm_, act_)      \n",
    "        self.layers = nn.Sequential(*layers_)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "def scatter_sum(src, idx, num=None, dim=0, out=None):\n",
    "    if not num: num = idx.max().item() + 1\n",
    "    sz, expanded_idx_sz = src.size(), src.size()\n",
    "    sz = sz[:dim] + torch.Size((num,)) + sz[(dim+1):]\n",
    "    expanded_idx = idx.unsqueeze(-1).expand(expanded_idx_sz)\n",
    "    if out is None: \n",
    "        out = torch.zeros(sz, dtype=src.dtype, device=src.device)\n",
    "    return out.scatter_add(dim, expanded_idx, src)\n",
    "\n",
    "def scatter_mean(src, idx, num=None, dim=0, out=None):\n",
    "    return (scatter_sum(src, idx, num, dim, out) \n",
    "            / scatter_sum(torch.ones_like(src), idx, num, \n",
    "                          dim).clamp(1.0))\n",
    "\n",
    "def softmax(x, idx, num=None, dim=0):\n",
    "    x = x.exp()\n",
    "    x = x / (scatter_sum(x, idx, num, dim)[idx] + 1e-16)\n",
    "    return x\n",
    "    \n",
    "def gather_nodes(x, idx, sz_last_dim):\n",
    "    idx = idx.unsqueeze(-1).expand(-1,-1,sz_last_dim)\n",
    "    return x.gather(1, idx)\n",
    "\n",
    "class ENNMessage(nn.Module):\n",
    "    PAD_VAL = -999\n",
    "    \n",
    "    def __init__(self, n_h, n_e, kernel_sz, enn_args={}, \n",
    "                 ann_args=None):\n",
    "        super().__init__()\n",
    "        assert kernel_sz <= n_h\n",
    "        self.n_h, self.kernel_sz = n_h, kernel_sz\n",
    "        self.enn = FullyConnectedNet(n_e, n_h*kernel_sz, **enn_args)\n",
    "        if ann_args: \n",
    "            self.ann = FullyConnectedNet(1, n_h, **ann_args)\n",
    "        else: self.ann = None\n",
    "    \n",
    "    def forward(self, h, edges, pairs_idx, angles=None, \n",
    "                angles_idx=None, t=0): \n",
    "        if t==0: \n",
    "            self.set_a_mat(edges)\n",
    "            if self.ann: self.set_att(angles)\n",
    "            self.pairs_idx = torch.cat(\n",
    "                [pairs_idx, pairs_idx[:,:,[1, 0]]], dim=1)\n",
    "        \n",
    "        return self.add_message(torch.zeros_like(h), h, angles_idx)\n",
    "    \n",
    "    def set_a_mat(self, edges):\n",
    "        n_edges = edges.size(1)\n",
    "        a_vect = self.enn(edges) \n",
    "        a_vect = a_vect / (self.kernel_sz ** .5) # rescale\n",
    "        mask = edges[:,:,0,None].expand(a_vect.size())==self.PAD_VAL\n",
    "        a_vect = a_vect.masked_fill(mask, 0.0)\n",
    "        self.a_mat = a_vect.view(-1, n_edges, self.n_h, self.kernel_sz)\n",
    "        self.a_mat = torch.cat([self.a_mat, self.a_mat], dim=1)\n",
    "    \n",
    "    def set_att(self, angles):\n",
    "        angles = angles.unsqueeze(-1)\n",
    "        self.att = self.ann(angles)\n",
    "        mask = angles.expand(self.att.size())==self.PAD_VAL\n",
    "        self.att = self.att.masked_fill(mask, 0.0)\n",
    "    \n",
    "    def add_message(self, m, h, angles_idx=None):\n",
    "        # select the 'h_{j}' feeding into the 'm_{i}'\n",
    "        h_in = gather_nodes(h, self.pairs_idx[:,:,1], self.n_h)\n",
    "        \n",
    "        # do the matrix multiplication 'A_{ij}h_{j}'\n",
    "        if self.kernel_sz==self.n_h:\n",
    "            ah = (h_in.unsqueeze(-2) @ self.a_mat).squeeze(-2)\n",
    "        else:\n",
    "            h_padded = F.pad(h_in, self.n_pad)\n",
    "            h_unfolded = h_padded.unfold(-1, self.kernel_sz, 1)\n",
    "            ah = (h_unfolded * self.a_mat).sum(-1)\n",
    "        \n",
    "        # apply atttention\n",
    "        if self.ann:\n",
    "            n_pairs = self.pairs_idx.size(1)\n",
    "            ave_att = scatter_mean(\n",
    "                self.att, angles_idx, num=n_pairs, dim=1, \n",
    "                out=torch.ones_like(ah)\n",
    "            )\n",
    "            ah = ave_att * ah\n",
    "        \n",
    "        # Sum up all 'A_{ij}h_{j}' per node 'i'\n",
    "        idx_0 = self.pairs_idx[:,:,0,None].expand(-1,-1,self.n_h)\n",
    "        return m.scatter_add(1, idx_0, ah)\n",
    "    \n",
    "    @property\n",
    "    def n_pad(self):\n",
    "        k = self.kernel_sz\n",
    "        return (k // 2, k // 2 - int(k % 2 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadedDistAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__()\n",
    "        self.d_model, self.d_k, self.h = d_model, d_model // h, h\n",
    "        self.attn = None\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 2)\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else None\n",
    "        \n",
    "    def forward(self, dists, x, mask):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.linears[0](x).view(batch_size, -1, self.h, self.d_k)\n",
    "        x, self.attn = self.apply_attn(dists, x, mask)\n",
    "        x = x.view(batch_size, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "    def apply_attn(self, dists, x, mask):\n",
    "        attn = self.create_raw_attn(dists, mask)\n",
    "        attn = attn.transpose(-2,-1).transpose(1, 2)\n",
    "        if self.dropout: attn = self.dropout(attn)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        x = torch.matmul(attn, x)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x, attn\n",
    "    \n",
    "    def create_raw_attn(self, dists, mask):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadedGraphDistAttention(MultiHeadedDistAttention):\n",
    "    def __init__(self, h, d_model, dropout=0.0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__(h, d_model, dropout)\n",
    "        self.embedding = nn.Embedding(11, h)\n",
    "    \n",
    "    def create_raw_attn(self, dists, mask):\n",
    "        emb_dists = self.embedding(dists)\n",
    "        mask = mask.unsqueeze(-1).expand(emb_dists.size())\n",
    "        emb_dists = emb_dists.masked_fill(mask==0, -1e9)\n",
    "        return F.softmax(emb_dists, dim=-2).masked_fill(mask==0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadedEuclDistAttention(MultiHeadedDistAttention):\n",
    "    def __init__(self, h, d_model, dropout=0.0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__(h, d_model, dropout)\n",
    "        self.log_prec = nn.Parameter(torch.Tensor(1, 1, 1, h))\n",
    "        self.locs = nn.Parameter(torch.Tensor(1, 1, 1, h))\n",
    "        nn.init.normal_(self.log_prec, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.locs, mean=0.0, std=1.0)\n",
    "    \n",
    "    def create_raw_attn(self, dists, mask):\n",
    "        dists = dists.unsqueeze(-1).expand(-1, -1, -1, self.h)\n",
    "        z = torch.exp(self.log_prec) * (dists - self.locs)\n",
    "        pdf = torch.exp(-0.5 * z ** 2)\n",
    "        return pdf / pdf.sum(dim=-2, keepdim=True).clamp(1e-9)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = (torch.matmul(query, key.transpose(-2, -1)) \n",
    "              / math.sqrt(d_k))\n",
    "    if mask is not None: scores = scores.masked_fill(mask==0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1).masked_fill(mask==0, 0)\n",
    "    if dropout is not None: p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__()\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_model, self.d_k, self.h = d_model, d_model // h, h\n",
    "        self.attn = None\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else None\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        # Same mask applied to all h heads.\n",
    "        mask = mask.unsqueeze(1)\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [\n",
    "            l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        x = x.view(batch_size, -1, self.d_model)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AttendingLayer(nn.Module):\n",
    "    def __init__(self, size, eucl_dist_attn, graph_dist_attn, \n",
    "                 self_attn, ff, dropout):\n",
    "        super().__init__()\n",
    "        self.eucl_dist_attn = eucl_dist_attn\n",
    "        self.graph_dist_attn = graph_dist_attn\n",
    "        self.self_attn = self_attn\n",
    "        self.ff = ff\n",
    "        self.subconns = clones(SublayerConnection(size, dropout), 4)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, eucl_dists, graph_dists, mask):\n",
    "        eucl_dist_sub = lambda x: self.eucl_dist_attn(eucl_dists, x, mask)\n",
    "        x = self.subconns[0](x, eucl_dist_sub)\n",
    "        graph_dist_sub = lambda x: self.graph_dist_attn(graph_dists, x, mask)\n",
    "        x = self.subconns[1](x, graph_dist_sub)\n",
    "        self_sub = lambda x: self.self_attn(x, x, x, mask)\n",
    "        x = self.subconns[2](x, self_sub)\n",
    "        return self.subconns[3](x, self.ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingLayer(nn.Module):\n",
    "    def __init__(self, size, bond_mess, sc_mess, dropout, N):\n",
    "        super().__init__()\n",
    "        self.bond_mess = bond_mess\n",
    "        self.sc_mess = sc_mess\n",
    "        self.linears = clones(nn.Linear(size, size), 2*N)\n",
    "        self.subconns = clones(SublayerConnection(size, dropout), 2*N)\n",
    "\n",
    "    def forward(self, x, bond_x, sc_x, angles, mask, bond_idx, \n",
    "                sc_idx, angles_idx, t=0):\n",
    "        bond_sub = lambda x: self.linears[2*t](\n",
    "            self.bond_mess(x, bond_x, bond_idx, angles, angles_idx, t))\n",
    "        x = self.subconns[2*t](x, bond_sub)\n",
    "        sc_sub = lambda x: self.linears[(2*t)+1](\n",
    "            self.sc_mess(x, sc_x, sc_idx, t=t))\n",
    "        return self.subconns[(2*t)+1](x, sc_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, mess_pass_layer, attn_layer, N):\n",
    "        super().__init__()\n",
    "        self.mess_pass_layer = mess_pass_layer\n",
    "        self.attn_layers = clones(attn_layer, N)\n",
    "        self.norm = nn.LayerNorm(attn_layer.size)\n",
    "        \n",
    "    def forward(self, x, bond_x, sc_x, eucl_dists, graph_dists, \n",
    "                angles, mask, bond_idx, sc_idx, angles_idx):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for t, attn_layer in enumerate(self.attn_layers):\n",
    "            x = self.mess_pass_layer(\n",
    "                x, bond_x, sc_x, angles, mask, bond_idx, sc_idx, \n",
    "                angles_idx, t\n",
    "            )\n",
    "            x = attn_layer(x, eucl_dists, graph_dists, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_contrib_head(n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "    layers = hidden_layer(n_in, n_h, False, dropout, layer_norm, act)\n",
    "    layers += hidden_layer(n_h, 1, False, 0.0) # output layer\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ContribsNet(nn.Module):\n",
    "    N_CONTRIBS = 5\n",
    "    CONTIB_SCALES = [1, 250, 45, 35, 500]\n",
    "    \n",
    "    def __init__(self, n_in, n_h, vec_in, act, dropout=0.0, \n",
    "                 layer_norm=True):\n",
    "        super().__init__()\n",
    "        contrib_head = create_contrib_head(\n",
    "            n_in, n_h, act, dropout, layer_norm) \n",
    "        self.blocks = clones(contrib_head, self.N_CONTRIBS)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ys = torch.cat(\n",
    "            [b(x)/s for b,s in zip(self.blocks, self.CONTIB_SCALES)], \n",
    "            dim=-1)\n",
    "        return torch.cat(\n",
    "            [ys[:,:-1], ys.sum(dim=-1, keepdim=True)], dim=-1)\n",
    "    \n",
    "class MyCustomHead(nn.Module):\n",
    "    PAD_VAL = -999\n",
    "    N_TYPES = 8\n",
    "    \n",
    "    def __init__(self, n_input, n_h, n_h_contribs, pre_layers=[], \n",
    "                 post_layers=[], act=nn.ReLU(True), \n",
    "                 dropout=3*[0.0], norm=False):\n",
    "        super().__init__()\n",
    "        true_n_input = n_input + 2 * n_h\n",
    "        fc_pre = hidden_layer(\n",
    "            n_input, n_h, False, dropout[0], norm, act)\n",
    "        self.preproc = nn.Sequential(*fc_pre)\n",
    "        fc_type = hidden_layer(\n",
    "            n_h, n_input, False, dropout[1], norm, act)\n",
    "        self.types_net = clones(nn.Sequential(*fc_type), N_TYPES)\n",
    "        self.contribs_net = ContribsNet(\n",
    "            n_input, n_h_contribs, n_h, act, dropout[2], \n",
    "            layer_norm=norm\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, sc_types):\n",
    "        # reshape inputs for easier processing\n",
    "        x, sc_types = x.view(-1, x.size(-1)), sc_types.view(-1)\n",
    "        mask =  sc_types != self.PAD_VAL\n",
    "        x, sc_types = x[mask], sc_types[mask]\n",
    "        \n",
    "        x_ = self.preproc(x)\n",
    "        x_types = torch.zeros_like(x)\n",
    "        for i in range(self.N_TYPES):\n",
    "            t_idx = sc_types==i\n",
    "            if torch.any(t_idx): \n",
    "                x_types[t_idx] = self.types_net[i](x_[t_idx])\n",
    "        x = x + x_types \n",
    "        return self.contribs_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_x, d_bond, d_sc_pair, d_sc_m, N=6, \n",
    "                 d_model=512, d_ff=2048, d_ff_contrib=128, h=8, \n",
    "                 dropout=0.1, kernel_sz=128, enn_args={}, ann_args={}):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_model = d_model\n",
    "        c = copy.deepcopy\n",
    "        bond_mess = ENNMessage(\n",
    "            d_model, d_bond, kernel_sz, enn_args, ann_args)\n",
    "        sc_mess = ENNMessage(d_model, d_sc_pair, kernel_sz, enn_args)\n",
    "        eucl_dist_attn = MultiHeadedEuclDistAttention(h, d_model)\n",
    "        graph_dist_attn = MultiHeadedGraphDistAttention(h, d_model)\n",
    "        self_attn = MultiHeadedSelfAttention(h, d_model, dropout)\n",
    "        ff = FullyConnectedNet(\n",
    "            d_model, d_model, [d_ff], dropout=[dropout])\n",
    "        \n",
    "        message_passing_layer = MessagePassingLayer(\n",
    "            d_model, bond_mess, sc_mess, dropout, N)\n",
    "        attending_layer = AttendingLayer(\n",
    "            d_model, c(eucl_dist_attn), c(graph_dist_attn), \n",
    "            c(self_attn), c(ff), dropout\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Linear(d_x, d_model)\n",
    "        self.encoder = Encoder(\n",
    "            message_passing_layer, attending_layer, N)\n",
    "        self.write_head = MyCustomHead(\n",
    "            2 * d_model + d_sc_m, d_ff, d_ff_contrib, norm=True)\n",
    "        \n",
    "    def forward(self, x, bond_x, sc_x, sc_m_x, eucl_dists, \n",
    "                graph_dists, angles, mask, bond_idx, sc_idx, \n",
    "                angles_idx, sc_types):\n",
    "        x = self.encoder(\n",
    "            self.projection(x), bond_x, sc_x, eucl_dists, \n",
    "            graph_dists, angles, mask, bond_idx, sc_idx, \n",
    "            angles_idx\n",
    "        )\n",
    "        x = torch.cat(\n",
    "            [gather_nodes(x, sc_idx[:,:,0], self.d_model), \n",
    "             gather_nodes(x, sc_idx[:,:,1], self.d_model), \n",
    "             sc_m_x], \n",
    "            dim=-1\n",
    "        )\n",
    "        return self.write_head(x, sc_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "mol_ids = train_df['molecule_id'].unique()\n",
    "n_obs = len(mol_ids)\n",
    "split = int(n_obs*0.75)\n",
    "mol_ids_ = np.random.choice(mol_ids, size=n_obs, replace=False)\n",
    "train_mol_ids, val_mol_ids = pd.Series(mol_ids_[:split]), pd.Series(mol_ids_[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def scale_features(df, features, train_mol_ids):\n",
    "    idx = df['molecule_id'].isin(train_mol_ids)\n",
    "    return df.loc[idx, features].mean(), df.loc[idx, features].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0,
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "if (any(train_df[SC_FEATS_TO_SCALE].mean().abs()>0.1) \n",
    "    or any((train_df[SC_FEATS_TO_SCALE].std()-1.0).abs()>0.1)):\n",
    "    sc_feat_means, sc_feat_stds = scale_features(train_df, SC_FEATS_TO_SCALE, train_mol_ids)\n",
    "    train_df[SC_FEATS_TO_SCALE] = (train_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "    test_df[SC_FEATS_TO_SCALE] = (test_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "if (any(atom_df[ATOM_FEATS_TO_SCALE].mean().abs()>0.1) \n",
    "    or any((atom_df[ATOM_FEATS_TO_SCALE].std()-1.0).abs()>0.1)):\n",
    "    atom_feat_means, atom_feat_stds = scale_features(atom_df, ATOM_FEATS_TO_SCALE, train_mol_ids)\n",
    "    atom_df[ATOM_FEATS_TO_SCALE] = (atom_df[ATOM_FEATS_TO_SCALE] - atom_feat_means) / atom_feat_stds\n",
    "if (any(edge_df[EDGE_FEATS_TO_SCALE].mean().abs()>0.1) \n",
    "    or any((edge_df[EDGE_FEATS_TO_SCALE].std()-1.0).abs()>0.1)):\n",
    "    edge_feat_means, edge_feat_stds = scale_features(edge_df, EDGE_FEATS_TO_SCALE, train_mol_ids)\n",
    "    edge_df[EDGE_FEATS_TO_SCALE] = (edge_df[EDGE_FEATS_TO_SCALE] - edge_feat_means) / edge_feat_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mol_sc = train_df.groupby('molecule_id')\n",
    "test_gb_mol_sc = test_df.groupby('molecule_id')\n",
    "gb_mol_atom = atom_df.groupby('molecule_id')\n",
    "gb_mol_edge = edge_df.groupby('molecule_id')\n",
    "gb_mol_struct = structures_df.groupby('molecule_id')\n",
    "gb_mol_angle_in = angle_in_df.groupby('molecule_id')\n",
    "gb_mol_angle_out = angle_out_df.groupby('molecule_id')\n",
    "gb_mol_graph_dist = graph_dist_df.groupby('molecule_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pytorch dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0,
     5,
     38
    ]
   },
   "outputs": [],
   "source": [
    "def get_existing_group(gb, i):\n",
    "    try: group_df = gb.get_group(i)\n",
    "    except KeyError: group_df = None\n",
    "    return group_df\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, \n",
    "                 gb_mol_struct, gb_mol_angle_in, gb_mol_angle_out, \n",
    "                 gb_mol_graph_dist):\n",
    "        self.n = len(mol_ids)\n",
    "        self.mol_ids = mol_ids\n",
    "        self.gb_mol_sc = gb_mol_sc\n",
    "        self.gb_mol_atom = gb_mol_atom\n",
    "        self.gb_mol_edge = gb_mol_edge\n",
    "        self.gb_mol_struct = gb_mol_struct\n",
    "        self.gb_mol_angle_in = gb_mol_angle_in\n",
    "        self.gb_mol_angle_out = gb_mol_angle_out\n",
    "        self.gb_mol_graph_dist = gb_mol_graph_dist\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gb_mol_sc.get_group(self.mol_ids[idx]),\n",
    "                self.gb_mol_atom.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_edge.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_struct.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_angle_in.get_group(self.mol_ids[idx]), \n",
    "                get_existing_group(\n",
    "                    self.gb_mol_angle_out, self.mol_ids[idx]),\n",
    "                self.gb_mol_graph_dist.get_group(self.mol_ids[idx]))\n",
    "\n",
    "def np_lst_to_padded_batch(arr_lst, dtype=torch.float, \n",
    "                           pad_val=BATCH_PAD_VAL):\n",
    "    tensor_list = [torch.Tensor(arr).type(dtype) for arr in arr_lst]\n",
    "    batch = nn.utils.rnn.pad_sequence(\n",
    "        tensor_list, batch_first=True, padding_value=pad_val)\n",
    "    return batch.contiguous()\n",
    "\n",
    "def get_dist_matrix(struct_df):\n",
    "    locs = struct_df[['x','y','z']].values\n",
    "    n_atoms = len(locs)\n",
    "    loc_tile = np.tile(locs.T, (n_atoms,1,1))\n",
    "    dist_mat = np.sqrt(((loc_tile - loc_tile.T)**2).sum(axis=1))\n",
    "    return dist_mat\n",
    "                   \n",
    "def collate_parallel_fn(batch, test=False):\n",
    "    batch_size, n_atom_sum, n_pairs_sum = len(batch), 0, 0\n",
    "    x, bond_x, sc_x, sc_m_x = [], [], [], []\n",
    "    eucl_dists, graph_dists = [], []\n",
    "    angles_in, angles_out = [], []\n",
    "    mask, bond_idx, sc_idx = [], [], []\n",
    "    angles_in_idx, angles_out_idx = [], []\n",
    "    sc_types, sc_vals = [], []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        (sc_df, atom_df, edge_df, struct_df, angle_in_df, \n",
    "         angle_out_df, graph_dist_df) = batch[b]\n",
    "        n_atoms, n_pairs = len(atom_df), len(edge_df)\n",
    "        n_pad = MAX_N_ATOMS - n_atoms\n",
    "        eucl_dists_ = get_dist_matrix(struct_df)\n",
    "        eucl_dists_ = np.pad(eucl_dists_, [(0, 0), (0, n_pad)],\n",
    "                             'constant', constant_values=999)\n",
    "        \n",
    "        x.append(atom_df[ATOM_FEATS].values)\n",
    "        bond_x.append(edge_df[EDGE_FEATS].values)\n",
    "        sc_x.append(sc_df[SC_EDGE_FEATS].values)\n",
    "        sc_m_x.append(sc_df[SC_MOL_FEATS].values)\n",
    "        sc_types.append(sc_df['type'].values)\n",
    "        if not test: \n",
    "            sc_vals.append(sc_df[CONTRIB_COLS+[TARGET_COL]].values)\n",
    "        eucl_dists.append(eucl_dists_)\n",
    "        graph_dists.append(graph_dist_df.values[:,:-1])\n",
    "        angles_in.append(angle_in_df['cos_angle'].values)\n",
    "        if angle_out_df is not None: \n",
    "            angles_out.append(angle_out_df['cos_angle'].values)\n",
    "        else:\n",
    "            angles_out.append(np.array([BATCH_PAD_VAL]))\n",
    "        \n",
    "        mask.append(np.pad(np.ones(2 * [n_atoms]), \n",
    "                           [(0, 0), (0, n_pad)] , 'constant'))\n",
    "        bond_idx.append(edge_df[['idx_0', 'idx_1']].values)\n",
    "        sc_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values)\n",
    "        angles_in_idx.append(angle_in_df['p_idx'].values)\n",
    "        if angle_out_df is not None: \n",
    "            angles_out_idx.append(angle_out_df['p_idx'].values)\n",
    "        else:\n",
    "            angles_out_idx.append(np.array([0.]))\n",
    "        \n",
    "        n_atom_sum += n_atoms\n",
    "        n_pairs_sum += n_pairs\n",
    "        \n",
    "    x = np_lst_to_padded_batch(x, pad_val=0.)\n",
    "    bond_x = np_lst_to_padded_batch(bond_x)\n",
    "    max_n_atoms = x.size(1)\n",
    "    max_n_bonds = bond_x.size(1)\n",
    "    angles_out_idx = [a + max_n_bonds for a in angles_out_idx]\n",
    "    \n",
    "    sc_x = np_lst_to_padded_batch(sc_x)\n",
    "    sc_m_x =np_lst_to_padded_batch(sc_m_x)\n",
    "    if not test: sc_vals = np_lst_to_padded_batch(sc_vals)\n",
    "    else: sc_vals = torch.tensor([0] * batch_size)\n",
    "    sc_types = np_lst_to_padded_batch(sc_types, torch.long)\n",
    "    mask = np_lst_to_padded_batch(mask, torch.uint8, 0)\n",
    "    mask = mask[:,:,:max_n_atoms].contiguous()\n",
    "    bond_idx = np_lst_to_padded_batch(bond_idx, torch.long, pad_val=0)\n",
    "    sc_idx = np_lst_to_padded_batch(sc_idx, torch.long, pad_val=0)\n",
    "    angles_in_idx = np_lst_to_padded_batch(angles_in_idx, torch.long, pad_val=0)\n",
    "    angles_out_idx = np_lst_to_padded_batch(angles_out_idx, torch.long, pad_val=0)\n",
    "    angles_idx = torch.cat((angles_in_idx, angles_out_idx), dim=-1).contiguous()\n",
    "    eucl_dists = np_lst_to_padded_batch(eucl_dists, pad_val=999)\n",
    "    eucl_dists = eucl_dists[:,:,:max_n_atoms].contiguous()\n",
    "    graph_dists = np_lst_to_padded_batch(graph_dists, torch.long, pad_val=10)\n",
    "    graph_dists = graph_dists[:,:,:max_n_atoms].contiguous()\n",
    "    angles_in = np_lst_to_padded_batch(angles_in)\n",
    "    angles_out = np_lst_to_padded_batch(angles_out)\n",
    "    angles = torch.cat((angles_in, angles_out), dim=-1).contiguous()\n",
    "    \n",
    "    return (x, bond_x, sc_x, sc_m_x, eucl_dists, graph_dists, \n",
    "            angles, mask, bond_idx, sc_idx, angles_idx, sc_types), sc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MoleculeDataset(\n",
    "    train_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_struct, \n",
    "    gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist\n",
    ")\n",
    "val_ds   = MoleculeDataset(\n",
    "    val_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_struct, \n",
    "    gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist\n",
    ")\n",
    "test_ds  = MoleculeDataset(\n",
    "    test_mol_ids, test_gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_struct, \n",
    "    gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist\n",
    ")\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8)\n",
    "val_dl   = DataLoader(val_ds, batch_size, num_workers=8)\n",
    "test_dl  = DeviceDataLoader.create(\n",
    "    test_ds, batch_size, num_workers=8, \n",
    "    collate_fn=partial(collate_parallel_fn, test=True)\n",
    ")\n",
    "db = DataBunch(train_dl, val_dl, collate_fn=collate_parallel_fn)\n",
    "db.test_dl = test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 25, 21])\n",
      "torch.Size([20, 26, 8])\n",
      "torch.Size([20, 107, 16])\n",
      "torch.Size([20, 107, 25])\n",
      "torch.Size([20, 25, 25])\n",
      "torch.Size([20, 25, 25])\n",
      "torch.Size([20, 111])\n",
      "torch.Size([20, 25, 25])\n",
      "torch.Size([20, 26, 2])\n",
      "torch.Size([20, 107, 2])\n",
      "torch.Size([20, 111])\n",
      "torch.Size([20, 107])\n",
      "torch.Size([20, 107, 5])\n"
     ]
    }
   ],
   "source": [
    "for el in batch[0]: print(el.size())\n",
    "print(batch[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[[ 0.0000,  1.0000,  0.0000,  ...,  0.8485, -2.6219, -0.3054],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.7292, -0.1759, -0.3054],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.8494, -2.6219, -0.3054],\n",
      "         ...,\n",
      "         [ 1.0000,  0.0000,  0.0000,  ...,  0.9138,  0.4355, -0.3054],\n",
      "         [ 1.0000,  0.0000,  0.0000,  ...,  0.9146,  0.4355, -0.3054],\n",
      "         [ 1.0000,  0.0000,  0.0000,  ...,  0.9130,  0.4355, -0.3054]],\n",
      "\n",
      "        [[ 0.0000,  1.0000,  0.0000,  ...,  0.8528, -2.6219, -0.3054],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.7194,  0.9791, -0.3054],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.7475,  0.4355,  3.2746],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  1.0000,  0.0000,  ...,  0.8554, -2.4181, -0.3054],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.6858,  0.4355, -0.3054],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.7980, -1.3989, -0.3054],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.8238,  0.4355,  3.2746],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.8216, -0.1080, -0.3054],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.8076, -0.9233, -0.3054],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.8734, -1.6027,  3.2746],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.7967, -1.1951, -0.3054],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.6950,  0.4355, -0.3054],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  1.0000,  0.0000,  ...,  0.8848, -1.6027, -0.3054],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.7821,  0.4355, -0.3054],\n",
      "         [ 0.0000,  1.0000,  0.0000,  ...,  0.7805,  0.4355, -0.3054],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "bond_x:\n",
      " tensor([[[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.3088e+00,  1.3957e+00],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.7581e-01, -7.4021e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.7524e-01, -7.3965e-01],\n",
      "         ...,\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.8263e-01, -7.4688e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.7274e-01, -7.3721e-01],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.1567e+00,  1.6124e+00],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.7665e-01, -1.1385e+00],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -9.0135e-01, -1.1719e+00],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           9.1705e-01,  9.5923e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.3670e-01, -8.5602e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.7621e-01, -8.9692e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -2.7966e-01, -1.1145e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           4.2719e-01,  7.0705e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -8.1305e-01, -7.2909e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           7.2882e-01,  7.4352e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -1.5434e+00, -1.5698e+00],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.1044e+00,  1.1259e+00],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  1.0000e+00,  ...,  0.0000e+00,\n",
      "          -3.0810e-01, -4.7134e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          -1.0408e+00, -1.5866e+00],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           4.3836e-01,  6.6483e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]]])\n",
      "sc_x:\n",
      " tensor([[[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00, -3.1265e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4379e-01,\n",
      "           7.4808e-01, -7.3544e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           4.0880e-01, -8.8779e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00, -3.3567e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.4184e-01,\n",
      "           7.5166e-01, -6.1882e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           7.5441e-01,  8.4547e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00, -3.0574e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -4.1228e-01,\n",
      "           7.8925e-01,  8.8484e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           3.6299e-01, -8.9500e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00, -5.5235e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.7710e-01,\n",
      "           7.9018e-01, -3.2291e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           8.9617e-01, -1.7733e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.1132e-01,\n",
      "           7.2155e-01, -9.6785e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           9.0261e-01, -2.4871e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           4.4042e-01,  5.5588e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00, -1.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+00,\n",
      "           1.0000e+00,  1.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.0000e+00, -9.9999e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]]])\n",
      "sc_m_x:\n",
      " tensor([[[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.7890e-01,\n",
      "          -8.4229e-01, -1.5555e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.7890e-01,\n",
      "          -8.4229e-01, -1.5555e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.7890e-01,\n",
      "          -8.4229e-01, -1.5555e+00],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0298e+00,\n",
      "          -2.4673e-01,  2.0165e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0298e+00,\n",
      "          -2.4673e-01,  2.0165e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0298e+00,\n",
      "          -2.4673e-01,  2.0165e+00],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.8237e-01,\n",
      "          -6.5743e-02, -6.7170e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.8237e-01,\n",
      "          -6.5743e-02, -6.7170e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.8237e-01,\n",
      "          -6.5743e-02, -6.7170e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.5784e+00,\n",
      "          -1.3030e+00,  1.3162e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.5784e+00,\n",
      "          -1.3030e+00,  1.3162e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.5784e+00,\n",
      "          -1.3030e+00,  1.3162e+00],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.9456e-01,\n",
      "          -7.3607e-03,  5.6706e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.9456e-01,\n",
      "          -7.3607e-03,  5.6706e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.9456e-01,\n",
      "          -7.3607e-03,  5.6706e-02],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.9886e+00,\n",
      "           1.4398e-02,  3.3345e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.9886e+00,\n",
      "           1.4398e-02,  3.3345e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.9886e+00,\n",
      "           1.4398e-02,  3.3345e+00],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02,  ..., -9.9900e+02,\n",
      "          -9.9900e+02, -9.9900e+02]]])\n",
      "eucl_dists:\n",
      " tensor([[[  0.0000,   1.5316,   2.5349,  ...,   5.2862,   4.9089,   6.2198],\n",
      "         [  1.5316,   0.0000,   1.5250,  ...,   4.0916,   4.1054,   5.3233],\n",
      "         [  2.5349,   1.5250,   0.0000,  ...,   4.4501,   4.4985,   5.8813],\n",
      "         ...,\n",
      "         [  5.2862,   4.0916,   4.4501,  ...,   0.0000,   1.7624,   1.7709],\n",
      "         [  4.9089,   4.1054,   4.4985,  ...,   1.7624,   0.0000,   1.7756],\n",
      "         [  6.2198,   5.3233,   5.8813,  ...,   1.7709,   1.7756,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   1.5011,   2.4292,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  1.5011,   0.0000,   1.3365,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  2.4292,   1.3365,   0.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         ...,\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000]],\n",
      "\n",
      "        [[  0.0000,   1.4532,   2.4885,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  1.4532,   0.0000,   1.4606,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  2.4885,   1.4606,   0.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         ...,\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.0000,   1.2139,   2.2728,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  1.2139,   0.0000,   1.3553,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  2.2728,   1.3553,   0.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         ...,\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000]],\n",
      "\n",
      "        [[  0.0000,   1.4156,   2.3730,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  1.4156,   0.0000,   1.4907,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  2.3730,   1.4907,   0.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         ...,\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000]],\n",
      "\n",
      "        [[  0.0000,   1.2082,   2.5658,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  1.2082,   0.0000,   1.3575,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [  2.5658,   1.3575,   0.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         ...,\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "         [999.0000, 999.0000, 999.0000,  ..., 999.0000, 999.0000, 999.0000]]])\n",
      "graph_dists:\n",
      " tensor([[[ 0,  1,  2,  ...,  6,  6,  6],\n",
      "         [ 1,  0,  1,  ...,  5,  5,  5],\n",
      "         [ 2,  1,  0,  ...,  6,  6,  6],\n",
      "         ...,\n",
      "         [ 6,  5,  6,  ...,  0,  2,  2],\n",
      "         [ 6,  5,  6,  ...,  2,  0,  2],\n",
      "         [ 6,  5,  6,  ...,  2,  2,  0]],\n",
      "\n",
      "        [[ 0,  1,  2,  ...,  0,  0,  0],\n",
      "         [ 1,  0,  1,  ...,  0,  0,  0],\n",
      "         [ 2,  1,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10]],\n",
      "\n",
      "        [[ 0,  1,  2,  ...,  0,  0,  0],\n",
      "         [ 1,  0,  1,  ...,  0,  0,  0],\n",
      "         [ 2,  1,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0,  1,  2,  ...,  0,  0,  0],\n",
      "         [ 1,  0,  1,  ...,  0,  0,  0],\n",
      "         [ 2,  1,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10]],\n",
      "\n",
      "        [[ 0,  1,  2,  ...,  0,  0,  0],\n",
      "         [ 1,  0,  1,  ...,  0,  0,  0],\n",
      "         [ 2,  1,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10]],\n",
      "\n",
      "        [[ 0,  1,  2,  ...,  0,  0,  0],\n",
      "         [ 1,  0,  1,  ...,  0,  0,  0],\n",
      "         [ 2,  1,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10],\n",
      "         [10, 10, 10,  ..., 10, 10, 10]]])\n",
      "angles:\n",
      " tensor([[-3.4379e-01, -3.5083e-01, -3.7061e-01,  ..., -9.9900e+02,\n",
      "         -9.9900e+02, -9.9900e+02],\n",
      "        [-3.4184e-01, -3.5016e-01, -3.4253e-01,  ..., -9.9900e+02,\n",
      "         -9.9900e+02, -9.9900e+02],\n",
      "        [-4.1228e-01, -3.2044e-01, -3.3627e-01,  ..., -9.9900e+02,\n",
      "         -9.9900e+02, -9.9900e+02],\n",
      "        ...,\n",
      "        [-3.7710e-01, -5.6378e-01, -5.5235e-01,  ..., -9.9900e+02,\n",
      "         -9.9900e+02, -9.9900e+02],\n",
      "        [-3.1132e-01, -3.2997e-01, -3.2998e-01,  ..., -9.9900e+02,\n",
      "         -9.9900e+02, -9.9900e+02],\n",
      "        [-1.0000e+00, -1.0000e+00, -9.9999e-01,  ..., -9.9900e+02,\n",
      "         -9.9900e+02, -9.9900e+02]])\n",
      "mask:\n",
      " tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n",
      "bond_idx:\n",
      " tensor([[[ 0,  1],\n",
      "         [ 0,  9],\n",
      "         [ 0, 10],\n",
      "         ...,\n",
      "         [ 8, 23],\n",
      "         [ 8, 24],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 0,  1],\n",
      "         [ 0,  9],\n",
      "         [ 0, 10],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 0,  1],\n",
      "         [ 0,  8],\n",
      "         [ 0,  9],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0,  1],\n",
      "         [ 1,  2],\n",
      "         [ 1,  9],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 0,  1],\n",
      "         [ 0,  9],\n",
      "         [ 1,  2],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 0,  1],\n",
      "         [ 0,  9],\n",
      "         [ 1,  2],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]]])\n",
      "sc_idx:\n",
      " tensor([[[ 9,  0],\n",
      "         [ 9,  1],\n",
      "         [ 9,  2],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 9,  0],\n",
      "         [ 9,  1],\n",
      "         [ 9,  2],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 8,  0],\n",
      "         [ 8,  1],\n",
      "         [ 8,  2],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9,  1],\n",
      "         [ 9,  2],\n",
      "         [ 9,  3],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 9,  1],\n",
      "         [ 9,  2],\n",
      "         [ 9, 10],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 9,  0],\n",
      "         [ 9,  1],\n",
      "         [ 9,  2],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]]])\n",
      "angles_idx:\n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 2,  ..., 0, 0, 0],\n",
      "        [0, 2, 2,  ..., 0, 0, 0],\n",
      "        [0, 2, 3,  ..., 0, 0, 0]])\n",
      "sc_types:\n",
      " tensor([[   0,    4,    6,  ..., -999, -999, -999],\n",
      "        [   0,    4,    7,  ..., -999, -999, -999],\n",
      "        [   0,    3,    6,  ..., -999, -999, -999],\n",
      "        ...,\n",
      "        [   0,    3,    6,  ..., -999, -999, -999],\n",
      "        [   4,    6,    5,  ..., -999, -999, -999],\n",
      "        [   0,    4,    6,  ..., -999, -999, -999]])\n",
      "y:\n",
      " tensor([[[ 1.9369e+00,  5.4106e-03,  2.3206e-02,  2.1021e-02,  1.9866e+00],\n",
      "         [-5.5558e-01,  2.5421e-03, -9.5274e-04,  1.8560e-04, -5.5380e-01],\n",
      "         [-3.5432e-01, -9.4523e-04, -7.8983e-04,  7.4180e-04, -3.5532e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 1.9496e+00,  4.4099e-03,  2.5895e-02,  1.9470e-02,  1.9994e+00],\n",
      "         [-6.2753e-01,  3.7553e-03, -2.2197e-03,  1.2266e-03, -6.2477e-01],\n",
      "         [-4.1462e-01,  1.1534e-04, -5.1503e-04, -2.3122e-03, -4.1734e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 2.0733e+00,  3.1757e-03,  1.3314e-02,  2.3095e-02,  2.1129e+00],\n",
      "         [-4.3980e-01,  5.5789e-04,  3.5311e-03, -1.2720e-03, -4.3698e-01],\n",
      "         [-3.2124e-01, -1.3415e-03, -3.1006e-04,  2.4451e-03, -3.2044e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.5560e+00,  8.0351e-03, -1.1715e-02,  3.3568e-02,  2.5859e+00],\n",
      "         [-4.0266e-02,  3.6817e-04, -7.5972e-03, -1.6124e-03, -4.9106e-02],\n",
      "         [-3.3417e-01,  1.6035e-03,  8.1310e-03, -1.4463e-02, -3.3890e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[-4.8575e-01,  8.2911e-04,  1.6379e-02, -1.5271e-02, -4.8382e-01],\n",
      "         [-2.1243e-01,  1.5207e-03,  1.1858e-02, -1.8212e-02, -2.1726e-01],\n",
      "         [-4.1516e-01,  1.4707e-03,  2.3535e-02, -2.3517e-02, -4.1367e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02]],\n",
      "\n",
      "        [[ 5.2971e+00,  2.6078e-02, -4.7567e-02,  1.2071e-02,  5.2877e+00],\n",
      "         [-6.6860e-02,  2.3256e-02,  1.4212e-01, -3.3926e-02,  6.4586e-02],\n",
      "         [-2.9492e-01,  2.2467e-03, -3.5169e-03, -1.4249e-02, -3.1044e-01],\n",
      "         ...,\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02],\n",
      "         [-9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02, -9.9900e+02]]])\n"
     ]
    }
   ],
   "source": [
    "b_dict = dict(x=batch[0][0], \n",
    "              bond_x=batch[0][1], \n",
    "              sc_x=batch[0][2], \n",
    "              sc_m_x=batch[0][3], \n",
    "              eucl_dists=batch[0][4], \n",
    "              graph_dists=batch[0][5], \n",
    "              angles=batch[0][6], \n",
    "              mask=batch[0][7], \n",
    "              bond_idx=batch[0][8], \n",
    "              sc_idx=batch[0][9],\n",
    "              angles_idx=batch[0][10],\n",
    "              sc_types=batch[0][11], \n",
    "              y=batch[1])\n",
    "for k,v in b_dict.items(): print(f'{k}:\\n {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the metric used for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types, epoch):\n",
    "    proc = lambda x: x.cpu().numpy().ravel() \n",
    "    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n",
    "    y_true = SC_MEAN + y_true * SC_STD\n",
    "    y_pred = SC_MEAN + y_pred * SC_STD\n",
    "    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n",
    "    gmlmae = np.log(maes).mean()\n",
    "    # print(f'Epoch: {epoch} - Group Mean Log Mae: {gmlmae}')\n",
    "    return gmlmae\n",
    "\n",
    "class GroupMeanLogMAE(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): \n",
    "        self.learn = learn\n",
    "        \n",
    "    def on_train_begin(self, **kwargs): \n",
    "        self.learn.recorder.add_metric_names(['group_mean_log_mae'])\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs): \n",
    "        self.input, self.output, self.target = [], [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, last_input, \n",
    "                     train, **kwargs):\n",
    "        if not train:\n",
    "            sc_types = last_input[-1].view(-1)\n",
    "            mask = sc_types != BATCH_PAD_VAL\n",
    "            self.input.append(sc_types[mask])\n",
    "            self.output.append(last_output[:,-1])\n",
    "            self.target.append(reshape_targs(last_target)[:,-1])\n",
    "                \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n",
    "        if (len(self.input) > 0) and (len(self.output) > 0):\n",
    "            inputs = torch.cat(self.input)\n",
    "            preds = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            metric = group_mean_log_mae(preds, target, inputs, epoch)\n",
    "            return add_metrics(last_metrics, [metric])\n",
    "\n",
    "def reshape_targs(targs):\n",
    "    targs = targs.view(-1, targs.size(-1))\n",
    "    return targs[targs[:,0]!=BATCH_PAD_VAL]\n",
    "        \n",
    "def contribs_rmse_loss(preds, targs):\n",
    "    \"\"\"\n",
    "    Returns the sum of RMSEs for each sc contribution and total sc value.\n",
    "    \n",
    "    Args:\n",
    "        - preds: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            predictions. Last column is the total scalar coupling value.\n",
    "        - targs: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            true values. Last column is the total scalar coupling value.\n",
    "    \"\"\"\n",
    "    targs = reshape_targs(targs)\n",
    "    return torch.mean((preds - targs) ** 2, dim=0).sqrt().sum()\n",
    "\n",
    "def rmse(preds, targs):\n",
    "    targs = reshape_targs(targs)\n",
    "    return torch.sqrt(F.mse_loss(preds[:,-1], targs[:,-1]))\n",
    "\n",
    "def mae(preds, targs):\n",
    "    targs = reshape_targs(targs)\n",
    "    return torch.abs(preds[:,-1] - targs[:,-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd, d_model = 1e-2, 768\n",
    "enn_args = dict(layers=3*[d_model], dropout=3*[0.0], layer_norm=True)\n",
    "ann_args = dict(layers=1*[d_model], dropout=1*[0.0], layer_norm=True, \n",
    "                out_act=nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "model = Transformer(\n",
    "    N_ATOM_FEATURES, N_EDGE_FEATURES, N_SC_EDGE_FEATURES, \n",
    "    N_SC_MOL_FEATURES, N=6, d_model=d_model, d_ff=d_model*4, \n",
    "    d_ff_contrib=d_model//4, h=8, dropout=0.0, \n",
    "    kernel_sz=128, enn_args=enn_args, ann_args=ann_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (projection): Linear(in_features=21, out_features=768, bias=True)\n",
      "  (encoder): Encoder(\n",
      "    (mess_pass_layer): MessagePassingLayer(\n",
      "      (bond_mess): ENNMessage(\n",
      "        (enn): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=768, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (4): ReLU(inplace)\n",
      "            (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (6): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (7): ReLU(inplace)\n",
      "            (8): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (9): Linear(in_features=768, out_features=98304, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ann): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1, out_features=768, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (4): Tanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (sc_mess): ENNMessage(\n",
      "        (enn): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=768, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (4): ReLU(inplace)\n",
      "            (5): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (6): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (7): ReLU(inplace)\n",
      "            (8): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (9): Linear(in_features=768, out_features=98304, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linears): ModuleList(\n",
      "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (5): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (6): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (7): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (8): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (9): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (10): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (11): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (subconns): ModuleList(\n",
      "        (0): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (1): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (2): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (3): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (4): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (5): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (6): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (7): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (8): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (9): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (10): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (11): SublayerConnection(\n",
      "          (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attn_layers): ModuleList(\n",
      "      (0): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (subconns): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (subconns): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (subconns): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (subconns): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (subconns): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (3): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (subconns): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (write_head): MyCustomHead(\n",
      "    (preproc): Sequential(\n",
      "      (0): Linear(in_features=1561, out_features=3072, bias=True)\n",
      "      (1): ReLU(inplace)\n",
      "      (2): LayerNorm(torch.Size([3072]), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (types_net): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): Linear(in_features=3072, out_features=1561, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1561]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (contribs_net): ContribsNet(\n",
      "      (blocks): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=1561, out_features=192, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([192]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=1, bias=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=1561, out_features=192, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([192]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=1, bias=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=1561, out_features=192, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([192]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=1, bias=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=1561, out_features=192, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([192]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=1, bias=True)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): Linear(in_features=1561, out_features=192, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([192]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2259e-01,  1.2903e-03,  7.1686e-03,  9.2168e-03,  3.4091e-01],\n",
      "        [ 8.3297e-01,  3.3319e-03,  1.8510e-02,  2.3799e-02,  8.8027e-01],\n",
      "        [ 3.6030e-01,  1.4412e-03,  8.0066e-03,  1.0294e-02,  3.8076e-01],\n",
      "        ...,\n",
      "        [ 4.1180e-02,  1.6472e-04,  9.1511e-04,  1.1766e-03,  4.3519e-02],\n",
      "        [ 1.3528e-01,  5.4112e-04,  3.0062e-03,  3.8652e-03,  1.4296e-01],\n",
      "        [-3.2128e-01, -1.2851e-03, -7.1395e-03, -9.1794e-03, -3.3953e-01]],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([1144, 5])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model(*batch[0]))\n",
    "print(model(*batch[0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(LearnerCallback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, learn:Learner, clip:float = 0., \n",
    "                 start_it:int = 100):\n",
    "        super().__init__(learn)\n",
    "        self.clip, self.start_it = clip, start_it\n",
    "\n",
    "    def on_backward_end(self, iteration, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip and (iteration > self.start_it): \n",
    "            nn.utils.clip_grad_norm_(\n",
    "                self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_fns = [partial(GradientClipping, clip=10), \n",
    "                GroupMeanLogMAE]\n",
    "learn = Learner(db, model, metrics=[rmse, mae], \n",
    "                callback_fns=callback_fns, wd=wd, \n",
    "                loss_func=contribs_rmse_loss)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    learn.model = nn.DataParallel(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>group_mean_log_mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='3188', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.03% [1/3188 06:17<334:30:14 2.5637]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-7, end_lr=1.0, num_it=100, stop_div=True)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(\n",
    "    10, max_lr=1e-3, callbacks=[\n",
    "        SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                          monitor='group_mean_log_mae', \n",
    "                          name=MODEL_STRING)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(skip_start=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_contrib_preds = learn.get_preds(DatasetType.Valid)\n",
    "test_contrib_preds = learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = val_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN\n",
    "test_preds = test_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_submit(predictions):\n",
    "    submit = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "    print(len(submit), len(predictions))   \n",
    "    submit['scalar_coupling_constant'] = predictions\n",
    "    submit.to_csv(f'{MODEL_STRING}-submission.csv', index=False)\n",
    "\n",
    "def store_oof(predictions, val_ids):\n",
    "    oof = pd.DataFrame(predictions, columns=['scalar_coupling_constants'])\n",
    "    print(oof.head())\n",
    "    oof.to_csv(f'{MODEL_STRING}-oof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_submit(test_preds)\n",
    "store_oof(val_preds, val_mol_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
