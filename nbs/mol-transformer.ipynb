{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.basic_data import DataBunch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID = 1\n",
    "VERSION = 1\n",
    "\n",
    "TYPES              = np.array(['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'])\n",
    "TYPES_MAP          = {t: i for i, t in enumerate(TYPES)}\n",
    "SC_EDGE_FEATS      = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "SC_MOL_FEATS       = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', \n",
    "                      #'inv_dist', 'normed_inv_dist', \n",
    "                      'std_bond_length', 'ave_bond_length', #'total_bond_length',  \n",
    "                      #'ave_inv_bond_length', 'total_inv_bond_length', \n",
    "                      'ave_atom_weight'#, 'total_atom_weight'\n",
    "                     ]\n",
    "ATOM_FEATS         = ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', \n",
    "                      'degree_1', 'degree_2', 'degree_3', 'degree_4', 'degree_5', \n",
    "                      'SP', 'SP2', 'SP3', 'hybridization_unspecified', \n",
    "                      'aromatic', 'formal_charge', 'atomic_num',\n",
    "                      'donor', 'acceptor', \n",
    "                      'ave_bond_length', \n",
    "                      #'ave_inv_bond_length',\n",
    "                      'ave_neighbor_weight']\n",
    "EDGE_FEATS         = ['single', 'double', 'triple', 'aromatic', \n",
    "                      'conjugated', 'in_ring',\n",
    "                      'dist', 'normed_dist', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "TARGET_COL         = 'scalar_coupling_constant'\n",
    "CONTRIB_COLS       = ['fc', 'sd', 'pso', 'dso']\n",
    "MAX_N_ATOMS        = 29\n",
    "N_EDGE_FEATURES    = len(EDGE_FEATS)\n",
    "N_SC_EDGE_FEATURES = len(SC_EDGE_FEATS)\n",
    "N_SC_MOL_FEATURES  = len(SC_MOL_FEATS)\n",
    "N_ATOM_FEATURES    = len(ATOM_FEATS)\n",
    "N_TYPES            = len(TYPES)\n",
    "N_MOLS             = 130775\n",
    "SC_MEAN            = 16\n",
    "SC_STD             = 35\n",
    "\n",
    "SC_FEATS_TO_SCALE   = ['dist', 'dist_min_rad', 'dist_electro_neg_adj', 'num_atoms', 'num_C_atoms', \n",
    "                       'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', 'inv_dist', \n",
    "                       'ave_bond_length', 'std_bond_length', 'total_bond_length',  'ave_inv_bond_length', \n",
    "                       'total_inv_bond_length', 'ave_atom_weight', 'total_atom_weight']\n",
    "ATOM_FEATS_TO_SCALE = ['atomic_num', 'ave_bond_length', 'ave_inv_bond_length', 'ave_neighbor_weight']\n",
    "EDGE_FEATS_TO_SCALE = ['dist', 'inv_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = '../tmp/'\n",
    "CV_IDXS_PATH = PATH\n",
    "GRAPH_PATH = PATH\n",
    "# DATA_PATH = '../input/champs-scalar-coupling/'\n",
    "# PATH = '../input/champs-processed-data-3/'\n",
    "# CV_IDXS_PATH = '../input/champs-cv-8-fold-idxs/'\n",
    "# GRAPH_PATH = '../input/champs-graph-dists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp/: ['atomic_features.csv', 'angle_out_df.csv', 'train_proc_df.csv', 'graph_dist_df.csv', 'mask.csv', 'train_idxs_8_fold_cv.csv', 'edge_mask.csv', 'atom_df.csv', 'pairs_idx.csv', 'edge_df.csv', 'train_idxs_4_fold_cv.csv', 'edge_features.csv', 'dist_df.csv', 'angle_in_df.csv', 'angle_df.csv', 'val_idxs_8_fold_cv.csv', 'val_idxs_4_fold_cv.csv', 'test_proc_df.csv']\n",
      "../data/: ['scalar_coupling_contributions.csv', 'mulliken_charges.csv', 'structures.csv', 'test.csv', 'train.csv', 'magnetic_shielding_tensors.csv', 'dipole_moments.csv', 'sample_submission.csv', 'potential_energy.csv']\n"
     ]
    }
   ],
   "source": [
    "def show_csv_files(path):\n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if f.find('.csv') != -1]\n",
    "    print(f'{path}:', files)\n",
    "show_csv_files(PATH)\n",
    "show_csv_files(DATA_PATH)\n",
    "# show_csv_files(CV_IDXS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/python36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(PATH+'train_proc_df.csv', index_col=0)\n",
    "test_df  = pd.read_csv(PATH+'test_proc_df.csv', index_col=0)\n",
    "atom_df  = pd.read_csv(PATH+'atom_df.csv', index_col=0)\n",
    "edge_df  = pd.read_csv(PATH+'edge_df.csv', index_col=0)\n",
    "angle_in_df  = pd.read_csv(PATH+'angle_in_df.csv', index_col=0)\n",
    "angle_out_df = pd.read_csv(PATH+'angle_out_df.csv', index_col=0)\n",
    "graph_dist_df = pd.read_csv(GRAPH_PATH+'graph_dist_df.csv', index_col=0, dtype=np.int32)\n",
    "\n",
    "structures_df = pd.read_csv(DATA_PATH+'structures.csv')\n",
    "mol_id_map = {m_name: m_id for m_id, m_name in enumerate(sorted(structures_df['molecule_name'].unique()))}\n",
    "structures_df['molecule_id'] = structures_df['molecule_name'].map(mol_id_map)\n",
    "\n",
    "train_mol_ids = pd.read_csv(CV_IDXS_PATH+'train_idxs_8_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "val_mol_ids   = pd.read_csv(CV_IDXS_PATH+'val_idxs_8_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "test_mol_ids  = pd.Series(test_df['molecule_id'].unique())\n",
    "\n",
    "contribs_df = pd.read_csv(DATA_PATH+'scalar_coupling_contributions.csv')\n",
    "train_df = pd.concat((train_df, contribs_df[CONTRIB_COLS]), axis=1)\n",
    "del contribs_df\n",
    "gc.collect()\n",
    "\n",
    "train_df[[TARGET_COL, 'fc']] = (train_df[[TARGET_COL, 'fc']] - SC_MEAN) / SC_STD\n",
    "train_df[CONTRIB_COLS[1:]] = train_df[CONTRIB_COLS[1:]] / SC_STD\n",
    "\n",
    "train_df['num_atoms'] = train_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                  'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "test_df['num_atoms'] = test_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "train_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10\n",
    "test_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>2.593389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914926</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035961</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.333287</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>3.922863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772420</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>-0.098103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.816496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783147</td>\n",
       "      <td>3.922924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772357</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081672</td>\n",
       "      <td>-0.098111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.333347</td>\n",
       "      <td>0.816502</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>3.922945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772340</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>-0.098112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>2.593385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914920</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "0      H      C             1             0   0.000000    0.000000   \n",
       "1      H      H             1             2  -0.333287    0.816483   \n",
       "2      H      H             1             3  -0.333335    0.816498   \n",
       "3      H      H             1             4  -0.333347    0.816502   \n",
       "4      H      C             2             0   0.000000    0.000000   \n",
       "\n",
       "   cos_angle1  diangle      dist  dist_electro_neg_adj    ...     \\\n",
       "0   -0.333335      0.0  1.091953              2.593389    ...      \n",
       "1    0.816482      0.0  1.783120              3.922863    ...      \n",
       "2    0.816496      0.0  1.783147              3.922924    ...      \n",
       "3    0.816500      0.0  1.783157              3.922945    ...      \n",
       "4   -0.333352      0.0  1.091952              2.593385    ...      \n",
       "\n",
       "   std_bond_length  total_bond_length  ave_inv_bond_length  \\\n",
       "0         0.000003           4.367799             0.915793   \n",
       "1         0.000003           4.367799             0.915793   \n",
       "2         0.000003           4.367799             0.915793   \n",
       "3         0.000003           4.367799             0.915793   \n",
       "4         0.000003           4.367799             0.915793   \n",
       "\n",
       "   total_inv_bond_length  ave_atom_weight  total_atom_weight        fc  \\\n",
       "0               3.663173              0.2                1.0  1.914926   \n",
       "1               3.663173              0.2                1.0 -0.772420   \n",
       "2               3.663173              0.2                1.0 -0.772357   \n",
       "3               3.663173              0.2                1.0 -0.772340   \n",
       "4               3.663173              0.2                1.0  1.914920   \n",
       "\n",
       "         sd       pso       dso  \n",
       "0  0.007274  0.035961  0.007772  \n",
       "1  0.010085  0.081668 -0.098103  \n",
       "2  0.010084  0.081672 -0.098111  \n",
       "3  0.010084  0.081673 -0.098112  \n",
       "4  0.007274  0.035960  0.007772  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>type_7</th>\n",
       "      <th>inv_dist</th>\n",
       "      <th>normed_inv_dist</th>\n",
       "      <th>ave_bond_length</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>7.311210</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300908</td>\n",
       "      <td>-2.038452</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "4658147      H      C             2             0       -1.0         1.0   \n",
       "4658148      H      C             2             1        0.0         0.0   \n",
       "4658149      H      H             2             3        0.0         1.0   \n",
       "4658150      H      C             3             0        0.0         0.0   \n",
       "4658151      H      C             3             1       -1.0         1.0   \n",
       "\n",
       "         cos_angle1  diangle      dist  dist_electro_neg_adj  \\\n",
       "4658147        -1.0      0.0  2.261178              5.370298   \n",
       "4658148        -1.0      0.0  1.062099              2.522485   \n",
       "4658149         1.0      0.0  3.323277              7.311210   \n",
       "4658150        -1.0      0.0  1.062099              2.522485   \n",
       "4658151        -1.0      0.0  2.261178              5.370298   \n",
       "\n",
       "               ...          type_7  inv_dist  normed_inv_dist  \\\n",
       "4658147        ...               0  0.442247        -0.815269   \n",
       "4658148        ...               0  0.941532         4.621731   \n",
       "4658149        ...               0  0.300908        -2.038452   \n",
       "4658150        ...               0  0.941532         4.621731   \n",
       "4658151        ...               0  0.442247        -0.815269   \n",
       "\n",
       "         ave_bond_length  std_bond_length  total_bond_length  \\\n",
       "4658147         1.107759         0.064573           3.323277   \n",
       "4658148         1.107759         0.064573           3.323277   \n",
       "4658149         1.107759         0.064573           3.323277   \n",
       "4658150         1.107759         0.064573           3.323277   \n",
       "4658151         1.107759         0.064573           3.323277   \n",
       "\n",
       "         ave_inv_bond_length  total_inv_bond_length  ave_atom_weight  \\\n",
       "4658147             0.905679               2.717037             0.35   \n",
       "4658148             0.905679               2.717037             0.35   \n",
       "4658149             0.905679               2.717037             0.35   \n",
       "4658150             0.905679               2.717037             0.35   \n",
       "4658151             0.905679               2.717037             0.35   \n",
       "\n",
       "         total_atom_weight  \n",
       "4658147                1.4  \n",
       "4658148                1.4  \n",
       "4658149                1.4  \n",
       "4658150                1.4  \n",
       "4658151                1.4  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Molecule Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     4,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def unstack_batch(x, mask, exp_idx, n_cols):\n",
    "    x = x.index_select(0, exp_idx)\n",
    "    x[mask[:,0]==0] = 0\n",
    "    sz = x.size()\n",
    "    if len(sz) <= 2: return x.view(-1, MAX_N_ATOMS, n_cols)\n",
    "    else: return x.view(-1, MAX_N_ATOMS, n_cols, *sz[2:])\n",
    "\n",
    "def stack_batch(x, mask, n_cols):\n",
    "    return x.view(-1, n_cols)[mask[:,0]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     4,
     13
    ]
   },
   "outputs": [],
   "source": [
    "def bn_init(m):\n",
    "    if isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.uniform_(m.weight)\n",
    "\n",
    "def hidden_layer(n_in, n_out, batch_norm, dropout, layer_norm=False, act=None):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if act: layers.append(act)\n",
    "    if batch_norm: layers.append(nn.BatchNorm1d(n_out))\n",
    "    if layer_norm: layers.append(nn.LayerNorm(n_out))\n",
    "    if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "    return layers\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], \n",
    "                 batch_norm=False, out_act=None, final_bn=False, layer_norm=False, \n",
    "                 final_ln=False):\n",
    "        super().__init__()\n",
    "        sizes = [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        layers_ = []\n",
    "        for i, (n_in, n_out, dr) in enumerate(zip(sizes[:-1], sizes[1:], dropout)):\n",
    "            act_ = act if i < len(layers) else out_act\n",
    "            batch_norm_ = batch_norm if i < len(layers) else final_bn\n",
    "            layer_norm_ = layer_norm if i < len(layers) else final_ln\n",
    "            layers_ += hidden_layer(n_in, n_out, batch_norm_, dr, layer_norm_, act_)      \n",
    "        self.layers = nn.Sequential(*layers_)\n",
    "        self.layers.apply(bn_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.BatchNorm1d(size) #nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        bn_init(self.norm)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     8,
     11,
     16
    ]
   },
   "outputs": [],
   "source": [
    "def scatter_sum(src, idx, num=None, out=None):\n",
    "    if not num: num = idx.max().item() + 1\n",
    "    sz = num, src.size(1)\n",
    "    exp_idx = idx[:,None].repeat(1, sz[1])\n",
    "    if out is None:\n",
    "        out = torch.zeros(sz, dtype=src.dtype, device=src.device)\n",
    "    return out.scatter_add(0, exp_idx, src)\n",
    "\n",
    "def scatter_mean(src, idx, num=None, out=None):\n",
    "    return scatter_sum(src, idx, num, out) / scatter_sum(torch.ones_like(src), idx, num).clamp(1.0)\n",
    "\n",
    "def softmax(x, idx, num=None):\n",
    "    x = x.exp()\n",
    "    x = x / (scatter_sum(x, idx, num=num)[idx] + 1e-16)\n",
    "    return x\n",
    "    \n",
    "class ENNMessage(nn.Module):\n",
    "    def __init__(self, n_h, n_e, stride, enn_args={}, ann_args=None):\n",
    "        super().__init__()\n",
    "        assert stride <= n_h\n",
    "        self.n_h, self.stride = n_h, stride\n",
    "        self.enn = FullyConnectedNet(n_e, n_h * self.stride, **enn_args)\n",
    "        if ann_args: self.ann = FullyConnectedNet(1, n_h, **ann_args)\n",
    "        else: self.ann = None\n",
    "    \n",
    "    def forward(self, h, e, pairs_idx, angles=None, angles_idx=None, t=0):\n",
    "        # compute 'A(e)' and angle attention masks\n",
    "        if t==0: \n",
    "            self.a_mat = self.get_a_mat(e)\n",
    "            if self.ann: self.att = self.ann(angles.view(-1,1))\n",
    "            self.pairs_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n",
    "            \n",
    "        # compute 'm_{i} = sum_{j in N(i)}(A_{ij}h_{j})' for all nodes 'i'\n",
    "        return self.add_message(torch.zeros_like(h), self.a_mat, h, \n",
    "                                self.pairs_idx, angles_idx)\n",
    "    \n",
    "    def get_a_mat(self, e):\n",
    "        a_vect = self.enn(e) / (self.stride ** .5)\n",
    "        a_mat = a_vect.view(-1, self.n_h, self.stride)\n",
    "        return torch.cat([a_mat, a_mat])\n",
    "    \n",
    "    def add_message(self, m, a, h, pairs_idx, angles_idx=None):\n",
    "        # select the 'h_{j}' feeding into the 'm_{i}'\n",
    "        h_in = h.index_select(0, pairs_idx[:,1])\n",
    "        \n",
    "        # do the matrix multiplication 'A_{ij}h_{j}'\n",
    "        if self.stride==self.n_h:\n",
    "            ah = (h_in.unsqueeze(1) @ a).squeeze(1)\n",
    "        else:\n",
    "            h_unfolded = F.pad(h_in, self.n_pad).unfold(1, self.stride, 1)\n",
    "            ah = (h_unfolded * a).sum(-1)\n",
    "        \n",
    "        # apply atttention\n",
    "        if self.ann:\n",
    "            ave_att = scatter_mean(self.att, angles_idx, num=pairs_idx.size(0), \n",
    "                                   out=torch.ones_like(ah))\n",
    "            ah = ave_att * ah\n",
    "        \n",
    "        # Sum up all 'A_{ij}h_{j}' per node 'i'\n",
    "        return m.scatter_add(0, pairs_idx[:,0,None].repeat(1, self.n_h), ah)\n",
    "    \n",
    "    @property\n",
    "    def n_pad(self):\n",
    "        return (self.stride // 2, self.stride // 2 - int(self.stride % 2 == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadedDistAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_model, self.d_k, self.h, self.attn = d_model, d_model // h, h, None\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 2)\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else None\n",
    "        \n",
    "    def forward(self, dists, x, mask, exp_idx):\n",
    "        x = self.linears[0](x).view(-1, self.h, self.d_k)\n",
    "        x, self.attn = self.apply_attn(dists, x, mask, exp_idx)\n",
    "        x = x.view(-1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "    def apply_attn(self, dists, x, mask, exp_idx):\n",
    "        attn = self.create_raw_attn(dists, mask)\n",
    "        attn = unstack_batch(attn, mask, exp_idx, MAX_N_ATOMS)\n",
    "        attn = attn.transpose(-2,-1).transpose(1, 2)\n",
    "        if self.dropout: attn = self.dropout(attn)\n",
    "        \n",
    "        x = unstack_batch(x, mask, exp_idx, self.h).transpose(1, 2)\n",
    "        x = torch.matmul(attn, x)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return stack_batch(x, mask, self.d_model), attn\n",
    "    \n",
    "    def create_raw_attn(self, dists, mask):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadedGraphDistAttention(MultiHeadedDistAttention):\n",
    "    def __init__(self, h, d_model, dropout=0.0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__(h, d_model, dropout)\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.embedding = nn.Embedding(11, h)\n",
    "    \n",
    "    def create_raw_attn(self, dists, mask):\n",
    "        emb_dists = self.embedding(dists)\n",
    "        emb_dists = emb_dists * mask[mask[:,0]==1].float().unsqueeze(-1)\n",
    "        return F.softmax(emb_dists, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadedEuclDistAttention(MultiHeadedDistAttention):\n",
    "    def __init__(self, h, d_model, dropout=0.0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__(h, d_model, dropout)\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.log_prec = nn.Parameter(torch.Tensor(1, 1, h))\n",
    "        self.locs = nn.Parameter(torch.Tensor(1, 1, h))\n",
    "        nn.init.normal_(self.log_prec, mean=0.0, std=0.1)\n",
    "        nn.init.normal_(self.locs, mean=0.0, std=1.0)\n",
    "    \n",
    "    def create_raw_attn(self, dists, mask):\n",
    "        dists = dists.unsqueeze(-1).expand(-1, -1, self.h)\n",
    "        z = torch.exp(self.log_prec) * (dists - self.locs)\n",
    "        pdf = torch.exp(-0.5 * z ** 2)\n",
    "        return pdf / pdf.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None: scores = scores.masked_fill(mask==0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None: p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_model, self.d_k, self.h, self.attn = d_model, d_model // h, h, None\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else None\n",
    "        \n",
    "    def forward(self, x, mask, exp_idx):\n",
    "        # reshape x and mask\n",
    "        x = unstack_batch(x, mask, exp_idx, self.d_model)\n",
    "        mask_ = mask.view(-1, MAX_N_ATOMS, MAX_N_ATOMS)\n",
    "        query, key, value = x, x, x\n",
    "        \n",
    "        # Same mask applied to all h heads.\n",
    "        mask_ = mask_.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask_, dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.d_model)\n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        # return x to original size\n",
    "        return stack_batch(x, mask, self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AttendingLayer(nn.Module):\n",
    "    def __init__(self, size, eucl_dist_attn, graph_dist_attn, self_attn, ff, dropout):\n",
    "        super().__init__()\n",
    "        self.eucl_dist_attn = eucl_dist_attn\n",
    "        self.graph_dist_attn = graph_dist_attn\n",
    "        self.self_attn = self_attn\n",
    "        self.ff = ff\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 4)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, eucl_dists, graph_dists, mask, exp_idx):\n",
    "        d_model = x.size(-1)\n",
    "        x = self.sublayer[0](x, lambda x: self.eucl_dist_attn(eucl_dists, x, mask, exp_idx))\n",
    "        x = self.sublayer[1](x, lambda x: self.graph_dist_attn(graph_dists, x, mask, exp_idx))\n",
    "        x = self.sublayer[2](x, lambda x: self.self_attn(x, mask, exp_idx))\n",
    "        return self.sublayer[3](x, self.ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MessagePassingLayer(nn.Module):\n",
    "    def __init__(self, size, bond_mess, sc_mess, dropout, N):\n",
    "        super().__init__()\n",
    "        self.bond_mess = bond_mess\n",
    "        self.sc_mess = sc_mess\n",
    "        self.linears = clones(nn.Linear(size, size), 2*N)\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2*N)\n",
    "\n",
    "    def forward(self, x, bond_x, sc_x, angles, mask, bond_idx, sc_idx, angles_idx, t=0):\n",
    "        d_model = x.size(-1)\n",
    "        x = self.sublayer[2*t](x, lambda x: self.linears[2*t](self.bond_mess(x, bond_x, bond_idx, angles, angles_idx, t=t)))\n",
    "        return self.sublayer[(2*t)+1](x, lambda x: self.linears[(2*t)+1](self.sc_mess(x, sc_x, sc_idx, t=t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, mess_pass_layer, attn_layer, N):\n",
    "        super().__init__()\n",
    "        self.mess_pass_layer = mess_pass_layer\n",
    "        self.attn_layers = clones(attn_layer, N)\n",
    "        self.norm = nn.BatchNorm1d(attn_layer.size) # nn.LayerNorm(attn_layer.size)\n",
    "        bn_init(self.norm)\n",
    "        \n",
    "    def forward(self, x, bond_x, sc_x, eucl_dists, graph_dists, angles, mask, \n",
    "                bond_idx, sc_idx, angles_idx, exp_idx):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for t, attn_layer in enumerate(self.attn_layers):\n",
    "            x = self.mess_pass_layer(x, bond_x, sc_x, angles, mask, bond_idx, sc_idx, angles_idx, t=t)\n",
    "            x = attn_layer(x, eucl_dists, graph_dists, mask, exp_idx)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_contrib_net(n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "    layers = hidden_layer(n_in, n_h, False, dropout, layer_norm, act)\n",
    "    layers += hidden_layer(n_h, 1, False, 0.0) # output layer\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ContribsNet(nn.Module):\n",
    "    N_CONTRIBS = 5\n",
    "    CONTIB_SCALES = [1, 250, 45, 35, 500]\n",
    "    \n",
    "    def __init__(self, n_in, n_h, vec_in, act, dropout=0.0, layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            create_contrib_net(n_in, n_h, act, dropout, layer_norm) \n",
    "            for _ in range(self.N_CONTRIBS)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ys = torch.cat([b(x) / s for b, s in zip(self.blocks, self.CONTIB_SCALES)], dim=-1)\n",
    "        return torch.cat([ys[:,:-1], ys.sum(dim=-1, keepdim=True)], dim=-1)\n",
    "\n",
    "class MyCustomHead(nn.Module):\n",
    "    N_TYPES = 8\n",
    "    \n",
    "    def __init__(self, n_input, n_h, n_h_contribs, pre_layers=[], post_layers=[], \n",
    "                 act=nn.ReLU(True), dropout=3*[0.0], norm=False):\n",
    "        super().__init__()\n",
    "        true_n_input = n_input + 2 * n_h\n",
    "        self.preproc = nn.Sequential(*hidden_layer(n_input, n_h, False, dropout[0], norm, act))\n",
    "        self.types_net = nn.ModuleList([\n",
    "            nn.Sequential(*hidden_layer(n_h, n_input, False, dropout[1], norm, act))\n",
    "            for _ in range(self.N_TYPES)\n",
    "        ])\n",
    "        self.contribs_net = ContribsNet(n_input, n_h_contribs, n_h, act, dropout[2], layer_norm=norm)\n",
    "        \n",
    "    def forward(self, x, sc_types):\n",
    "        x_ = self.preproc(x)\n",
    "        x_types = torch.zeros_like(x)\n",
    "        for i in range(self.N_TYPES):\n",
    "            if torch.any(sc_types==i): x_types[sc_types==i] = self.types_net[i](x_[sc_types==i])\n",
    "        x = x + x_types \n",
    "        y = self.contribs_net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_x, d_bond, d_sc_pair, d_sc_m, N=6, d_model=512, d_ff=2048, \n",
    "                 d_ff_contrib=128, h=8, dropout=0.1, stride=128, enn_args={}, ann_args={}):\n",
    "        super().__init__()\n",
    "        c = copy.deepcopy\n",
    "        bond_mess = ENNMessage(d_model, d_bond, stride, enn_args, ann_args)\n",
    "        sc_mess = ENNMessage(d_model, d_sc_pair, stride, enn_args)\n",
    "        eucl_dist_attn = MultiHeadedEuclDistAttention(h, d_model)\n",
    "        graph_dist_attn = MultiHeadedGraphDistAttention(h, d_model)\n",
    "        self_attn = MultiHeadedSelfAttention(h, d_model, dropout)\n",
    "        ff = FullyConnectedNet(d_model, d_model, [d_ff], dropout=[dropout])\n",
    "        \n",
    "        message_passing_layer = MessagePassingLayer(d_model, bond_mess, sc_mess, dropout, N)\n",
    "        attending_layer = AttendingLayer(d_model, c(eucl_dist_attn), c(graph_dist_attn), \n",
    "                                         c(self_attn), c(ff), dropout)\n",
    "        \n",
    "        self.projection = nn.Linear(d_x, d_model)\n",
    "        self.encoder = Encoder(message_passing_layer, attending_layer, N)\n",
    "        self.write_head = MyCustomHead(2 * d_model + d_sc_m, d_ff, d_ff_contrib, norm=True)\n",
    "        \n",
    "    def forward(self, x, bond_x, sc_x, sc_m_x, eucl_dists, graph_dists, angles, mask, bond_idx, \n",
    "                sc_idx, angle_idx, exp_idx, sc_types):\n",
    "        x = self.encoder(self.projection(x), bond_x, sc_x, eucl_dists, graph_dists, \n",
    "                         angles, mask, bond_idx, sc_idx, angle_idx, exp_idx)\n",
    "        x = torch.cat([x.index_select(0, sc_idx[:,0]), x.index_select(0, sc_idx[:,1]), sc_m_x], dim=-1)\n",
    "        return self.write_head(x, sc_types)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "mol_ids = train_df['molecule_id'].unique()\n",
    "n_obs = len(mol_ids)\n",
    "split = int(n_obs*0.75)\n",
    "mol_ids_ = np.random.choice(mol_ids, size=n_obs, replace=False)\n",
    "train_mol_ids, val_mol_ids = pd.Series(mol_ids_[:split]), pd.Series(mol_ids_[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def scale_features(df, features, train_mol_ids):\n",
    "    idx = df['molecule_id'].isin(train_mol_ids)\n",
    "    return df.loc[idx, features].mean(), df.loc[idx, features].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0,
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "if any(train_df[SC_FEATS_TO_SCALE].mean().abs()>0.1) or any((train_df[SC_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    sc_feat_means, sc_feat_stds = scale_features(train_df, SC_FEATS_TO_SCALE, train_mol_ids)\n",
    "    train_df[SC_FEATS_TO_SCALE] = (train_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "    test_df[SC_FEATS_TO_SCALE] = (test_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "if any(atom_df[ATOM_FEATS_TO_SCALE].mean().abs()>0.1) or any((atom_df[ATOM_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    atom_feat_means, atom_feat_stds = scale_features(atom_df, ATOM_FEATS_TO_SCALE, train_mol_ids)\n",
    "    atom_df[ATOM_FEATS_TO_SCALE] = (atom_df[ATOM_FEATS_TO_SCALE] - atom_feat_means) / atom_feat_stds\n",
    "if any(edge_df[EDGE_FEATS_TO_SCALE].mean().abs()>0.1) or any((edge_df[EDGE_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    edge_feat_means, edge_feat_stds = scale_features(edge_df, EDGE_FEATS_TO_SCALE, train_mol_ids)\n",
    "    edge_df[EDGE_FEATS_TO_SCALE] = (edge_df[EDGE_FEATS_TO_SCALE] - edge_feat_means) / edge_feat_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mol_sc = train_df.groupby('molecule_id')\n",
    "test_gb_mol_sc = test_df.groupby('molecule_id')\n",
    "gb_mol_atom = atom_df.groupby('molecule_id')\n",
    "gb_mol_edge = edge_df.groupby('molecule_id')\n",
    "gb_mol_struct = structures_df.groupby('molecule_id')\n",
    "gb_mol_angle_in = angle_in_df.groupby('molecule_id')\n",
    "gb_mol_angle_out = angle_out_df.groupby('molecule_id')\n",
    "gb_mol_graph_dist = graph_dist_df.groupby('molecule_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pytorch dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_existing_group(gb, i):\n",
    "    try: group_df = gb.get_group(i)\n",
    "    except KeyError: group_df = None\n",
    "    return group_df\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_struct, \n",
    "                 gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist):\n",
    "        self.n = len(mol_ids)\n",
    "        self.mol_ids = mol_ids\n",
    "        self.gb_mol_sc = gb_mol_sc\n",
    "        self.gb_mol_atom = gb_mol_atom\n",
    "        self.gb_mol_edge = gb_mol_edge\n",
    "        self.gb_mol_struct = gb_mol_struct\n",
    "        self.gb_mol_angle_in = gb_mol_angle_in\n",
    "        self.gb_mol_angle_out = gb_mol_angle_out\n",
    "        self.gb_mol_graph_dist = gb_mol_graph_dist\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gb_mol_sc.get_group(self.mol_ids[idx]),\n",
    "                self.gb_mol_atom.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_edge.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_struct.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_angle_in.get_group(self.mol_ids[idx]), \n",
    "                get_existing_group(self.gb_mol_angle_out, self.mol_ids[idx]),\n",
    "                self.gb_mol_graph_dist.get_group(self.mol_ids[idx]))\n",
    "\n",
    "def np_lst_to_torch(arr_lst, dtype=torch.float):\n",
    "    return torch.from_numpy(np.ascontiguousarray(np.concatenate(arr_lst))).type(dtype)\n",
    "\n",
    "def get_dist_matrix(struct_df):\n",
    "    locs = struct_df[['x','y','z']].values\n",
    "    n_atoms = len(locs)\n",
    "    loc_tile = np.tile(locs.T, (n_atoms,1,1))\n",
    "    dist_mat = np.sqrt(((loc_tile - loc_tile.T)**2).sum(axis=1))\n",
    "    return dist_mat\n",
    "\n",
    "def collate_fn(batch, test=False):\n",
    "    batch_size, n_atom_sum, n_pairs_sum = len(batch), 0, 0\n",
    "    x, bond_x, sc_x, sc_m_x = [], [], [], []\n",
    "    eucl_dists, graph_dists, angles_in, angles_out = [], [], [], []\n",
    "    mask, bond_idx, sc_idx, exp_idx, angles_in_idx, angles_out_idx = [], [], [], [], [], []\n",
    "    sc_types, sc_vals = [], []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        sc_df, atom_df, edge_df, struct_df, angle_in_df, angle_out_df, graph_dist_df = batch[b]\n",
    "        n_atoms, n_pairs = len(atom_df), len(edge_df)\n",
    "        n_pad = MAX_N_ATOMS - n_atoms\n",
    "        assert len(struct_df)==n_atoms\n",
    "        eucl_dists_ = get_dist_matrix(struct_df)\n",
    "        eucl_dists_ = np.pad(eucl_dists_, [(0, 0), (0, n_pad)] , 'constant', constant_values=999)\n",
    "        \n",
    "        x.append(atom_df[ATOM_FEATS].values)\n",
    "        bond_x.append(edge_df[EDGE_FEATS].values)\n",
    "        sc_x.append(sc_df[SC_EDGE_FEATS].values)\n",
    "        sc_m_x.append(sc_df[SC_MOL_FEATS].values)\n",
    "        sc_types.append(sc_df['type'].values)\n",
    "        if not test: sc_vals.append(sc_df[CONTRIB_COLS+[TARGET_COL]].values)\n",
    "        eucl_dists.append(eucl_dists_)\n",
    "        graph_dists.append(graph_dist_df.values[:,:-1])\n",
    "        angles_in.append(angle_in_df['cos_angle'].values)\n",
    "        if angle_out_df is not None: angles_out.append(angle_out_df['cos_angle'].values)\n",
    "        \n",
    "        mask.append(np.pad(np.ones(2 * [n_atoms]), [(0, n_pad), (0, n_pad)] , 'constant'))\n",
    "        exp_idx.append(np.arange(n_atoms) + n_atom_sum)\n",
    "        exp_idx.append(np.array((MAX_N_ATOMS - n_atoms) * [n_atoms + n_atom_sum - 1]))\n",
    "        bond_idx.append(edge_df[['idx_0', 'idx_1']].values + n_atom_sum)\n",
    "        sc_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values + n_atom_sum)\n",
    "        angles_in_idx.append(angle_in_df['p_idx'].values + n_pairs_sum)\n",
    "        if angle_out_df is not None: angles_out_idx.append(angle_out_df['p_idx'].values + n_pairs_sum)\n",
    "        \n",
    "        n_atom_sum += n_atoms\n",
    "        n_pairs_sum += n_pairs\n",
    "        \n",
    "    x, bond_x = np_lst_to_torch(x), np_lst_to_torch(bond_x), \n",
    "    sc_x, sc_m_x = np_lst_to_torch(sc_x), np_lst_to_torch(sc_m_x)\n",
    "    if not test: sc_vals = np_lst_to_torch(sc_vals)\n",
    "    else: sc_vals = torch.tensor([0] * len(sc_types))\n",
    "    sc_types = np_lst_to_torch(sc_types, torch.long)\n",
    "    mask = np_lst_to_torch(mask, torch.uint8)\n",
    "    exp_idx = np_lst_to_torch(exp_idx, torch.long)\n",
    "    bond_idx = np_lst_to_torch(bond_idx, torch.long)\n",
    "    sc_idx = np_lst_to_torch(sc_idx, torch.long)\n",
    "    angles_in_idx = np_lst_to_torch(angles_in_idx, torch.long)\n",
    "    angles_out_idx = np_lst_to_torch(angles_out_idx, torch.long) + n_pairs_sum\n",
    "    angles_idx = torch.cat((angles_in_idx, angles_out_idx))\n",
    "    eucl_dists = np_lst_to_torch(eucl_dists)\n",
    "    graph_dists = np_lst_to_torch(graph_dists, torch.long)\n",
    "    angles = np_lst_to_torch(angles_in + angles_out)\n",
    "    \n",
    "    return (x, bond_x, sc_x, sc_m_x, eucl_dists, graph_dists, angles, mask, \n",
    "            bond_idx, sc_idx, angles_idx, exp_idx, sc_types), sc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MoleculeDataset(train_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_struct, gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist)\n",
    "val_ds   = MoleculeDataset(val_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_struct, gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist)\n",
    "test_ds  = MoleculeDataset(test_mol_ids, test_gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_struct, gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8)\n",
    "val_dl   = DataLoader(val_ds, batch_size, num_workers=8)\n",
    "test_dl  = DeviceDataLoader.create(test_ds, batch_size, num_workers=8, collate_fn=partial(collate_fn, test=True))\n",
    "db = DataBunch(train_dl, val_dl, collate_fn=collate_fn)\n",
    "db.test_dl = test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([375, 21])\n",
      "torch.Size([387, 8])\n",
      "torch.Size([1144, 16])\n",
      "torch.Size([1144, 25])\n",
      "torch.Size([375, 29])\n",
      "torch.Size([375, 29])\n",
      "torch.Size([1454])\n",
      "torch.Size([580, 29])\n",
      "torch.Size([387, 2])\n",
      "torch.Size([1144, 2])\n",
      "torch.Size([1454])\n",
      "torch.Size([580])\n",
      "torch.Size([1144])\n",
      "torch.Size([1144, 5])\n"
     ]
    }
   ],
   "source": [
    "for el in batch[0]: print(el.size())\n",
    "print(batch[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 0.0000,  1.0000,  0.0000,  ...,  0.8485, -2.6219, -0.3054],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ...,  0.7292, -0.1759, -0.3054],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ...,  0.8494, -2.6219, -0.3054],\n",
      "        ...,\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9419,  0.4355, -0.3054],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9272,  0.4355, -0.3054],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9257,  0.4355, -0.3054]])\n",
      "bond_x:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  1.3088,  1.3957],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8758, -0.7402],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8752, -0.7397],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.6314,  0.9587],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.1754,  0.2646],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9483, -1.4458]])\n",
      "sc_x:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3127],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.3438,  0.7481, -0.7354],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.4088, -0.8878],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.9860, -0.9776],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4963,  0.8363,  0.8911],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.4963]])\n",
      "sc_m_x:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.2789, -0.8423, -1.5555],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.2789, -0.8423, -1.5555],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.2789, -0.8423, -1.5555],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -3.9886,  0.0144,  3.3345],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -3.9886,  0.0144,  3.3345],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -3.9886,  0.0144,  3.3345]])\n",
      "eucl_dists:\n",
      " tensor([[  0.0000,   1.5316,   2.5349,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  1.5316,   0.0000,   1.5250,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  2.5349,   1.5250,   0.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        ...,\n",
      "        [  1.0617,   2.2700,   3.6275,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  6.2866,   5.1639,   3.9633,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  6.1979,   5.0861,   3.9053,  ..., 999.0000, 999.0000, 999.0000]])\n",
      "graph_dists:\n",
      " tensor([[0, 1, 2,  ..., 0, 0, 0],\n",
      "        [1, 0, 1,  ..., 0, 0, 0],\n",
      "        [2, 1, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 2, 3,  ..., 0, 0, 0],\n",
      "        [6, 5, 4,  ..., 0, 0, 0],\n",
      "        [6, 5, 4,  ..., 0, 0, 0]])\n",
      "angles:\n",
      " tensor([-0.3438, -0.3508, -0.3706,  ..., -0.6136, -0.4963, -0.3809])\n",
      "mask:\n",
      " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n",
      "bond_idx:\n",
      " tensor([[  0,   1],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  1,   2],\n",
      "        [  1,   3],\n",
      "        [  1,  12],\n",
      "        [  2,  13],\n",
      "        [  2,  14],\n",
      "        [  2,  15],\n",
      "        [  3,   4],\n",
      "        [  4,  16],\n",
      "        [  4,   7],\n",
      "        [  4,   5],\n",
      "        [  5,   6],\n",
      "        [  5,  17],\n",
      "        [  5,  18],\n",
      "        [  6,   7],\n",
      "        [  6,  19],\n",
      "        [  6,  20],\n",
      "        [  7,   8],\n",
      "        [  7,  21],\n",
      "        [  8,  22],\n",
      "        [  8,  23],\n",
      "        [  8,  24],\n",
      "        [ 25,  26],\n",
      "        [ 25,  34],\n",
      "        [ 25,  35],\n",
      "        [ 25,  36],\n",
      "        [ 26,  27],\n",
      "        [ 26,  31],\n",
      "        [ 27,  28],\n",
      "        [ 28,  29],\n",
      "        [ 28,  32],\n",
      "        [ 29,  30],\n",
      "        [ 29,  37],\n",
      "        [ 30,  31],\n",
      "        [ 30,  38],\n",
      "        [ 32,  33],\n",
      "        [ 39,  40],\n",
      "        [ 39,  47],\n",
      "        [ 39,  48],\n",
      "        [ 39,  49],\n",
      "        [ 40,  41],\n",
      "        [ 40,  42],\n",
      "        [ 41,  42],\n",
      "        [ 41,  50],\n",
      "        [ 41,  51],\n",
      "        [ 42,  46],\n",
      "        [ 42,  43],\n",
      "        [ 43,  44],\n",
      "        [ 43,  52],\n",
      "        [ 43,  53],\n",
      "        [ 44,  45],\n",
      "        [ 44,  54],\n",
      "        [ 44,  55],\n",
      "        [ 45,  46],\n",
      "        [ 46,  56],\n",
      "        [ 46,  57],\n",
      "        [ 58,  59],\n",
      "        [ 58,  67],\n",
      "        [ 58,  68],\n",
      "        [ 58,  69],\n",
      "        [ 59,  60],\n",
      "        [ 59,  65],\n",
      "        [ 60,  61],\n",
      "        [ 61,  62],\n",
      "        [ 61,  63],\n",
      "        [ 62,  70],\n",
      "        [ 63,  64],\n",
      "        [ 63,  71],\n",
      "        [ 64,  65],\n",
      "        [ 64,  72],\n",
      "        [ 65,  66],\n",
      "        [ 73,  74],\n",
      "        [ 73,  82],\n",
      "        [ 73,  83],\n",
      "        [ 73,  84],\n",
      "        [ 74,  75],\n",
      "        [ 74,  85],\n",
      "        [ 74,  86],\n",
      "        [ 75,  76],\n",
      "        [ 75,  78],\n",
      "        [ 75,  81],\n",
      "        [ 76,  87],\n",
      "        [ 76,  88],\n",
      "        [ 76,  77],\n",
      "        [ 77,  89],\n",
      "        [ 77,  90],\n",
      "        [ 77,  91],\n",
      "        [ 78,  79],\n",
      "        [ 78,  92],\n",
      "        [ 78,  93],\n",
      "        [ 79,  80],\n",
      "        [ 79,  81],\n",
      "        [ 79,  94],\n",
      "        [ 80,  96],\n",
      "        [ 80,  81],\n",
      "        [ 80,  95],\n",
      "        [ 81,  97],\n",
      "        [ 98,  99],\n",
      "        [ 98, 107],\n",
      "        [ 98, 108],\n",
      "        [ 98, 109],\n",
      "        [ 99, 100],\n",
      "        [ 99, 102],\n",
      "        [ 99, 110],\n",
      "        [100, 101],\n",
      "        [100, 102],\n",
      "        [100, 111],\n",
      "        [101, 112],\n",
      "        [102, 103],\n",
      "        [102, 106],\n",
      "        [103, 104],\n",
      "        [103, 113],\n",
      "        [103, 114],\n",
      "        [104, 105],\n",
      "        [104, 106],\n",
      "        [104, 115],\n",
      "        [105, 106],\n",
      "        [106, 116],\n",
      "        [117, 118],\n",
      "        [117, 126],\n",
      "        [117, 127],\n",
      "        [117, 128],\n",
      "        [118, 119],\n",
      "        [118, 124],\n",
      "        [119, 120],\n",
      "        [119, 129],\n",
      "        [120, 121],\n",
      "        [120, 123],\n",
      "        [121, 122],\n",
      "        [121, 130],\n",
      "        [121, 131],\n",
      "        [122, 132],\n",
      "        [123, 124],\n",
      "        [124, 125],\n",
      "        [133, 134],\n",
      "        [133, 142],\n",
      "        [133, 143],\n",
      "        [133, 144],\n",
      "        [134, 135],\n",
      "        [134, 145],\n",
      "        [134, 146],\n",
      "        [135, 136],\n",
      "        [135, 147],\n",
      "        [135, 148],\n",
      "        [136, 137],\n",
      "        [137, 141],\n",
      "        [137, 140],\n",
      "        [137, 138],\n",
      "        [138, 139],\n",
      "        [138, 149],\n",
      "        [138, 150],\n",
      "        [139, 151],\n",
      "        [139, 152],\n",
      "        [139, 153],\n",
      "        [140, 141],\n",
      "        [140, 154],\n",
      "        [140, 155],\n",
      "        [141, 156],\n",
      "        [141, 157],\n",
      "        [158, 159],\n",
      "        [158, 167],\n",
      "        [158, 168],\n",
      "        [158, 169],\n",
      "        [159, 160],\n",
      "        [159, 161],\n",
      "        [159, 170],\n",
      "        [160, 171],\n",
      "        [160, 161],\n",
      "        [161, 162],\n",
      "        [161, 165],\n",
      "        [162, 163],\n",
      "        [162, 172],\n",
      "        [162, 173],\n",
      "        [163, 164],\n",
      "        [164, 174],\n",
      "        [165, 166],\n",
      "        [165, 175],\n",
      "        [176, 177],\n",
      "        [176, 184],\n",
      "        [176, 185],\n",
      "        [176, 186],\n",
      "        [177, 178],\n",
      "        [177, 187],\n",
      "        [177, 188],\n",
      "        [178, 189],\n",
      "        [178, 182],\n",
      "        [178, 179],\n",
      "        [179, 180],\n",
      "        [179, 190],\n",
      "        [179, 191],\n",
      "        [180, 181],\n",
      "        [181, 192],\n",
      "        [181, 193],\n",
      "        [181, 194],\n",
      "        [182, 183],\n",
      "        [195, 196],\n",
      "        [195, 204],\n",
      "        [195, 205],\n",
      "        [195, 206],\n",
      "        [196, 197],\n",
      "        [196, 203],\n",
      "        [197, 198],\n",
      "        [197, 203],\n",
      "        [197, 207],\n",
      "        [198, 208],\n",
      "        [198, 199],\n",
      "        [198, 200],\n",
      "        [199, 200],\n",
      "        [199, 209],\n",
      "        [200, 201],\n",
      "        [200, 203],\n",
      "        [201, 210],\n",
      "        [201, 202],\n",
      "        [203, 211],\n",
      "        [212, 213],\n",
      "        [212, 221],\n",
      "        [212, 222],\n",
      "        [212, 223],\n",
      "        [213, 214],\n",
      "        [213, 215],\n",
      "        [213, 224],\n",
      "        [214, 225],\n",
      "        [214, 226],\n",
      "        [214, 227],\n",
      "        [215, 228],\n",
      "        [215, 219],\n",
      "        [215, 216],\n",
      "        [216, 217],\n",
      "        [216, 229],\n",
      "        [216, 230],\n",
      "        [217, 218],\n",
      "        [217, 219],\n",
      "        [217, 231],\n",
      "        [218, 219],\n",
      "        [219, 220],\n",
      "        [220, 232],\n",
      "        [220, 233],\n",
      "        [220, 234],\n",
      "        [235, 236],\n",
      "        [235, 244],\n",
      "        [235, 245],\n",
      "        [235, 246],\n",
      "        [236, 237],\n",
      "        [236, 240],\n",
      "        [236, 242],\n",
      "        [237, 248],\n",
      "        [237, 247],\n",
      "        [237, 238],\n",
      "        [238, 239],\n",
      "        [238, 249],\n",
      "        [238, 250],\n",
      "        [239, 240],\n",
      "        [240, 241],\n",
      "        [241, 251],\n",
      "        [242, 243],\n",
      "        [243, 252],\n",
      "        [253, 254],\n",
      "        [253, 262],\n",
      "        [254, 255],\n",
      "        [254, 263],\n",
      "        [255, 256],\n",
      "        [256, 257],\n",
      "        [256, 260],\n",
      "        [256, 261],\n",
      "        [257, 264],\n",
      "        [257, 259],\n",
      "        [257, 258],\n",
      "        [258, 259],\n",
      "        [258, 261],\n",
      "        [258, 265],\n",
      "        [259, 260],\n",
      "        [259, 266],\n",
      "        [260, 261],\n",
      "        [260, 267],\n",
      "        [261, 268],\n",
      "        [269, 270],\n",
      "        [269, 278],\n",
      "        [269, 279],\n",
      "        [269, 280],\n",
      "        [270, 271],\n",
      "        [270, 276],\n",
      "        [270, 281],\n",
      "        [271, 272],\n",
      "        [272, 282],\n",
      "        [272, 283],\n",
      "        [272, 273],\n",
      "        [273, 274],\n",
      "        [274, 275],\n",
      "        [274, 277],\n",
      "        [274, 284],\n",
      "        [275, 276],\n",
      "        [275, 285],\n",
      "        [275, 286],\n",
      "        [276, 277],\n",
      "        [276, 287],\n",
      "        [277, 288],\n",
      "        [277, 289],\n",
      "        [290, 291],\n",
      "        [290, 299],\n",
      "        [291, 292],\n",
      "        [291, 295],\n",
      "        [292, 293],\n",
      "        [292, 300],\n",
      "        [292, 301],\n",
      "        [293, 296],\n",
      "        [293, 302],\n",
      "        [293, 294],\n",
      "        [294, 295],\n",
      "        [294, 303],\n",
      "        [294, 304],\n",
      "        [296, 297],\n",
      "        [296, 298],\n",
      "        [296, 305],\n",
      "        [297, 307],\n",
      "        [297, 298],\n",
      "        [297, 306],\n",
      "        [298, 308],\n",
      "        [309, 310],\n",
      "        [309, 318],\n",
      "        [309, 319],\n",
      "        [309, 320],\n",
      "        [310, 311],\n",
      "        [310, 313],\n",
      "        [310, 321],\n",
      "        [311, 315],\n",
      "        [311, 322],\n",
      "        [311, 312],\n",
      "        [312, 313],\n",
      "        [312, 323],\n",
      "        [313, 314],\n",
      "        [315, 316],\n",
      "        [315, 317],\n",
      "        [315, 324],\n",
      "        [316, 326],\n",
      "        [316, 317],\n",
      "        [316, 325],\n",
      "        [317, 327],\n",
      "        [328, 329],\n",
      "        [329, 330],\n",
      "        [329, 337],\n",
      "        [330, 331],\n",
      "        [330, 338],\n",
      "        [331, 332],\n",
      "        [331, 339],\n",
      "        [331, 340],\n",
      "        [332, 333],\n",
      "        [332, 334],\n",
      "        [334, 335],\n",
      "        [334, 341],\n",
      "        [334, 342],\n",
      "        [335, 336],\n",
      "        [336, 343],\n",
      "        [344, 345],\n",
      "        [344, 353],\n",
      "        [345, 346],\n",
      "        [345, 354],\n",
      "        [345, 355],\n",
      "        [346, 347],\n",
      "        [346, 352],\n",
      "        [347, 348],\n",
      "        [347, 356],\n",
      "        [348, 351],\n",
      "        [348, 349],\n",
      "        [348, 350],\n",
      "        [349, 350],\n",
      "        [349, 357],\n",
      "        [349, 358],\n",
      "        [350, 359],\n",
      "        [350, 360],\n",
      "        [351, 352],\n",
      "        [352, 361],\n",
      "        [352, 362],\n",
      "        [363, 364],\n",
      "        [363, 372],\n",
      "        [364, 365],\n",
      "        [365, 366],\n",
      "        [366, 367],\n",
      "        [367, 368],\n",
      "        [367, 371],\n",
      "        [368, 369],\n",
      "        [368, 373],\n",
      "        [369, 370],\n",
      "        [370, 371],\n",
      "        [371, 374]])\n",
      "sc_idx:\n",
      " tensor([[  9,   0],\n",
      "        [  9,   1],\n",
      "        [  9,   2],\n",
      "        ...,\n",
      "        [374, 368],\n",
      "        [374, 370],\n",
      "        [374, 371]])\n",
      "angles_idx:\n",
      " tensor([  0,   0,   0,  ..., 768, 772, 772])\n",
      "exp_idx:\n",
      " tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  24,  24,  24,\n",
      "         24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,\n",
      "         38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,  38,\n",
      "         38,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,\n",
      "         51,  52,  53,  54,  55,  56,  57,  57,  57,  57,  57,  57,  57,  57,\n",
      "         57,  57,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
      "         69,  70,  71,  72,  72,  72,  72,  72,  72,  72,  72,  72,  72,  72,\n",
      "         72,  72,  72,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
      "         83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,\n",
      "         97,  97,  97,  97,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
      "        107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 116, 116, 116, 116,\n",
      "        116, 116, 116, 116, 116, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
      "        125, 126, 127, 128, 129, 130, 131, 132, 132, 132, 132, 132, 132, 132,\n",
      "        132, 132, 132, 132, 132, 132, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 157, 157, 157, 157, 158, 159, 160, 161, 162, 163,\n",
      "        164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 175, 175,\n",
      "        175, 175, 175, 175, 175, 175, 175, 175, 175, 176, 177, 178, 179, 180,\n",
      "        181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "        194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 195, 196, 197, 198,\n",
      "        199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 211,\n",
      "        211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 211, 212, 213, 214,\n",
      "        215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228,\n",
      "        229, 230, 231, 232, 233, 234, 234, 234, 234, 234, 234, 234, 235, 236,\n",
      "        237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "        251, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 253,\n",
      "        254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267,\n",
      "        268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268,\n",
      "        269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282,\n",
      "        283, 284, 285, 286, 287, 288, 289, 289, 289, 289, 289, 289, 289, 289,\n",
      "        289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
      "        303, 304, 305, 306, 307, 308, 308, 308, 308, 308, 308, 308, 308, 308,\n",
      "        308, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320,\n",
      "        321, 322, 323, 324, 325, 326, 327, 327, 327, 327, 327, 327, 327, 327,\n",
      "        327, 327, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
      "        339, 340, 341, 342, 343, 343, 343, 343, 343, 343, 343, 343, 343, 343,\n",
      "        343, 343, 343, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 362, 362, 362, 362, 362,\n",
      "        362, 362, 362, 362, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371,\n",
      "        372, 373, 374, 374, 374, 374, 374, 374, 374, 374, 374, 374, 374, 374,\n",
      "        374, 374, 374, 374, 374, 374])\n",
      "sc_types:\n",
      " tensor([0, 4, 6,  ..., 6, 3, 0])\n",
      "y:\n",
      " tensor([[ 1.9369e+00,  5.4106e-03,  2.3206e-02,  2.1021e-02,  1.9866e+00],\n",
      "        [-5.5558e-01,  2.5421e-03, -9.5274e-04,  1.8560e-04, -5.5380e-01],\n",
      "        [-3.5432e-01, -9.4523e-04, -7.8983e-04,  7.4180e-04, -3.5532e-01],\n",
      "        ...,\n",
      "        [-3.5487e-01, -3.2623e-03,  4.8877e-03, -1.7251e-02, -3.7050e-01],\n",
      "        [-8.3000e-02, -1.1509e-03, -2.1405e-02, -3.2985e-03, -1.0885e-01],\n",
      "        [ 3.3917e+00,  9.3996e-03, -4.2295e-03,  2.8833e-02,  3.4257e+00]])\n"
     ]
    }
   ],
   "source": [
    "b_dict = dict(x=batch[0][0], \n",
    "              bond_x=batch[0][1], \n",
    "              sc_x=batch[0][2], \n",
    "              sc_m_x=batch[0][3], \n",
    "              eucl_dists=batch[0][4], \n",
    "              graph_dists=batch[0][5], \n",
    "              angles=batch[0][6], \n",
    "              mask=batch[0][7], \n",
    "              bond_idx=batch[0][8], \n",
    "              sc_idx=batch[0][9],\n",
    "              angles_idx=batch[0][10],\n",
    "              exp_idx=batch[0][11], \n",
    "              sc_types=batch[0][12], \n",
    "              y=batch[1])\n",
    "for k,v in b_dict.items(): print(f'{k}:\\n {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the metric used for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0,
     31,
     43,
     46
    ]
   },
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types, epoch):\n",
    "    proc = lambda x: x.cpu().numpy().ravel() \n",
    "    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n",
    "    y_true = SC_MEAN + y_true * SC_STD\n",
    "    y_pred = SC_MEAN + y_pred * SC_STD\n",
    "    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n",
    "    gmlmae = np.log(maes).mean()\n",
    "    # print(f'Epoch: {epoch} - Group Mean Log Mae: {gmlmae}')\n",
    "    return gmlmae\n",
    "\n",
    "class GroupMeanLogMAE(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['group_mean_log_mae'])\n",
    "    def on_epoch_begin(self, **kwargs): self.input, self.output, self.target = [], [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, last_input, train, **kwargs):\n",
    "        if not train:\n",
    "            self.input.append(last_input[-1])\n",
    "            self.output.append(last_output[:,-1])\n",
    "            self.target.append(last_target[:,-1])\n",
    "                \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n",
    "        if (len(self.input) > 0) and (len(self.output) > 0):\n",
    "            inputs = torch.cat(self.input)\n",
    "            preds = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            metric = group_mean_log_mae(preds, target, inputs, epoch)\n",
    "            return add_metrics(last_metrics, [metric])\n",
    "\n",
    "def contribs_rmse_loss(preds, targs):\n",
    "    \"\"\"\n",
    "    Returns the sum of RMSEs for each sc contribution and total sc value.\n",
    "    \n",
    "    Args:\n",
    "        - preds: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            predictions. Last column is the total scalar coupling value.\n",
    "        - targs: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            true values. Last column is the total scalar coupling value.\n",
    "    \"\"\"\n",
    "    return torch.mean((preds - targs) ** 2, dim=0).sqrt().sum()\n",
    "\n",
    "def rmse(preds, targs):\n",
    "    return torch.sqrt(F.mse_loss(preds[:,-1], targs[:,-1]))\n",
    "\n",
    "def mae(preds, targs):\n",
    "    return torch.abs(preds[:,-1] - targs[:,-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-2\n",
    "d_x, d_model, d_bond, d_sc_pair, d_sc_m = N_ATOM_FEATURES, 512, N_EDGE_FEATURES, N_SC_EDGE_FEATURES, N_SC_MOL_FEATURES\n",
    "enn_args = dict(layers=3*[d_model], dropout=3*[0.0], batch_norm=True)\n",
    "ann_args = dict(layers=1*[d_model], dropout=1*[0.0], batch_norm=True, out_act=nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "model = Transformer(d_x, d_bond, d_sc_pair, d_sc_m, N=6, d_model=d_model, d_ff=d_model*4, d_ff_contrib=d_model//4, \n",
    "                    h=8, dropout=0.0, stride=128, enn_args=enn_args, ann_args=ann_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (projection): Linear(in_features=21, out_features=512, bias=True)\n",
      "  (encoder): Encoder(\n",
      "    (mess_pass_layer): MessagePassingLayer(\n",
      "      (bond_mess): ENNMessage(\n",
      "        (enn): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=8, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (4): ReLU(inplace)\n",
      "            (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (7): ReLU(inplace)\n",
      "            (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (9): Linear(in_features=512, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ann): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (4): Tanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (sc_mess): ENNMessage(\n",
      "        (enn): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=512, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (4): ReLU(inplace)\n",
      "            (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (7): ReLU(inplace)\n",
      "            (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (9): Linear(in_features=512, out_features=65536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linears): ModuleList(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (5): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (7): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (8): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (9): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (10): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (11): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (sublayer): ModuleList(\n",
      "        (0): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (1): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (2): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (3): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (4): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (5): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (6): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (7): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (8): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (9): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (10): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "        (11): SublayerConnection(\n",
      "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (dropout): Dropout(p=0.0)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attn_layers): ModuleList(\n",
      "      (0): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): AttendingLayer(\n",
      "        (eucl_dist_attn): MultiHeadedEuclDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (graph_dist_attn): MultiHeadedGraphDistAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (embedding): Embedding(11, 8)\n",
      "        )\n",
      "        (self_attn): MultiHeadedSelfAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ff): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "          (3): SublayerConnection(\n",
      "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (dropout): Dropout(p=0.0)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (write_head): MyCustomHead(\n",
      "    (preproc): Sequential(\n",
      "      (0): Linear(in_features=1049, out_features=2048, bias=True)\n",
      "      (1): ReLU(inplace)\n",
      "      (2): LayerNorm(torch.Size([2048]), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (types_net): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=1049, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): LayerNorm(torch.Size([1049]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (contribs_net): ContribsNet(\n",
      "      (blocks): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=1049, out_features=128, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=1049, out_features=128, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=1049, out_features=128, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=1049, out_features=128, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): Linear(in_features=1049, out_features=128, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)\n",
      "          (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0633e+00, -2.0671e-03,  4.4499e-03, -2.1575e-04, -1.0633e+00],\n",
      "        [ 2.3078e-01,  9.3059e-04, -1.2115e-02,  2.1701e-02,  2.4060e-01],\n",
      "        [ 6.0988e-01, -2.0550e-03, -4.9458e-03,  3.1262e-03,  6.0697e-01],\n",
      "        ...,\n",
      "        [ 5.5447e-01,  7.0328e-04, -2.8908e-02, -1.6123e-02,  5.1079e-01],\n",
      "        [ 8.4897e-01, -4.0527e-04, -8.0956e-03,  2.5952e-02,  8.6873e-01],\n",
      "        [-1.6030e-01, -5.5304e-03, -4.2218e-03,  5.5054e-04, -1.6857e-01]],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([1144, 5])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model(*batch[0]))\n",
    "print(model(*batch[0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(LearnerCallback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, learn:Learner, clip:float = 0., start_it:int = 100):\n",
    "        super().__init__(learn)\n",
    "        self.clip, self.start_it = clip, start_it\n",
    "\n",
    "    def on_backward_end(self, iteration, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip and (iteration > self.start_it): nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics=[rmse, mae], opt_func=partial(AdamW, betas=(0.95, 0.99)),\n",
    "                callback_fns=[partial(GradientClipping, clip=10), GroupMeanLogMAE], \n",
    "                wd=wd, loss_func=contribs_rmse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.create_opt(1e-3, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimWrapper over Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.95, 0.99)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.95, 0.99)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ").\n",
       "True weight decay: True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find(start_lr=1e-7, end_lr=1.0, num_it=100, stop_div=True)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.00% [8/10 2:03:55<30:58]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>group_mean_log_mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.063132</td>\n",
       "      <td>0.060790</td>\n",
       "      <td>0.028409</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>-0.464270</td>\n",
       "      <td>15:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.050103</td>\n",
       "      <td>0.048432</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.015548</td>\n",
       "      <td>-0.616353</td>\n",
       "      <td>15:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.038285</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>0.018126</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>15:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032635</td>\n",
       "      <td>0.033202</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>-1.134398</td>\n",
       "      <td>15:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>0.027757</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.008388</td>\n",
       "      <td>-1.335252</td>\n",
       "      <td>15:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.022509</td>\n",
       "      <td>0.023166</td>\n",
       "      <td>0.010843</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>-1.561162</td>\n",
       "      <td>15:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018938</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.009322</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>-1.668216</td>\n",
       "      <td>15:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015883</td>\n",
       "      <td>0.017750</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>-1.843306</td>\n",
       "      <td>15:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2136' class='' max='3188', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      67.00% [2136/3188 08:35<04:13 0.0147]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with group_mean_log_mae value: -0.4642700254917145.\n",
      "Better model found at epoch 1 with group_mean_log_mae value: -0.616352915763855.\n",
      "Better model found at epoch 2 with group_mean_log_mae value: -0.9927537441253662.\n",
      "Better model found at epoch 3 with group_mean_log_mae value: -1.1343979835510254.\n",
      "Better model found at epoch 4 with group_mean_log_mae value: -1.335252046585083.\n",
      "Better model found at epoch 5 with group_mean_log_mae value: -1.5611624717712402.\n",
      "Better model found at epoch 6 with group_mean_log_mae value: -1.6682157516479492.\n",
      "Better model found at epoch 7 with group_mean_log_mae value: -1.8433058261871338.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, max_lr=1e-3, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  \n",
    "                                                                  name=f'mol_transformer_v{VERSION}_fold{FOLD_ID}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(skip_start=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_contrib_preds = learn.get_preds(DatasetType.Valid)\n",
    "test_contrib_preds = learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = val_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN\n",
    "test_preds = test_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_submit(predictions):\n",
    "    submit = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "    print(len(submit), len(predictions))   \n",
    "    submit['scalar_coupling_constant'] = predictions\n",
    "    submit.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-submission.csv', index=False)\n",
    "\n",
    "def store_oof(predictions, val_ids):\n",
    "    oof = pd.DataFrame(predictions, columns=['scalar_coupling_constants'])\n",
    "    print(oof.head())\n",
    "    oof.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-oof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_submit(test_preds)\n",
    "store_oof(val_preds, val_mol_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
