{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\n\nfrom fastai.tabular import *\nfrom fastai.callbacks import SaveModelCallback\nfrom fastai.basic_data import DataBunch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# __print__ = print\n# def print(*strings):\n#     for string in strings:\n#         os.system(f'echo \\\"{string}\\\"')\n#         __print__(string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define some constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"FOLD_ID = 2\nVERSION = 2\n\nTYPES              = np.array(['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'])\nTYPES_MAP          = {t: i for i, t in enumerate(TYPES)}\nSC_EDGE_FEATS      = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n                      #'inv_dist', 'normed_inv_dist'\n                     ]\nSC_MOL_FEATS       = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n                      'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', \n                      #'inv_dist', 'normed_inv_dist', \n                      'std_bond_length', 'ave_bond_length', #'total_bond_length',  \n                      #'ave_inv_bond_length', 'total_inv_bond_length', \n                      'ave_atom_weight'#, 'total_atom_weight'\n                     ]\nATOM_FEATS         = ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', \n                      'degree_1', 'degree_2', 'degree_3', 'degree_4', 'degree_5', \n                      'SP', 'SP2', 'SP3', 'hybridization_unspecified', \n                      'aromatic', 'formal_charge', 'atomic_num',\n                      'donor', 'acceptor', \n                      'ave_bond_length', \n                      #'ave_inv_bond_length',\n                      'ave_neighbor_weight']\nEDGE_FEATS         = ['single', 'double', 'triple', 'aromatic', \n                      'conjugated', 'in_ring',\n                      'dist', 'normed_dist', \n                      #'inv_dist', 'normed_inv_dist'\n                     ]\nTARGET_COL         = 'scalar_coupling_constant'\nCONTRIB_COLS       = ['fc', 'sd', 'pso', 'dso']\nN_EDGE_FEATURES    = len(EDGE_FEATS)\nN_SC_EDGE_FEATURES = len(SC_EDGE_FEATS)\nN_SC_MOL_FEATURES  = len(SC_MOL_FEATS)\nN_ATOM_FEATURES    = len(ATOM_FEATS)\nN_TYPES            = len(TYPES)\nN_MOLS             = 130775\nSC_MEAN            = 16\nSC_STD             = 35\n\nSC_FEATS_TO_SCALE   = ['dist', 'dist_min_rad', 'dist_electro_neg_adj', 'num_atoms', 'num_C_atoms', \n                       'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', 'inv_dist', \n                       'ave_bond_length', 'std_bond_length', 'total_bond_length',  'ave_inv_bond_length', \n                       'total_inv_bond_length', 'ave_atom_weight', 'total_atom_weight']\nATOM_FEATS_TO_SCALE = ['atomic_num', 'ave_bond_length', 'ave_inv_bond_length', 'ave_neighbor_weight']\nEDGE_FEATS_TO_SCALE = ['dist', 'inv_dist']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA_PATH = '../data/'\n# PATH = '../tmp/'\nDATA_PATH = '../input/champs-scalar-coupling/'\nPATH = '../input/champs-processed-data-2/'\nCV_IDXS_PATH = '../input/champs-cv-4-fold-idxs/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_csv_files(path):\n    files = os.listdir(path)\n    files = [f for f in files if f.find('.csv') != -1]\n    print(f'{path}:', files)\nshow_csv_files(PATH)\nshow_csv_files(DATA_PATH)\nshow_csv_files(CV_IDXS_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(PATH+'train_proc_df.csv', index_col=0)\ntest_df  = pd.read_csv(PATH+'test_proc_df.csv', index_col=0)\natom_df  = pd.read_csv(PATH+'atom_df.csv', index_col=0)\nedge_df  = pd.read_csv(PATH+'edge_df.csv', index_col=0)\n\ntrain_mol_ids = pd.read_csv(CV_IDXS_PATH+'train_idxs_4_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\nval_mol_ids   = pd.read_csv(CV_IDXS_PATH+'val_idxs_4_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\ntest_mol_ids  = pd.Series(test_df['molecule_id'].unique())\n\ncontribs_df = pd.read_csv(DATA_PATH+'scalar_coupling_contributions.csv')\ntrain_df = pd.concat((train_df, contribs_df[CONTRIB_COLS]), axis=1)\ndel contribs_df\ngc.collect()\n\ntrain_df[[TARGET_COL, 'fc']] = (train_df[[TARGET_COL, 'fc']] - SC_MEAN) / SC_STD\ntrain_df[CONTRIB_COLS[1:]] = train_df[CONTRIB_COLS[1:]] / SC_STD\n\ntrain_df['num_atoms'] = train_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n                                  'num_N_atoms', 'num_O_atoms']].sum(axis=1)\ntest_df['num_atoms'] = test_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n                                'num_N_atoms', 'num_O_atoms']].sum(axis=1)\ntrain_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10\ntest_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define MPNN"},{"metadata":{},"cell_type":"markdown","source":"General dense feedforward NN"},{"metadata":{"code_folding":[5,12,18,27],"trusted":true},"cell_type":"code","source":"def bn_init(m): pass\n#     if type(m) == nn.BatchNorm1d: \n#         nn.init.ones_(m.weight)\n#         nn.init.zeros_(m.bias)\n\ndef selu_weights_init(m):\n    if type(m) == nn.Linear:\n        fan_in = m.weight.size(1)\n        m.weight.data.normal_(0.0, 1.0 / math.sqrt(fan_in))\n        m.bias.fill_(0.0)\n    bn_init(m)\n\ndef relu_weights_init(m): \n#     if type(m) == nn.Linear:\n#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n#         m.bias.data.fill_(0.0)\n    bn_init(m)\n\ndef hidden_layer(n_in, n_out, batch_norm, dropout, layer_norm=False, act=None):\n    layers = []\n    layers.append(nn.Linear(n_in, n_out))\n    if act: layers.append(act)\n    if batch_norm: layers.append(nn.BatchNorm1d(n_out))\n    if layer_norm: layers.append(nn.LayerNorm(n_out))\n    if dropout != 0: layers.append(nn.Dropout(dropout))\n    return layers\n\nclass FullyConnectedNet(nn.Module):\n    \n    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], \n                 batch_norm=False, out_act=None, final_bn=False, layer_norm=False, \n                 final_ln=False):\n        super().__init__()\n        sizes = [n_input] + layers\n        if n_output: \n            sizes += [n_output]\n            dropout += [0.0]\n        layers_ = []\n        for i, (n_in, n_out, dr) in enumerate(zip(sizes[:-1], sizes[1:], dropout)):\n            act_ = act if i < len(layers) else out_act\n            batch_norm_ = batch_norm if i < len(layers) else final_bn\n            layer_norm_ = layer_norm if i < len(layers) else final_ln\n            layers_ += hidden_layer(n_in, n_out, batch_norm_, dr, layer_norm_, act_)      \n        self.layers = nn.Sequential(*layers_)\n        if type(act) == nn.SELU: self.layers.apply(selu_weights_init)\n        else: self.layers.apply(relu_weights_init)\n        \n    def forward(self, x):\n        return self.layers(x)\n    \nclass ResFullyConnectedNet(nn.Module):\n    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], \n                 batch_norm=False, out_act=None, final_bn=False, layer_norm=False, \n                 final_ln=False):\n        super().__init__()\n        n_layers, sizes = len(layers), [n_input] + layers\n        if n_output: \n            sizes += [n_output]\n            dropout += [0.0]\n        assert ((n_layers - 1) % 2) == 0\n        self.n_blocks, blocks =(n_layers - 1) // 2, [], \n        self.fc1 = nn.Sequential(*hidden_layer(n_input, layers[0], batch_norm, \n                                               dropout.pop(0), layer_norm, act))\n        for i in range(self.n_blocks):\n            blocks.append(FullyConnectedNet(layers[2*i], layers[2*(i+1)], [layers[(2*i)+1]], act, \n                                            dropout[2*i:2*(i+1)], batch_norm, act, \n                                            batch_norm, layer_norm, layer_norm))\n        self.blocks = nn.ModuleList(blocks)\n        self.fc_out = nn.Sequential(*hidden_layer(layers[-1], n_output, final_bn, \n                                                  0.0, final_ln, out_act))\n            \n    def forward(self, x):\n        x = self.fc1(x)\n        for i in range(self.n_blocks):\n            x_ = self.blocks[i](x)\n            x = x + x_\n        y = self.fc_out(x)\n        return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LSTM cell as describedi in the set2set paper (https://arxiv.org/pdf/1511.06391.pdf). Doesn't take any inputs."},{"metadata":{"code_folding":[0],"trusted":true},"cell_type":"code","source":"class HiddenLSTMCell(nn.Module):\n    \"\"\"Implements the LSTM cell update described in the sec 4.2 of https://arxiv.org/pdf/1511.06391.pdf.\"\"\"\n    \n    def __init__(self, n_h_out):\n        \"\"\"This LSTM cell takes no external 'x' inputs, but has a hidden state appended with the \n        readout from a content based attention mechanism. Therefore the hidden state is of a dimension\n        that is two times the number of nodes in the set.\"\"\"\n        super().__init__()\n        self.n_h_out, self.n_h = n_h_out, n_h_out * 2 \n        self.w_h = nn.Parameter(torch.Tensor(self.n_h, n_h_out * 4))\n        self.b = nn.Parameter(torch.Tensor(n_h_out * 4))\n        self.init_weights()\n    \n    def init_weights(self):\n        for p in self.parameters():\n            if p.data.ndimension() >= 2:\n                nn.init.xavier_uniform_(p.data)\n                # nn.init.orthogonal_(p.data)\n            else: \n                nn.init.zeros_(p.data)\n                # initialize the forget gate bias to 1\n                p.data[self.n_h_out:self.n_h_out*2] = torch.ones(self.n_h_out)\n        \n    def forward(self, h_prev, c_prev):\n        \"\"\"Takes previuos hidden and cell states as arguments and performs a \n        single LSTM step using no external input.\n        \"\"\"\n        n_h_ = self.n_h_out # number of output hidden states\n        # batch the computations into a single matrix multiplication\n        gates = h_prev @ self.w_h + self.b\n        i_g, f_g, g, o_g = (\n            torch.sigmoid(gates[:, :n_h_]), # input\n            torch.sigmoid(gates[:, n_h_:n_h_*2]), # forget\n            torch.tanh(gates[:, n_h_*2:n_h_*3]),\n            torch.sigmoid(gates[:, n_h_*3:]), # output\n        )\n        c = f_g * c_prev + i_g * g\n        h = o_g * torch.tanh(c)\n        return h, c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class IndRNNCell(nn.Module):\n    def __init__(self, n_in, n_h, act=nn.ReLU(True), layer_norm=True, dropout=0.0):\n        super().__init__()\n        self.lin = nn.Linear(n_in, n_h)\n        act_ln_dr = [act]\n        if layer_norm: act_ln_dr.append(nn.LayerNorm(n_h))\n        if dropout!=0.0: act_ln_dr.append(nn.Dropout(dropout))\n        self.act_ln_dr = nn.Sequential(*act_ln_dr)\n        self.w_h = nn.Parameter(torch.Tensor(n_h))\n        nn.init.uniform_(self.w_h, a=0, b=1)\n        \n    def forward(self, x, h_prev):\n        h = self.act_ln_dr(self.lin(x) + self.w_h * h_prev)\n        return h\n        \nclass IndRNN(nn.Module):\n    def __init__(self, n_x, n_h, n_layers, layer_norm=True, dropout=[]):\n        super().__init__()\n        self.n_layers = n_layers\n        if len(dropout)==0: dropout = n_layers * [0.0]\n        assert len(dropout) == n_layers\n        layers = []\n        for i, dr in enumerate(dropout):\n            n_in = n_x if 1==0 else n_h\n            layers.append(IndRNNCell(n_in, n_h, layer_norm=layer_norm, dropout=dr))\n        self.layers = nn.ModuleList(layers)\n            \n    def forward(self, x, h_prev):\n        h, hs = x, []\n        for i in range(self.n_layers):\n            h = self.layers[i](h, h_prev[i])\n            hs.append(h)\n        return h, torch.cat(hs, dim=0)\n        \nclass ResIndRNN(nn.Module):\n    def __init__(self, n_x, n_h, n_blocks, layer_norm=True, dropout=[]):\n        super().__init__()\n        self.n_blocks = n_blocks\n        if len(dropout)==0: dropout = n_blocks * [0.0]\n        assert len(dropout) == n_blocks\n        blocks = []\n        for i, dr in enumerate(dropout):\n            n_in = n_x if 1==0 else n_h\n            blocks.append(IndRNN(n_in, n_h, n_layers=2, layer_norm=layer_norm, dropout=2*[dr]))\n        self.blocks = nn.ModuleList(blocks)\n            \n    def forward(self, x, h_prev):\n        hs = []\n        for i in range(self.n_blocks):\n            h, hs_ = self.blocks[i](x, h_prev[(i*2):((i+1)*2)])\n            x = h + x\n            hs.append(hs_)\n        return x, torch.cat(hs, dim=0)\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set2set module."},{"metadata":{"code_folding":[0,6,11,41],"trusted":true},"cell_type":"code","source":"def scatter_sum(src, idx, num):\n    sz = num, src.size(1)\n    exp_idx = idx[:,None].repeat(1, sz[1])\n    out = torch.zeros(sz, dtype=src.dtype, device=src.device)\n    return out.scatter_add(0, exp_idx, src)\n\ndef scatter_mean(src, idx, num):\n    return scatter_sum(src, idx, num) / scatter_sum(torch.ones_like(src), idx, num).clamp(1.0)\n\ndef softmax(x, idx, num=None):\n    x = x.exp()\n    x = x / (scatter_sum(x, idx, num=num)[idx] + 1e-16)\n    return x\n\nclass SumReadout(nn.Module):\n    def __init__(self): super().__init__()\n    def forward(self, x, node_idx): return scatter_sum(x, node_idx, num=node_idx.max().item()+1)\n    \nclass MeanReadout(nn.Module):\n    def __init__(self): super().__init__()\n    def forward(self, x, node_idx): return scatter_mean(x, node_idx, num=node_idx.max().item()+1)\n\nclass Set2SetIndRNN(nn.Module):\n    def __init__(self, n_set_in, proc_steps, n_blocks=3):\n        super().__init__()\n        self.proc_steps = proc_steps\n        self.gru = ResIndRNN(n_set_in, n_set_in, n_blocks, layer_norm=True, dropout=[])\n        self.init_q = nn.Parameter(torch.zeros(2 * n_blocks, 1, n_set_in))\n        self.init_r = nn.Parameter(torch.zeros(1, n_set_in))\n\n    def forward(self, x, node_idx):\n        \"\"\"\n        x - input tensor of shape (batch_size * n_nodes, in_channels).\n        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n            node to its corresponding index in the batch.\n        \"\"\"\n        batch_size = node_idx.max().item() + 1\n        qs = self.init_q.expand(-1, batch_size, -1).contiguous()\n        r = self.init_r.expand(batch_size, -1).contiguous()\n        for i in range(self.proc_steps):\n            q, qs = self.gru(r, qs)\n            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n            a = softmax(e, node_idx, num=batch_size)\n            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n        return torch.cat([q, r], dim=-1) #q_star\n    \n    \nclass Set2SetGRU(nn.Module):\n    def __init__(self, n_set_in, proc_steps):\n        super().__init__()\n        self.proc_steps = proc_steps\n        self.gru = nn.GRUCell(n_set_in, n_set_in)\n        self.init_q = nn.Parameter(torch.zeros(1, n_set_in))\n        self.init_r = nn.Parameter(torch.zeros(1, n_set_in))\n\n    def forward(self, x, node_idx):\n        \"\"\"\n        x - input tensor of shape (batch_size * n_nodes, in_channels).\n        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n            node to its corresponding index in the batch.\n        \"\"\"\n        batch_size = node_idx.max().item() + 1\n        q = self.init_q.expand(batch_size, -1).contiguous()\n        r = self.init_r.expand(batch_size, -1).contiguous()\n        for i in range(self.proc_steps):\n            q = self.gru(r, q)\n            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n            a = softmax(e, node_idx, num=batch_size)\n            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n        return torch.cat([q, r], dim=-1) #q_star\n\nclass Set2SetLSTM(nn.Module):\n    \"\"\"\n    Adapted from: https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric\\\n        /nn/glob/set2set.html#Set2Set\n    \"\"\"\n    def __init__(self, n_set_in, proc_steps):\n        super().__init__()\n        self.n_set_in, n_set_out = n_set_in, 2 * n_set_in\n        self.proc_steps = proc_steps\n        self.lstm = HiddenLSTMCell(n_set_in)\n        self.init_q_star = nn.Parameter(torch.zeros(1, n_set_out))\n        self.init_h = nn.Parameter(torch.zeros(1, n_set_in))\n\n    def forward(self, x, node_idx):\n        \"\"\"\n        x - input tensor of shape (batch_size * n_nodes, in_channels).\n        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n            node to its corresponding index in the batch.\n        \"\"\"\n        batch_size = node_idx.max().item() + 1\n        h = self.init_h.expand(batch_size, -1).contiguous()\n        q_star = self.init_q_star.expand(batch_size, -1).contiguous()\n        for i in range(self.proc_steps):\n            q, h = self.lstm(q_star, h)\n            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n            a = softmax(e, node_idx, num=batch_size)\n            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n            q_star = torch.cat([q, r], dim=-1)\n        return q_star","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Edge network message function as described in the MPNN paper (https://arxiv.org/pdf/1704.01212.pdf). Adds in seperate edge network to allow messages to flow along scalar coupling edges."},{"metadata":{"code_folding":[0],"trusted":true},"cell_type":"code","source":"class EdgeNetwork(nn.Module):\n    def __init__(self, n_h, n_e, n_sc_e, net_args={}):\n        super().__init__()\n        self.n_h = n_h\n        self.adj_net = FullyConnectedNet(n_e, n_h**2, **net_args)\n        self.sc_adj_net = FullyConnectedNet(n_sc_e, n_h**2, **net_args)\n        self.b = nn.Parameter(torch.Tensor(n_h)) # bias for the message function\n        self.weight_inits()\n        \n    def weight_inits(self):\n        nn.init.zeros_(self.b)\n    \n    def forward(self, h, e, sc_e, pairs_idx, sc_pairs_idx, t=0):\n        \"\"\"\n        Compute message vector m_t given the previuos hidden state\n        h_t-1 and edge features e.\n        - h: tensor of hidden states of shape (batch_size * n_nodes, n_h)\n        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n        - sc_e: tensor of scalar coupling edge features of shape \n            (batch_size * n_sc, n_sc_e).\n        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n            indexes (first column) to the other atom indexes they form a \n            bond with (second column). Atom indices are unique to the entire\n            batch.\n        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n            indices of the atoms for which the scalar coupling constant\n            need to be predicted. Atom indices are unique to the entire\n            batch.\n        - t: update iteration. \n        \"\"\"\n        # compute 'A(e)'\n        if t==0: \n            self.a_mat = self.get_a_mat(self.adj_net(e))\n            self.a_sc_mat = self.get_a_mat(self.sc_adj_net(sc_e))\n            \n        # compute 'm_{i} = sum_{j in N(i)}(A_{ij}h_{j})' for all nodes 'i'\n        m = self.add_message(torch.zeros_like(h), self.a_mat, h, pairs_idx)\n        m = self.add_message(m, self.a_sc_mat, h, sc_pairs_idx)\n        \n        # add message bias\n        m = m + self.b\n        return m # apply optional batch norm\n    \n    def get_a_mat(self, a_vect):\n        return a_vect.view(-1, self.n_h, self.n_h) / (self.n_h ** .5)\n    \n    def add_message(self, m, a, h, pairs_idx):\n        # transform 'pairs_idx' and 'a' to make messages go both in to and out of all nodes\n        in_out_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n        a_ = torch.cat((a, a)) \n        \n        # select the 'h_{j}' feeding into the 'm_{i}'\n        h_in = h.index_select(0, in_out_idx[:,1])\n        \n        # do the matrix multiplication 'A_{ij}h_{j}'\n        ah = (h_in.unsqueeze(1) @ a_).squeeze(1)\n        \n        # Sum up all 'A_{ij}h_{j}' per node 'i'\n        return m.scatter_add(0, in_out_idx[:,0,None].repeat(1, self.n_h), ah)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GRU update function as described in the MPNN paper."},{"metadata":{"code_folding":[0],"trusted":true},"cell_type":"code","source":"class GRUUpdate(nn.Module):\n    def __init__(self, n_h):\n        super().__init__()\n        self.gru = nn.GRUCell(n_h, n_h)\n        \n    def forward(self, m, h_prev):\n        \"\"\"\n        Update hidden state h.\n        - h_prev is vector of hidden states of shape (batch_size * n_nodes, n_h)\n        - m is vector of messages of shape (batch_size * n_nodes, n_h)\n        \"\"\"\n        return self.gru(m, h_prev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom readout network following th set2set processing stage. Allows some final specialization/fine-tuning for each scalar coupling type."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_contrib_head(n_in, n_h, act, dropout=0.0, layer_norm=True):\n    layers = hidden_layer(n_in, n_h, False, dropout, layer_norm, act)\n    layers += hidden_layer(n_h, 1, False, 0.0) # output layer\n    return nn.Sequential(*layers)\n\nclass ContribsHead(nn.Module):\n    N_CONTRIBS = 5\n    CONTIB_SCALES = [1, 250, 45, 35, 500]\n    \n    def __init__(self, n_in, n_h, act, dropout=0.0, layer_norm=True):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            create_contrib_head(n_in, n_h, act, dropout, layer_norm) \n            for _ in range(self.N_CONTRIBS)\n        ])\n        \n    def forward(self, x):\n        ys = torch.cat([b(x) / s for b, s in zip(self.blocks, self.CONTIB_SCALES)], dim=-1)\n        return torch.cat([ys[:,:-1], ys.sum(dim=-1, keepdim=True)], dim=-1)\n\nclass MyCustomHead(nn.Module):\n    N_OUTPUTS = 5\n    \n    def __init__(self, n_input, n_h_contribs, pre_layers=[], post_layers=[], \n                 act=nn.ReLU(True), dropout=[], norm=False):\n        super().__init__()\n        n_pre_layers = len(pre_layers)\n        self.preproc = FullyConnectedNet(n_input, None, pre_layers, act, \n                                         dropout[:n_pre_layers], batch_norm=norm)\n        self.types_proc = nn.ModuleList([\n            FullyConnectedNet(pre_layers[-1], None, post_layers, act, dropout[n_pre_layers:-1], layer_norm=norm)\n            for _ in range(N_TYPES)\n        ])\n        self.contribs_head = ContribsHead(post_layers[-1], n_h_contribs, act, dropout[-1], layer_norm=norm)\n        \n    def forward(self, x, sc_types):\n        x_ = self.preproc(x)\n        x_types = torch.zeros(x.size(0), x.size(1), device=x.device)\n        for i in range(N_TYPES):\n            if torch.any(sc_types == i): \n                x_types[sc_types == i] = self.types_proc[i](x_[sc_types == i])\n        x = x + x_types \n        y = self.contribs_head(x)\n        return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combines all the the components of the readout function described in the MPNN network using set2set processing and the scalar coupling type customized head. Also adds in some skip connections to final node states and scalar coupling input features."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Set2SetOutput(nn.Module):\n    def __init__(self, n_x, n_h, n_sc_m, proc_steps=10, readout_type='Set2SetGRU', net_args={}):\n        super().__init__()\n        self.R_proj = nn.Linear(n_h + n_x, n_h)\n        if readout_type=='Sum': self.R_proc = SumReadout()\n        if readout_type=='Mean': self.R_proc = MeanReadout()\n        if readout_type=='Set2SetGRU': self.R_proc = Set2SetGRU(n_h, proc_steps)\n        if readout_type=='Set2SetLSTM': self.R_proc = Set2SetLSTM(n_h, proc_steps)\n        if readout_type=='Set2SetIndRNN': self.R_proc = Set2SetIndRNN(n_h, proc_steps)\n        n_readout = n_h if readout_type[:7]!='Set2Set' else 2 * n_h \n        self.R_write = MyCustomHead(n_readout + (2 * n_h) + n_sc_m, **net_args)\n    \n    def forward(self, h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types):\n        \"\"\"\n        Make prediction.\n        - h is vector of hidden states of shape (batch_size * n_nodes, n_h).\n        - x is vector of input features of shape (batch_size * n_nodes, n_x).\n        - sc_m: tensor of scalar coupling molecule level features of shape \n            (batch_size * n_sc, n_sc_m).\n        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n            node to its corresponding index in the batch.\n        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n            scalar coupling constant to its corresponding index in the batch.\n        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n            indices of the atoms for which the scalar coupling constant\n            need to be predicted. Atom indices are unique to the entire\n            batch.\n        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n            coupling type of each observation. \n        \"\"\"\n        m = self.R_proj(torch.cat([h, x], dim=1))\n        q = self.R_proc(m, node_idx)\n        \n        # introduce skip connection to final node states of scalar coupling atoms\n        inp = torch.cat([\n            q.index_select(0, sc_idx),\n            h.index_select(0, sc_pairs_idx[:,0]),\n            h.index_select(0, sc_pairs_idx[:,1]),\n            sc_m\n        ], dim=-1)\n        y = self.R_write(inp, sc_types)\n        return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combines the edge-network message (M), GRU update (U) and set2set readout (R) functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MPNN(nn.Module):\n    def __init__(self, n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps=3, proc_steps=10, \n                 readout_type='Set2SetGRU', preproc_net_args={}, enn_args={}, R_net_args={}):\n        super().__init__()\n        self.preproc_net = FullyConnectedNet(n_x, n_h, **preproc_net_args)\n        self.M = EdgeNetwork(n_h, n_e, n_sc_e, enn_args)\n        self.U = GRUUpdate(n_h)\n        self.R = Set2SetOutput(n_x, n_h, n_sc_m, proc_steps, readout_type, R_net_args)\n        self.update_steps = update_steps\n        \n    def forward(self, x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types):\n        \"\"\"\n        Args:\n        - x: tensor of node features of shape (batch_size * n_nodes, n_x).\n        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n        - sc_e: tensor of scalar coupling edge features of shape \n            (batch_size * n_sc, n_sc_e).\n        - sc_m: tensor of scalar coupling molecule level features of shape \n            (batch_size * n_sc, n_sc_m).\n        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n            node to its corresponding index in the batch.\n        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n            indexes (first column) to the other atom indexes they form a \n            bond with (second column). Atom indices are unique to the entire\n            batch.\n        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n            scalar coupling constant to its corresponding index in the batch.\n        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n            indices of the atoms for which the scalar coupling constant\n            need to be predicted. Atom indices are unique to the entire\n            batch.\n        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n            coupling type of each observation. \n        \"\"\"\n        h = self.preproc_net(x)\n        for t in range(self.update_steps):\n            m = self.M(h, e, sc_e, pairs_idx, sc_pairs_idx, t)\n            h = self.U(m, h)\n        y = self.R(h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types)\n        return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed=100):\n    # python RNG\n    random.seed(seed)\n\n    # pytorch RNGs\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\n    # numpy RNG\n    np.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mol_ids = train_df['molecule_id'].unique()\nn_obs = len(mol_ids)\nsplit = int(n_obs*0.75)\nset_seed(100)\nmol_ids_ = np.random.choice(mol_ids, size=n_obs, replace=False)\ntrain_mol_ids, val_mol_ids = pd.Series(mol_ids_[:split]), pd.Series(mol_ids_[split:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_features(df, features, train_mol_ids):\n    idx = df['molecule_id'].isin(train_mol_ids)\n    return df.loc[idx, features].mean(), df.loc[idx, features].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if any(train_df[SC_FEATS_TO_SCALE].mean().abs()>0.1) or any((train_df[SC_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n    sc_feat_means, sc_feat_stds = scale_features(train_df, SC_FEATS_TO_SCALE, train_mol_ids)\n    train_df[SC_FEATS_TO_SCALE] = (train_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n    test_df[SC_FEATS_TO_SCALE] = (test_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\nif any(atom_df[ATOM_FEATS_TO_SCALE].mean().abs()>0.1) or any((atom_df[ATOM_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n    atom_feat_means, atom_feat_stds = scale_features(atom_df, ATOM_FEATS_TO_SCALE, train_mol_ids)\n    atom_df[ATOM_FEATS_TO_SCALE] = (atom_df[ATOM_FEATS_TO_SCALE] - atom_feat_means) / atom_feat_stds\nif any(edge_df[EDGE_FEATS_TO_SCALE].mean().abs()>0.1) or any((edge_df[EDGE_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n    edge_feat_means, edge_feat_stds = scale_features(edge_df, EDGE_FEATS_TO_SCALE, train_mol_ids)\n    edge_df[EDGE_FEATS_TO_SCALE] = (edge_df[EDGE_FEATS_TO_SCALE] - edge_feat_means) / edge_feat_stds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_mol_sc = train_df.groupby('molecule_id')\ntest_gb_mol_sc = test_df.groupby('molecule_id')\ngb_mol_atom = atom_df.groupby('molecule_id')\ngb_mol_edge = edge_df.groupby('molecule_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the pytorch dataset class."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass MoleculeDataset(Dataset):\n    \n    def __init__(self, mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge):\n        self.n = len(mol_ids)\n        self.mol_ids = mol_ids\n        self.gb_mol_sc = gb_mol_sc\n        self.gb_mol_atom = gb_mol_atom\n        self.gb_mol_edge = gb_mol_edge\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return (self.gb_mol_sc.get_group(self.mol_ids[idx]),\n                self.gb_mol_atom.get_group(self.mol_ids[idx]), \n                self.gb_mol_edge.get_group(self.mol_ids[idx]))\n\ndef np_lst_to_torch(arr_lst, dtype=torch.float):\n    return torch.from_numpy(np.ascontiguousarray(np.concatenate(arr_lst))).type(dtype)\n\ndef collate_fn(batch, test=False):\n    batch_size, n_atom_sum = len(batch), 0\n    x, e, sc_e, sc_m = [], [], [], []\n    sc_types, sc_vals = [], []\n    node_idx, pairs_idx, sc_pairs_idx, sc_idx = [], [], [], []\n\n    for b in range(batch_size):\n        sc_df, atom_df, edge_df = batch[b]\n        n_atoms, n_sc = len(atom_df), len(sc_df)\n        \n        x.append(atom_df[ATOM_FEATS].values)\n        e.append(edge_df[EDGE_FEATS].values)\n        sc_e.append(sc_df[SC_EDGE_FEATS].values)\n        sc_m.append(sc_df[SC_MOL_FEATS].values)\n        sc_types.append(sc_df['type'].values)\n        if not test: sc_vals.append(sc_df[CONTRIB_COLS+[TARGET_COL]].values)\n        \n        node_idx.append(np.repeat(b, n_atoms))\n        sc_idx.append(np.repeat(b, n_sc))\n        pairs_idx.append(edge_df[['idx_0', 'idx_1']].values + n_atom_sum)\n        sc_pairs_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values + n_atom_sum)\n        \n        n_atom_sum += n_atoms\n        \n    \n    x, e = np_lst_to_torch(x), np_lst_to_torch(e), \n    sc_e, sc_m = np_lst_to_torch(sc_e), np_lst_to_torch(sc_m)\n    if not test: sc_vals = np_lst_to_torch(sc_vals)\n    else: sc_vals = torch.tensor([0] * len(sc_types))\n    sc_types = np_lst_to_torch(sc_types, torch.long)\n    node_idx = np_lst_to_torch(node_idx, torch.long)\n    sc_idx = np_lst_to_torch(sc_idx, torch.long)\n    pairs_idx = np_lst_to_torch(pairs_idx, torch.long)\n    sc_pairs_idx = np_lst_to_torch(sc_pairs_idx, torch.long)\n    \n    return (x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types), sc_vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seed(100)\nbatch_size = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = MoleculeDataset(train_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge)\nval_ds   = MoleculeDataset(val_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge)\ntest_ds  = MoleculeDataset(test_mol_ids, test_gb_mol_sc, gb_mol_atom, gb_mol_edge)\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8)\nval_dl   = DataLoader(val_ds, batch_size, num_workers=8)\ntest_dl  = DeviceDataLoader.create(test_ds, batch_size, num_workers=8, collate_fn=partial(collate_fn, test=True))\ndb = DataBunch(train_dl, val_dl, collate_fn=collate_fn)\ndb.test_dl = test_dl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = next(iter(train_dl))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for el in batch[0]: print(el.size())\nprint(batch[1].size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b_dict = dict(x=batch[0][0], \n              e=batch[0][1], \n              sc_e=batch[0][2], \n              sc_m=batch[0][3], \n              node_idx=batch[0][4], \n              pairs_idx=batch[0][5], \n              sc_idx=batch[0][6], \n              sc_pairs_idx=batch[0][7], \n              sc_types=batch[0][8], \n              y=batch[1])\nfor k,v in b_dict.items(): print(f'{k}:\\n {v}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implement the metric used for this competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_mean_log_mae(y_true, y_pred, types, epoch):\n    proc = lambda x: x.cpu().numpy().ravel() \n    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n    y_true = SC_MEAN + y_true * SC_STD\n    y_pred = SC_MEAN + y_pred * SC_STD\n    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n    gmlmae = np.log(maes).mean()\n    print(f'Epoch: {epoch} - Group Mean Log Mae: {gmlmae}')\n    return gmlmae\n\nclass GroupMeanLogMAE(Callback):\n    _order = -20 #Needs to run before the recorder\n\n    def __init__(self, learn, **kwargs): self.learn = learn\n    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['group_mean_log_mae'])\n    def on_epoch_begin(self, **kwargs): self.input, self.output, self.target = [], [], []\n    \n    def on_batch_end(self, last_target, last_output, last_input, train, **kwargs):\n        if not train:\n            self.input.append(last_input[-1])\n            self.output.append(last_output[:,-1])\n            self.target.append(last_target[:,-1])\n                \n    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n        if (len(self.input) > 0) and (len(self.output) > 0):\n            inputs = torch.cat(self.input)\n            preds = torch.cat(self.output)\n            target = torch.cat(self.target)\n            metric = group_mean_log_mae(preds, target, inputs, epoch)\n            return add_metrics(last_metrics, [metric])\n\ndef contribs_rmse_loss(preds, targs):\n    \"\"\"\n    Returns the sum of RMSEs for each sc contribution and total sc value.\n    \n    Args:\n        - preds: tensor of shape (batch_size * n_sc, 5) containing \n            predictions. Last column is the total scalar coupling value.\n        - targs: tensor of shape (batch_size * n_sc, 5) containing \n            true values. Last column is the total scalar coupling value.\n    \"\"\"\n    return torch.mean((preds - targs) ** 2, dim=0).sqrt().sum()\n\ndef rmse(preds, targs):\n    return torch.sqrt(F.mse_loss(preds[:,-1], targs[:,-1]))\n\ndef mae(preds, targs):\n    return torch.abs(preds[:,-1] - targs[:,-1]).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wd, norm, act = 0, True, nn.ReLU(True)\nupdate_steps, proc_steps, readout_type = 5, 6, 'Set2SetGRU'\nn_x, n_h, n_e, n_sc_e, n_sc_m = N_ATOM_FEATURES, 300, N_EDGE_FEATURES, N_SC_EDGE_FEATURES, N_SC_MOL_FEATURES\npreproc_net_args = dict(layers=[], act=act, dropout=[], out_act=nn.Tanh())\nenn_args = dict(layers=3*[n_h], act=act, dropout=3*[0.0], batch_norm=norm)\nR_net_args = dict(pre_layers=[1500], post_layers=[n_sc_m+4*n_h], n_h_contribs=200, act=act, dropout=[0.0, 0.0, 0.0], \n                  norm=norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seed(100)\nmodel = MPNN(n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps, proc_steps, readout_type, preproc_net_args, enn_args, R_net_args)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)\nprint(model(*batch[0]))\nprint(model(*batch[0]).size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GradientClipping(LearnerCallback):\n    \"Gradient clipping during training.\"\n    def __init__(self, learn:Learner, clip:float = 0., start_it:int = 100):\n        super().__init__(learn)\n        self.clip, self.start_it = clip, start_it\n\n    def on_backward_end(self, iteration, **kwargs):\n        \"Clip the gradient before the optimizer step.\"\n        if self.clip and (iteration > self.start_it): nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(db, model, metrics=[rmse, mae], \n                callback_fns=[partial(GradientClipping, clip=10), GroupMeanLogMAE], \n                wd=wd, loss_func=contribs_rmse_loss)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"learn.lr_find(start_lr=1e-6, end_lr=1.0, num_it=100, stop_div=True)\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, max_lr=1e-3, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n                                                                  monitor='group_mean_log_mae',  name='mpnn1')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_losses(skip_start=450)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_contrib_preds = learn.get_preds(DatasetType.Valid)\ntest_contrib_preds = learn.get_preds(DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = val_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN\ntest_preds = test_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def store_submit(predictions):\n    submit = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n    print(len(submit), len(predictions))   \n    submit['scalar_coupling_constant'] = predictions\n    submit.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-submission.csv', index=False)\n\ndef store_oof(predictions, val_ids):\n    oof = pd.DataFrame(predictions, columns=['scalar_coupling_constants'])\n    print(oof.head())\n    oof.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-oof.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# store_submit(test_preds)\n# store_oof(val_preds, val_mol_ids)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}