{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.basic_data import DataBunch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TYPES              = np.array(['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'])\n",
    "TYPES_MAP          = {t: i for i, t in enumerate(TYPES)}\n",
    "SC_EDGE_FEATS      = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "SC_MOL_FEATS       = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', \n",
    "                      #'inv_dist', 'normed_inv_dist', \n",
    "                      'std_bond_length', 'ave_bond_length', #'total_bond_length',  \n",
    "                      #'ave_inv_bond_length', 'total_inv_bond_length', \n",
    "                      'ave_atom_weight', #'total_atom_weight'\n",
    "                     ]\n",
    "ATOM_FEATS         = ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', \n",
    "                      'degree_1', 'degree_2', 'degree_3', 'degree_4', 'degree_5', \n",
    "                      'SP', 'SP2', 'SP3', 'hybridization_unspecified', \n",
    "                      'aromatic', 'formal_charge', 'atomic_num',\n",
    "                      'donor', 'acceptor', \n",
    "                      'ave_bond_length', \n",
    "                      #'ave_inv_bond_length',\n",
    "                      'ave_neighbor_weight']\n",
    "EDGE_FEATS         = ['single', 'double', 'triple', 'aromatic', \n",
    "                      'conjugated', 'in_ring',\n",
    "                      'dist', 'normed_dist', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "CONTRIB_COLS       = ['fc', 'sd', 'pso', 'dso']\n",
    "N_EDGE_FEATURES    = len(EDGE_FEATS)\n",
    "N_SC_EDGE_FEATURES = len(SC_EDGE_FEATS)\n",
    "N_SC_MOL_FEATURES  = len(SC_MOL_FEATS)\n",
    "N_ATOM_FEATURES    = len(ATOM_FEATS)\n",
    "N_TYPES            = len(TYPES)\n",
    "N_MOLS             = 130775\n",
    "SC_MEAN            = 16\n",
    "SC_STD             = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_FEATS_TO_SCALE   = ['dist', 'dist_min_rad', 'dist_electro_neg_adj', 'num_atoms', 'num_C_atoms', \n",
    "                       'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', 'inv_dist', \n",
    "                       'ave_bond_length', 'std_bond_length', 'total_bond_length',  'ave_inv_bond_length', \n",
    "                       'total_inv_bond_length', 'ave_atom_weight', 'total_atom_weight']\n",
    "ATOM_FEATS_TO_SCALE = ['atomic_num', 'ave_bond_length', 'ave_inv_bond_length', 'ave_neighbor_weight']\n",
    "EDGE_FEATS_TO_SCALE = ['dist', 'inv_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = '../tmp/'\n",
    "# DATA_PATH = '../input/champs-scalar-coupling/'\n",
    "# PATH = '../input/champs-processed-data-2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atomic_features.csv',\n",
       " 'train_proc_df.csv',\n",
       " 'mask.csv',\n",
       " 'edge_mask.csv',\n",
       " 'atom_df.csv',\n",
       " 'pairs_idx.csv',\n",
       " 'edge_df.csv',\n",
       " 'train_idxs_4_fold_cv.csv',\n",
       " 'edge_features.csv',\n",
       " 'val_idxs_4_fold_cv.csv',\n",
       " 'test_proc_df.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(PATH)\n",
    "files = [f for f in files if f.find('.csv') != -1]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalar_coupling_contributions.csv',\n",
       " 'mulliken_charges.csv',\n",
       " 'structures.csv',\n",
       " 'test.csv',\n",
       " 'train.csv',\n",
       " 'magnetic_shielding_tensors.csv',\n",
       " 'dipole_moments.csv',\n",
       " 'sample_submission.csv',\n",
       " 'potential_energy.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(DATA_PATH)\n",
    "files = [f for f in files if f.find('.csv') != -1]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/python36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(PATH+'train_proc_df.csv', index_col=0)\n",
    "test_df  = pd.read_csv(PATH+'test_proc_df.csv', index_col=0)\n",
    "atom_df  = pd.read_csv(PATH+'atom_df.csv', index_col=0)\n",
    "edge_df  = pd.read_csv(PATH+'edge_df.csv', index_col=0)\n",
    "contribs_df = pd.read_csv(DATA_PATH+'scalar_coupling_contributions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribs_df['molecule_id'] = train_df['molecule_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['scalar_coupling_constant'] = (train_df['scalar_coupling_constant'] - SC_MEAN) / SC_STD\n",
    "contribs_df['fc'] = contribs_df['fc'] - SC_MEAN\n",
    "contribs_df[CONTRIB_COLS] = contribs_df[CONTRIB_COLS] / SC_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['num_atoms'] = train_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                  'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "test_df['num_atoms'] = test_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "train_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10\n",
    "test_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MPNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General dense feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     5,
     12,
     18,
     27
    ]
   },
   "outputs": [],
   "source": [
    "def bn_init(m): pass\n",
    "#     if type(m) == nn.BatchNorm1d: \n",
    "#         nn.init.ones_(m.weight)\n",
    "#         nn.init.zeros_(m.bias)\n",
    "\n",
    "def selu_weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        fan_in = m.weight.size(1)\n",
    "        m.weight.data.normal_(0.0, 1.0 / math.sqrt(fan_in))\n",
    "        m.bias.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def relu_weights_init(m): \n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "#         m.bias.data.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def hidden_layer(n_in, n_out, batch_norm, dropout, layer_norm=False, act=None):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if act: layers.append(act)\n",
    "    if batch_norm: layers.append(nn.BatchNorm1d(n_out))\n",
    "    if layer_norm: layers.append(nn.LayerNorm(n_out))\n",
    "    if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "    return layers\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], batch_norm=False, \n",
    "                 out_act=None, final_bn=False, layer_norm=False):\n",
    "        super().__init__()\n",
    "        sizes = [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        layers_ = []\n",
    "        for i, (n_in, n_out, dr) in enumerate(zip(sizes[:-1], sizes[1:], dropout)):\n",
    "            act_ = act if i < len(layers) else out_act\n",
    "            batch_norm_ = batch_norm if i < len(layers) else final_bn\n",
    "            layer_norm_ = layer_norm if i < len(layers) else False\n",
    "            layers_ += hidden_layer(n_in, n_out, batch_norm_, dr, layer_norm_, act_)      \n",
    "        self.layers = nn.Sequential(*layers_)\n",
    "        if type(act) == nn.SELU: self.layers.apply(selu_weights_init)\n",
    "        else: self.layers.apply(relu_weights_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM cell as describedi in the set2set paper (https://arxiv.org/pdf/1511.06391.pdf). Doesn't take any inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class HiddenLSTMCell(nn.Module):\n",
    "    \"\"\"Implements the LSTM cell update described in the sec 4.2 of https://arxiv.org/pdf/1511.06391.pdf.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_h_out):\n",
    "        \"\"\"This LSTM cell takes no external 'x' inputs, but has a hidden state appended with the \n",
    "        readout from a content based attention mechanism. Therefore the hidden state is of a dimension\n",
    "        that is two times the number of nodes in the set.\"\"\"\n",
    "        super().__init__()\n",
    "        self.n_h_out, self.n_h = n_h_out, n_h_out * 2 \n",
    "        self.w_h = nn.Parameter(torch.Tensor(self.n_h, n_h_out * 4))\n",
    "        self.b = nn.Parameter(torch.Tensor(n_h_out * 4))\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "                # nn.init.orthogonal_(p.data)\n",
    "            else: \n",
    "                nn.init.zeros_(p.data)\n",
    "                # initialize the forget gate bias to 1\n",
    "                p.data[self.n_h_out:self.n_h_out*2] = torch.ones(self.n_h_out)\n",
    "        \n",
    "    def forward(self, h_prev, c_prev):\n",
    "        \"\"\"Takes previuos hidden and cell states as arguments and performs a \n",
    "        single LSTM step using no external input.\n",
    "        \"\"\"\n",
    "        n_h_ = self.n_h_out # number of output hidden states\n",
    "        # batch the computations into a single matrix multiplication\n",
    "        gates = h_prev @ self.w_h + self.b\n",
    "        i_g, f_g, g, o_g = (\n",
    "            torch.sigmoid(gates[:, :n_h_]), # input\n",
    "            torch.sigmoid(gates[:, n_h_:n_h_*2]), # forget\n",
    "            torch.tanh(gates[:, n_h_*2:n_h_*3]),\n",
    "            torch.sigmoid(gates[:, n_h_*3:]), # output\n",
    "        )\n",
    "        c = f_g * c_prev + i_g * g\n",
    "        h = o_g * torch.tanh(c)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set2set module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     6,
     11,
     41
    ]
   },
   "outputs": [],
   "source": [
    "def scatter_add(src, idx, num):\n",
    "    sz = num, src.size(1)\n",
    "    exp_idx = idx[:,None].repeat(1, sz[1])\n",
    "    out = torch.zeros(sz, dtype=src.dtype, device=src.device)\n",
    "    return out.scatter_add(0, exp_idx, src)\n",
    "\n",
    "def softmax(x, idx, num=None):\n",
    "    x = x.exp()\n",
    "    x = x / (scatter_add(x, idx, num=num)[idx] + 1e-16)\n",
    "    return x\n",
    "\n",
    "class Set2SetGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric\\\n",
    "        /nn/glob/set2set.html#Set2Set\n",
    "    \"\"\"\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.proc_steps = proc_steps\n",
    "        self.gru = nn.GRUCell(n_set_in, n_set_in)\n",
    "        self.init_q = nn.Parameter(torch.Tensor(1, n_set_in))\n",
    "        self.init_r = nn.Parameter(torch.Tensor(1, n_set_in))\n",
    "        nn.init.zeros_(self.init_q)\n",
    "        nn.init.zeros_(self.init_r)\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        q = self.init_q.expand(batch_size, -1).contiguous()\n",
    "        r = self.init_r.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q = self.gru(r, q)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_add(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "        return torch.cat([q, r], dim=-1) #q_star\n",
    "\n",
    "class Set2SetLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric\\\n",
    "        /nn/glob/set2set.html#Set2Set\n",
    "    \"\"\"\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.n_set_in, n_set_out = n_set_in, 2 * n_set_in\n",
    "        self.proc_steps = proc_steps\n",
    "        self.lstm = HiddenLSTMCell(n_set_in)\n",
    "        self.init_q_star = nn.Parameter(torch.Tensor(1, n_set_out))\n",
    "        self.init_h = nn.Parameter(torch.Tensor(1, n_set_in))\n",
    "        nn.init.zeros_(self.init_q_star)\n",
    "        nn.init.zeros_(self.init_h)\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        h = self.init_h.expand(batch_size, -1).contiguous()\n",
    "        q_star = self.init_q_star.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q, h = self.lstm(q_star, h)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_add(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "            q_star = torch.cat([q, r], dim=-1)\n",
    "            \n",
    "        return q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge network message function as described in the MPNN paper (https://arxiv.org/pdf/1704.01212.pdf). Adds in seperate edge network to allow messages to flow along scalar coupling edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EdgeNetwork(nn.Module):\n",
    "    def __init__(self, n_h, n_e, n_sc_e, net_args={}):\n",
    "        super().__init__()\n",
    "        self.n_h = n_h\n",
    "        self.adj_net = FullyConnectedNet(n_e, n_h**2, **net_args)\n",
    "        self.sc_adj_net = FullyConnectedNet(n_sc_e, n_h**2, **net_args)\n",
    "        self.b = nn.Parameter(torch.Tensor(n_h)) # bias for the message function\n",
    "        nn.init.zeros_(self.b)\n",
    "    \n",
    "    def forward(self, h, e, sc_e, pairs_idx, sc_pairs_idx, t=0):\n",
    "        \"\"\"\n",
    "        Compute message vector m_t given the previuos hidden state\n",
    "        h_t-1 and edge features e.\n",
    "        - h: tensor of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_e: tensor of scalar coupling edge features of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - t: update iteration. \n",
    "        \"\"\"\n",
    "        # compute 'A(e)'\n",
    "        if t==0: \n",
    "            self.a_mat = self.get_a_mat(self.adj_net(e))\n",
    "            self.a_sc_mat = self.get_a_mat(self.sc_adj_net(sc_e))\n",
    "            \n",
    "        # compute 'm_{i} = sum_{j in N(i)}(A_{ij}h_{j})' for all nodes 'i'\n",
    "        m = self.add_message(torch.zeros_like(h), self.a_mat, h, pairs_idx)\n",
    "        m = self.add_message(m, self.a_sc_mat, h, sc_pairs_idx)\n",
    "        return m + self.b # add message bias\n",
    "    \n",
    "    def get_a_mat(self, a_vect):\n",
    "        return a_vect.view(-1, self.n_h, self.n_h) / (self.n_h ** .5)\n",
    "    \n",
    "    def add_message(self, m, a, h, pairs_idx):\n",
    "        # transform 'pairs_idx' and 'a' to make messages go both in to and out of all nodes\n",
    "        in_out_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n",
    "        a_ = torch.cat((a, a)) \n",
    "        \n",
    "        # select the 'h_{j}' feeding into the 'm_{i}'\n",
    "        h_in = h.index_select(0, in_out_idx[:,1])\n",
    "        \n",
    "        # do the matrix multiplication 'A_{ij}h_{j}'\n",
    "        ah = (h_in.unsqueeze(1) @ a_).squeeze(1)\n",
    "        \n",
    "        # Sum up all 'A_{ij}h_{j}' per node 'i'\n",
    "        return m.scatter_add(0, in_out_idx[:,0,None].repeat(1, self.n_h), ah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU update function as described in the MPNN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GRUUpdate(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(n_h, n_h)\n",
    "        \n",
    "    def forward(self, m, h_prev):\n",
    "        \"\"\"\n",
    "        Update hidden state h.\n",
    "        - h_prev is vector of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - m is vector of messages of shape (batch_size * n_nodes, n_h)\n",
    "        \"\"\"\n",
    "        return self.gru(m, h_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom readout network following th set2set processing stage. Allows some final specialization/fine-tuning for each scalar coupling type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contribs_head(n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "    layers = hidden_layer(n_in, n_h, False, dropout, layer_norm, act)\n",
    "    layers += hidden_layer(n_h, 1, False, 0.0) # output layer\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ContribsHead(nn.Module):\n",
    "    N_CONTRIBS = 5\n",
    "    CONTIB_SCALES = [1, 250, 45, 35, 500]\n",
    "    def __init__(self, n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            create_contribs_head(n_in, n_h, act, dropout, layer_norm) for _ in range(self.N_CONTRIBS)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ys = torch.cat([b(x) / s for b, s in zip(self.blocks, self.CONTIB_SCALES)], dim=-1)\n",
    "        return torch.cat([ys[:,:-1], ys.sum(dim=-1, keepdim=True)], dim=-1)\n",
    "\n",
    "class MyCustomHead(nn.Module):\n",
    "    N_OUTPUTS = 5\n",
    "    def __init__(self, n_input, n_h_contribs, pre_layers=[], post_layers=[], \n",
    "                 act=nn.ReLU(True), dropout=[], batch_norm=False):\n",
    "        super().__init__()\n",
    "        n_pre_layers = len(pre_layers)\n",
    "        self.preproc = FullyConnectedNet(n_input, None, pre_layers, act, \n",
    "                                         dropout[:n_pre_layers], batch_norm)\n",
    "        self.postproc = nn.ModuleList([\n",
    "            nn.Sequential(*[\n",
    "                FullyConnectedNet(pre_layers[-1], None, post_layers, act, \n",
    "                                  dropout[n_pre_layers:-1], batch_norm=False, \n",
    "                                  layer_norm=batch_norm),\n",
    "                ContribsHead(post_layers[-1], n_h_contribs, act, dropout[-1], batch_norm)\n",
    "            ])\n",
    "            for _ in range(N_TYPES)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, sc_types):\n",
    "        x_ = self.preproc(x)\n",
    "        y = torch.zeros(sc_types.size(0), self.N_OUTPUTS, device=x.device)\n",
    "        for i in range(N_TYPES):\n",
    "            if torch.any(sc_types == i): \n",
    "                y[sc_types == i] = self.postproc[i](x_[sc_types == i])\n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines all the the components of the readout function described in the MPNN network using set2set processing and the scalar coupling type customized head. Also adds in some skip connections to final node states and scalar coupling input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Set2SetOutput(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_sc_m, proc_steps, net_args):\n",
    "        super().__init__()\n",
    "        self.R_proj = nn.Linear(n_h + n_x, n_h)\n",
    "        self.R_proc = Set2SetGRU(n_h, proc_steps)\n",
    "        self.R_write = MyCustomHead((4 * n_h) + n_sc_m, **net_args)\n",
    "    \n",
    "    def forward(self, h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types):\n",
    "        \"\"\"\n",
    "        Make prediction.\n",
    "        - h is vector of hidden states of shape (batch_size * n_nodes, n_h).\n",
    "        - x is vector of input features of shape (batch_size * n_nodes, n_x).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        m = self.R_proj(torch.cat([h, x], dim=1))\n",
    "        q = self.R_proc(m, node_idx)\n",
    "        \n",
    "        # introduce skip connection to final node states of scalar coupling atoms\n",
    "        inp = torch.cat([\n",
    "            q.index_select(0, sc_idx),\n",
    "            h.index_select(0, sc_pairs_idx[:,0]),\n",
    "            h.index_select(0, sc_pairs_idx[:,1]),\n",
    "            sc_m\n",
    "        ], dim=-1)\n",
    "        y = self.R_write(inp, sc_types)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines the edge-network message (M), GRU update (U) and set2set readout (R) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps=3, proc_steps=10, \n",
    "                 preproc_net_args={}, enn_args={}, R_net_args={}):\n",
    "        super().__init__()\n",
    "        self.preproc_net = FullyConnectedNet(n_x, n_h, **preproc_net_args)\n",
    "        self.M = EdgeNetwork(n_h, n_e, n_sc_e, enn_args)\n",
    "        self.U = GRUUpdate(n_h)\n",
    "        self.R = Set2SetOutput(n_x, n_h, n_sc_m, proc_steps, R_net_args)\n",
    "        self.update_steps = update_steps\n",
    "        \n",
    "    def forward(self, x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: tensor of node features of shape (batch_size * n_nodes, n_x).\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_e: tensor of scalar coupling edge features of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        h = self.preproc_net(x)\n",
    "        for t in range(self.update_steps):\n",
    "            m = self.M(h, e, sc_e, pairs_idx, sc_pairs_idx, t)\n",
    "            h = self.U(m, h)\n",
    "        y = self.R(h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ids = train_df['molecule_id'].unique()\n",
    "n_obs = len(mol_ids)\n",
    "split = int(n_obs*0.75)\n",
    "set_seed(100)\n",
    "mol_ids_ = np.random.choice(mol_ids, size=n_obs, replace=False)\n",
    "train_mol_ids, val_mol_ids = pd.Series(mol_ids_[:split]), pd.Series(mol_ids_[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df, features, train_mol_ids):\n",
    "    idx = df['molecule_id'].isin(train_mol_ids)\n",
    "    return df.loc[idx, features].mean(), df.loc[idx, features].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(train_df[SC_FEATS_TO_SCALE].mean().abs()>0.1) or any((train_df[SC_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    sc_feat_means, sc_feat_stds = scale_features(train_df, SC_FEATS_TO_SCALE, train_mol_ids)\n",
    "    train_df[SC_FEATS_TO_SCALE] = (train_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "    test_df[SC_FEATS_TO_SCALE] = (test_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "if any(atom_df[ATOM_FEATS_TO_SCALE].mean().abs()>0.1) or any((atom_df[ATOM_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    atom_feat_means, atom_feat_stds = scale_features(atom_df, ATOM_FEATS_TO_SCALE, train_mol_ids)\n",
    "    atom_df[ATOM_FEATS_TO_SCALE] = (atom_df[ATOM_FEATS_TO_SCALE] - atom_feat_means) / atom_feat_stds\n",
    "if any(edge_df[EDGE_FEATS_TO_SCALE].mean().abs()>0.1) or any((edge_df[EDGE_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    edge_feat_means, edge_feat_stds = scale_features(edge_df, EDGE_FEATS_TO_SCALE, train_mol_ids)\n",
    "    edge_df[EDGE_FEATS_TO_SCALE] = (edge_df[EDGE_FEATS_TO_SCALE] - edge_feat_means) / edge_feat_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mol_sc = train_df.groupby('molecule_id')\n",
    "test_gb_mol_sc = test_df.groupby('molecule_id')\n",
    "gb_mol_contribs = contribs_df.groupby('molecule_id')\n",
    "gb_mol_atom = atom_df.groupby('molecule_id')\n",
    "gb_mol_edge = edge_df.groupby('molecule_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pytorch dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0,
     6,
     25,
     28
    ]
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_np(x):\n",
    "    sz = len(x), len(np.unique(x))\n",
    "    x_one_hot = np.zeros(sz, dtype=np.long)\n",
    "    x_one_hot[np.arange(sz[0]), x] = 1\n",
    "    return x_one_hot\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mol_ids, gb_mol_sc, gb_mol_contribs, gb_mol_atom, gb_mol_edge):\n",
    "        self.n = len(mol_ids)\n",
    "        self.mol_ids = mol_ids\n",
    "        self.gb_mol_sc = gb_mol_sc\n",
    "        self.gb_mol_contribs = gb_mol_contribs\n",
    "        self.gb_mol_atom = gb_mol_atom\n",
    "        self.gb_mol_edge = gb_mol_edge\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gb_mol_sc.get_group(self.mol_ids[idx]),\n",
    "                self.gb_mol_contribs.get_group(self.mol_ids[idx]),\n",
    "                self.gb_mol_atom.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_edge.get_group(self.mol_ids[idx]))\n",
    "\n",
    "def np_lst_to_torch(arr_lst, dtype=torch.float):\n",
    "    return torch.from_numpy(np.ascontiguousarray(np.concatenate(arr_lst))).type(dtype)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_size, n_atom_sum = len(batch), 0\n",
    "    x, e, sc_e, sc_m = [], [], [], []\n",
    "    sc_types, sc_vals, sc_contribs = [], [], []\n",
    "    node_idx, pairs_idx, sc_pairs_idx, sc_idx = [], [], [], []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        sc_df, contribs_df, atom_df, edge_df = batch[b]\n",
    "        n_atoms, n_sc = len(atom_df), len(sc_df)\n",
    "        \n",
    "        x.append(atom_df[ATOM_FEATS].values)\n",
    "        e.append(edge_df[EDGE_FEATS].values)\n",
    "        sc_e.append(sc_df[SC_EDGE_FEATS].values)\n",
    "        sc_m.append(sc_df[SC_MOL_FEATS].values)\n",
    "        sc_types.append(sc_df['type'].values)\n",
    "        sc_vals.append(sc_df['scalar_coupling_constant'].values)\n",
    "        sc_contribs.append(contribs_df[CONTRIB_COLS].values)\n",
    "        \n",
    "        node_idx.append(np.repeat(b, n_atoms))\n",
    "        sc_idx.append(np.repeat(b, n_sc))\n",
    "        pairs_idx.append(edge_df[['idx_0', 'idx_1']].values + n_atom_sum)\n",
    "        sc_pairs_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values + n_atom_sum)\n",
    "        \n",
    "        n_atom_sum += n_atoms\n",
    "    \n",
    "    x, e = np_lst_to_torch(x), np_lst_to_torch(e), \n",
    "    sc_e, sc_m = np_lst_to_torch(sc_e), np_lst_to_torch(sc_m)\n",
    "    sc_vals = np_lst_to_torch(sc_vals)\n",
    "    sc_types = np_lst_to_torch(sc_types, torch.long)\n",
    "    sc_contribs = np_lst_to_torch(sc_contribs)\n",
    "    node_idx = np_lst_to_torch(node_idx, torch.long)\n",
    "    sc_idx = np_lst_to_torch(sc_idx, torch.long)\n",
    "    pairs_idx = np_lst_to_torch(pairs_idx, torch.long)\n",
    "    sc_pairs_idx = np_lst_to_torch(sc_pairs_idx, torch.long)\n",
    "    \n",
    "    y = torch.cat([sc_contribs, sc_vals.view(-1,1)], dim=-1)\n",
    "    \n",
    "    return (x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MoleculeDataset(train_mol_ids, gb_mol_sc, gb_mol_contribs, gb_mol_atom, gb_mol_edge)\n",
    "val_ds   = MoleculeDataset(val_mol_ids, gb_mol_sc, gb_mol_contribs, gb_mol_atom, gb_mol_edge)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size, num_workers=8, drop_last=True)\n",
    "db = DataBunch(train_dl, val_dl, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([375, 21])\n",
      "torch.Size([387, 8])\n",
      "torch.Size([1144, 16])\n",
      "torch.Size([1144, 25])\n",
      "torch.Size([375])\n",
      "torch.Size([387, 2])\n",
      "torch.Size([1144])\n",
      "torch.Size([1144, 2])\n",
      "torch.Size([1144])\n",
      "torch.Size([1144, 5])\n"
     ]
    }
   ],
   "source": [
    "for el in batch[0]: print(el.size())\n",
    "print(batch[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 0.0000,  1.0000,  0.0000,  ...,  0.8485, -2.6219, -0.3054],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ...,  0.7292, -0.1759, -0.3054],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ...,  0.8494, -2.6219, -0.3054],\n",
      "        ...,\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9419,  0.4355, -0.3054],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9272,  0.4355, -0.3054],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9257,  0.4355, -0.3054]])\n",
      "e:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  1.3088,  1.3957],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8758, -0.7402],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8752, -0.7397],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.6314,  0.9587],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.1754,  0.2646],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9483, -1.4458]])\n",
      "sc_e:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3127],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.3438,  0.7481, -0.7354],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.4088, -0.8878],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.9860, -0.9776],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4963,  0.8363,  0.8911],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.4963]])\n",
      "sc_m:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.2789, -0.8423, -1.5555],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.2789, -0.8423, -1.5555],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.2789, -0.8423, -1.5555],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -3.9886,  0.0144,  3.3345],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -3.9886,  0.0144,  3.3345],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -3.9886,  0.0144,  3.3345]])\n",
      "node_idx:\n",
      " tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19])\n",
      "pairs_idx:\n",
      " tensor([[  0,   1],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  1,   2],\n",
      "        [  1,   3],\n",
      "        [  1,  12],\n",
      "        [  2,  13],\n",
      "        [  2,  14],\n",
      "        [  2,  15],\n",
      "        [  3,   4],\n",
      "        [  4,  16],\n",
      "        [  4,   7],\n",
      "        [  4,   5],\n",
      "        [  5,   6],\n",
      "        [  5,  17],\n",
      "        [  5,  18],\n",
      "        [  6,   7],\n",
      "        [  6,  19],\n",
      "        [  6,  20],\n",
      "        [  7,   8],\n",
      "        [  7,  21],\n",
      "        [  8,  22],\n",
      "        [  8,  23],\n",
      "        [  8,  24],\n",
      "        [ 25,  26],\n",
      "        [ 25,  34],\n",
      "        [ 25,  35],\n",
      "        [ 25,  36],\n",
      "        [ 26,  27],\n",
      "        [ 26,  31],\n",
      "        [ 27,  28],\n",
      "        [ 28,  29],\n",
      "        [ 28,  32],\n",
      "        [ 29,  30],\n",
      "        [ 29,  37],\n",
      "        [ 30,  31],\n",
      "        [ 30,  38],\n",
      "        [ 32,  33],\n",
      "        [ 39,  40],\n",
      "        [ 39,  47],\n",
      "        [ 39,  48],\n",
      "        [ 39,  49],\n",
      "        [ 40,  41],\n",
      "        [ 40,  42],\n",
      "        [ 41,  42],\n",
      "        [ 41,  50],\n",
      "        [ 41,  51],\n",
      "        [ 42,  46],\n",
      "        [ 42,  43],\n",
      "        [ 43,  44],\n",
      "        [ 43,  52],\n",
      "        [ 43,  53],\n",
      "        [ 44,  45],\n",
      "        [ 44,  54],\n",
      "        [ 44,  55],\n",
      "        [ 45,  46],\n",
      "        [ 46,  56],\n",
      "        [ 46,  57],\n",
      "        [ 58,  59],\n",
      "        [ 58,  67],\n",
      "        [ 58,  68],\n",
      "        [ 58,  69],\n",
      "        [ 59,  60],\n",
      "        [ 59,  65],\n",
      "        [ 60,  61],\n",
      "        [ 61,  62],\n",
      "        [ 61,  63],\n",
      "        [ 62,  70],\n",
      "        [ 63,  64],\n",
      "        [ 63,  71],\n",
      "        [ 64,  65],\n",
      "        [ 64,  72],\n",
      "        [ 65,  66],\n",
      "        [ 73,  74],\n",
      "        [ 73,  82],\n",
      "        [ 73,  83],\n",
      "        [ 73,  84],\n",
      "        [ 74,  75],\n",
      "        [ 74,  85],\n",
      "        [ 74,  86],\n",
      "        [ 75,  76],\n",
      "        [ 75,  78],\n",
      "        [ 75,  81],\n",
      "        [ 76,  87],\n",
      "        [ 76,  88],\n",
      "        [ 76,  77],\n",
      "        [ 77,  89],\n",
      "        [ 77,  90],\n",
      "        [ 77,  91],\n",
      "        [ 78,  79],\n",
      "        [ 78,  92],\n",
      "        [ 78,  93],\n",
      "        [ 79,  80],\n",
      "        [ 79,  81],\n",
      "        [ 79,  94],\n",
      "        [ 80,  96],\n",
      "        [ 80,  81],\n",
      "        [ 80,  95],\n",
      "        [ 81,  97],\n",
      "        [ 98,  99],\n",
      "        [ 98, 107],\n",
      "        [ 98, 108],\n",
      "        [ 98, 109],\n",
      "        [ 99, 100],\n",
      "        [ 99, 102],\n",
      "        [ 99, 110],\n",
      "        [100, 101],\n",
      "        [100, 102],\n",
      "        [100, 111],\n",
      "        [101, 112],\n",
      "        [102, 103],\n",
      "        [102, 106],\n",
      "        [103, 104],\n",
      "        [103, 113],\n",
      "        [103, 114],\n",
      "        [104, 105],\n",
      "        [104, 106],\n",
      "        [104, 115],\n",
      "        [105, 106],\n",
      "        [106, 116],\n",
      "        [117, 118],\n",
      "        [117, 126],\n",
      "        [117, 127],\n",
      "        [117, 128],\n",
      "        [118, 119],\n",
      "        [118, 124],\n",
      "        [119, 120],\n",
      "        [119, 129],\n",
      "        [120, 121],\n",
      "        [120, 123],\n",
      "        [121, 122],\n",
      "        [121, 130],\n",
      "        [121, 131],\n",
      "        [122, 132],\n",
      "        [123, 124],\n",
      "        [124, 125],\n",
      "        [133, 134],\n",
      "        [133, 142],\n",
      "        [133, 143],\n",
      "        [133, 144],\n",
      "        [134, 135],\n",
      "        [134, 145],\n",
      "        [134, 146],\n",
      "        [135, 136],\n",
      "        [135, 147],\n",
      "        [135, 148],\n",
      "        [136, 137],\n",
      "        [137, 141],\n",
      "        [137, 140],\n",
      "        [137, 138],\n",
      "        [138, 139],\n",
      "        [138, 149],\n",
      "        [138, 150],\n",
      "        [139, 151],\n",
      "        [139, 152],\n",
      "        [139, 153],\n",
      "        [140, 141],\n",
      "        [140, 154],\n",
      "        [140, 155],\n",
      "        [141, 156],\n",
      "        [141, 157],\n",
      "        [158, 159],\n",
      "        [158, 167],\n",
      "        [158, 168],\n",
      "        [158, 169],\n",
      "        [159, 160],\n",
      "        [159, 161],\n",
      "        [159, 170],\n",
      "        [160, 171],\n",
      "        [160, 161],\n",
      "        [161, 162],\n",
      "        [161, 165],\n",
      "        [162, 163],\n",
      "        [162, 172],\n",
      "        [162, 173],\n",
      "        [163, 164],\n",
      "        [164, 174],\n",
      "        [165, 166],\n",
      "        [165, 175],\n",
      "        [176, 177],\n",
      "        [176, 184],\n",
      "        [176, 185],\n",
      "        [176, 186],\n",
      "        [177, 178],\n",
      "        [177, 187],\n",
      "        [177, 188],\n",
      "        [178, 189],\n",
      "        [178, 182],\n",
      "        [178, 179],\n",
      "        [179, 180],\n",
      "        [179, 190],\n",
      "        [179, 191],\n",
      "        [180, 181],\n",
      "        [181, 192],\n",
      "        [181, 193],\n",
      "        [181, 194],\n",
      "        [182, 183],\n",
      "        [195, 196],\n",
      "        [195, 204],\n",
      "        [195, 205],\n",
      "        [195, 206],\n",
      "        [196, 197],\n",
      "        [196, 203],\n",
      "        [197, 198],\n",
      "        [197, 203],\n",
      "        [197, 207],\n",
      "        [198, 208],\n",
      "        [198, 199],\n",
      "        [198, 200],\n",
      "        [199, 200],\n",
      "        [199, 209],\n",
      "        [200, 201],\n",
      "        [200, 203],\n",
      "        [201, 210],\n",
      "        [201, 202],\n",
      "        [203, 211],\n",
      "        [212, 213],\n",
      "        [212, 221],\n",
      "        [212, 222],\n",
      "        [212, 223],\n",
      "        [213, 214],\n",
      "        [213, 215],\n",
      "        [213, 224],\n",
      "        [214, 225],\n",
      "        [214, 226],\n",
      "        [214, 227],\n",
      "        [215, 228],\n",
      "        [215, 219],\n",
      "        [215, 216],\n",
      "        [216, 217],\n",
      "        [216, 229],\n",
      "        [216, 230],\n",
      "        [217, 218],\n",
      "        [217, 219],\n",
      "        [217, 231],\n",
      "        [218, 219],\n",
      "        [219, 220],\n",
      "        [220, 232],\n",
      "        [220, 233],\n",
      "        [220, 234],\n",
      "        [235, 236],\n",
      "        [235, 244],\n",
      "        [235, 245],\n",
      "        [235, 246],\n",
      "        [236, 237],\n",
      "        [236, 240],\n",
      "        [236, 242],\n",
      "        [237, 248],\n",
      "        [237, 247],\n",
      "        [237, 238],\n",
      "        [238, 239],\n",
      "        [238, 249],\n",
      "        [238, 250],\n",
      "        [239, 240],\n",
      "        [240, 241],\n",
      "        [241, 251],\n",
      "        [242, 243],\n",
      "        [243, 252],\n",
      "        [253, 254],\n",
      "        [253, 262],\n",
      "        [254, 255],\n",
      "        [254, 263],\n",
      "        [255, 256],\n",
      "        [256, 257],\n",
      "        [256, 260],\n",
      "        [256, 261],\n",
      "        [257, 264],\n",
      "        [257, 259],\n",
      "        [257, 258],\n",
      "        [258, 259],\n",
      "        [258, 261],\n",
      "        [258, 265],\n",
      "        [259, 260],\n",
      "        [259, 266],\n",
      "        [260, 261],\n",
      "        [260, 267],\n",
      "        [261, 268],\n",
      "        [269, 270],\n",
      "        [269, 278],\n",
      "        [269, 279],\n",
      "        [269, 280],\n",
      "        [270, 271],\n",
      "        [270, 276],\n",
      "        [270, 281],\n",
      "        [271, 272],\n",
      "        [272, 282],\n",
      "        [272, 283],\n",
      "        [272, 273],\n",
      "        [273, 274],\n",
      "        [274, 275],\n",
      "        [274, 277],\n",
      "        [274, 284],\n",
      "        [275, 276],\n",
      "        [275, 285],\n",
      "        [275, 286],\n",
      "        [276, 277],\n",
      "        [276, 287],\n",
      "        [277, 288],\n",
      "        [277, 289],\n",
      "        [290, 291],\n",
      "        [290, 299],\n",
      "        [291, 292],\n",
      "        [291, 295],\n",
      "        [292, 293],\n",
      "        [292, 300],\n",
      "        [292, 301],\n",
      "        [293, 296],\n",
      "        [293, 302],\n",
      "        [293, 294],\n",
      "        [294, 295],\n",
      "        [294, 303],\n",
      "        [294, 304],\n",
      "        [296, 297],\n",
      "        [296, 298],\n",
      "        [296, 305],\n",
      "        [297, 307],\n",
      "        [297, 298],\n",
      "        [297, 306],\n",
      "        [298, 308],\n",
      "        [309, 310],\n",
      "        [309, 318],\n",
      "        [309, 319],\n",
      "        [309, 320],\n",
      "        [310, 311],\n",
      "        [310, 313],\n",
      "        [310, 321],\n",
      "        [311, 315],\n",
      "        [311, 322],\n",
      "        [311, 312],\n",
      "        [312, 313],\n",
      "        [312, 323],\n",
      "        [313, 314],\n",
      "        [315, 316],\n",
      "        [315, 317],\n",
      "        [315, 324],\n",
      "        [316, 326],\n",
      "        [316, 317],\n",
      "        [316, 325],\n",
      "        [317, 327],\n",
      "        [328, 329],\n",
      "        [329, 330],\n",
      "        [329, 337],\n",
      "        [330, 331],\n",
      "        [330, 338],\n",
      "        [331, 332],\n",
      "        [331, 339],\n",
      "        [331, 340],\n",
      "        [332, 333],\n",
      "        [332, 334],\n",
      "        [334, 335],\n",
      "        [334, 341],\n",
      "        [334, 342],\n",
      "        [335, 336],\n",
      "        [336, 343],\n",
      "        [344, 345],\n",
      "        [344, 353],\n",
      "        [345, 346],\n",
      "        [345, 354],\n",
      "        [345, 355],\n",
      "        [346, 347],\n",
      "        [346, 352],\n",
      "        [347, 348],\n",
      "        [347, 356],\n",
      "        [348, 351],\n",
      "        [348, 349],\n",
      "        [348, 350],\n",
      "        [349, 350],\n",
      "        [349, 357],\n",
      "        [349, 358],\n",
      "        [350, 359],\n",
      "        [350, 360],\n",
      "        [351, 352],\n",
      "        [352, 361],\n",
      "        [352, 362],\n",
      "        [363, 364],\n",
      "        [363, 372],\n",
      "        [364, 365],\n",
      "        [365, 366],\n",
      "        [366, 367],\n",
      "        [367, 368],\n",
      "        [367, 371],\n",
      "        [368, 369],\n",
      "        [368, 373],\n",
      "        [369, 370],\n",
      "        [370, 371],\n",
      "        [371, 374]])\n",
      "sc_idx:\n",
      " tensor([ 0,  0,  0,  ..., 19, 19, 19])\n",
      "sc_pairs_idx:\n",
      " tensor([[  9,   0],\n",
      "        [  9,   1],\n",
      "        [  9,   2],\n",
      "        ...,\n",
      "        [374, 368],\n",
      "        [374, 370],\n",
      "        [374, 371]])\n",
      "sc_types:\n",
      " tensor([0, 4, 6,  ..., 6, 3, 0])\n",
      "y:\n",
      " tensor([[ 1.9369e+00,  5.4106e-03,  2.3206e-02,  2.1021e-02,  1.9866e+00],\n",
      "        [-5.5558e-01,  2.5421e-03, -9.5274e-04,  1.8560e-04, -5.5380e-01],\n",
      "        [-3.5432e-01, -9.4523e-04, -7.8983e-04,  7.4180e-04, -3.5532e-01],\n",
      "        ...,\n",
      "        [-3.5487e-01, -3.2623e-03,  4.8877e-03, -1.7251e-02, -3.7050e-01],\n",
      "        [-8.3000e-02, -1.1509e-03, -2.1405e-02, -3.2985e-03, -1.0885e-01],\n",
      "        [ 3.3917e+00,  9.3996e-03, -4.2295e-03,  2.8833e-02,  3.4257e+00]])\n"
     ]
    }
   ],
   "source": [
    "b_dict = dict(x=batch[0][0], \n",
    "              e=batch[0][1], \n",
    "              sc_e=batch[0][2], \n",
    "              sc_m=batch[0][3], \n",
    "              node_idx=batch[0][4], \n",
    "              pairs_idx=batch[0][5], \n",
    "              sc_idx=batch[0][6], \n",
    "              sc_pairs_idx=batch[0][7], \n",
    "              sc_types=batch[0][8], \n",
    "              y=batch[1])\n",
    "for k,v in b_dict.items(): print(f'{k}:\\n {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the metric used for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types):\n",
    "    proc = lambda x: x.cpu().numpy().ravel() \n",
    "    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n",
    "    y_true = SC_MEAN + y_true * SC_STD\n",
    "    y_pred = SC_MEAN + y_pred * SC_STD\n",
    "    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes).mean()\n",
    "\n",
    "class GroupMeanLogMAE(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['group_mean_log_mae'])\n",
    "    def on_epoch_begin(self, **kwargs): self.input, self.output, self.target = [], [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, last_input, train, **kwargs):\n",
    "        if not train:\n",
    "            self.input.append(last_input[-1])\n",
    "            self.output.append(last_output[:,-1])\n",
    "            self.target.append(last_target[:,-1])\n",
    "                \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if (len(self.input) > 0) and (len(self.output) > 0):\n",
    "            inputs = torch.cat(self.input)\n",
    "            preds = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            metric = group_mean_log_mae(preds, target, inputs)\n",
    "            return add_metrics(last_metrics, [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contribs_rmse_loss(preds, targs):\n",
    "    \"\"\"\n",
    "    Returns the sum of RMSEs for each sc contribution and total sc values.\n",
    "    \n",
    "    Args:\n",
    "        - preds: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            predictions. Last column is the total scalar coupling value.\n",
    "        - targs: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            true values. Last column is the total scalar coupling value.\n",
    "    \"\"\"\n",
    "    return torch.mean((preds - targs) ** 2, dim=0).sqrt().sum()\n",
    "\n",
    "def rmse(preds, targs):\n",
    "    return torch.sqrt(F.mse_loss(preds[:,-1], targs[:,-1]))\n",
    "\n",
    "def mae(preds, targs):\n",
    "    return torch.abs(preds[:,-1] - targs[:,-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd, batch_norm, act = 0, True, nn.ReLU(True)\n",
    "update_steps, proc_steps = 5, 10\n",
    "n_x, n_h, n_e, n_sc_e, n_sc_m = N_ATOM_FEATURES, 150, N_EDGE_FEATURES, N_SC_EDGE_FEATURES, N_SC_MOL_FEATURES\n",
    "preproc_net_args = dict(layers=[], act=act, dropout=[], batch_norm=batch_norm, out_act=nn.Tanh())\n",
    "enn_args = dict(layers=3*[n_h], act=act, dropout=3*[0.0], batch_norm=batch_norm)\n",
    "R_net_args = dict(pre_layers=[1000], post_layers=[500], n_h_contribs=250, act=act, dropout=[0.0, 0.0, 0.0], \n",
    "                  batch_norm=batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "model = MPNN(n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps, proc_steps, preproc_net_args, enn_args, R_net_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNN(\n",
      "  (preproc_net): FullyConnectedNet(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=21, out_features=150, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (M): EdgeNetwork(\n",
      "    (adj_net): FullyConnectedNet(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=150, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (4): ReLU(inplace)\n",
      "        (5): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (7): ReLU(inplace)\n",
      "        (8): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): Linear(in_features=150, out_features=22500, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sc_adj_net): FullyConnectedNet(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=150, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (4): ReLU(inplace)\n",
      "        (5): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (7): ReLU(inplace)\n",
      "        (8): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): Linear(in_features=150, out_features=22500, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (U): GRUUpdate(\n",
      "    (gru): GRUCell(150, 150)\n",
      "  )\n",
      "  (R): Set2SetOutput(\n",
      "    (R_proj): Linear(in_features=171, out_features=150, bias=True)\n",
      "    (R_proc): Set2SetGRU(\n",
      "      (gru): GRUCell(150, 150)\n",
      "    )\n",
      "    (R_write): MyCustomHead(\n",
      "      (preproc): FullyConnectedNet(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=625, out_features=1000, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (postproc): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): FullyConnectedNet(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "              (1): ReLU(inplace)\n",
      "              (2): LayerNorm(torch.Size([500]), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): ContribsHead(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (3): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Linear(in_features=500, out_features=250, bias=True)\n",
      "                (1): ReLU(inplace)\n",
      "                (2): LayerNorm(torch.Size([250]), eps=1e-05, elementwise_affine=True)\n",
      "                (3): Linear(in_features=250, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.2394e-02,  2.2614e-03, -1.9877e-03, -1.5119e-02,  3.9189e-02],\n",
      "        [ 5.6019e-01,  3.0978e-04, -6.0807e-03, -3.7116e-02,  5.1921e-01],\n",
      "        [ 5.9761e-01, -3.5234e-04,  1.6543e-03,  6.2727e-03,  6.0450e-01],\n",
      "        ...,\n",
      "        [-6.9747e-01, -3.6514e-04, -1.0818e-02,  3.2957e-03, -7.0594e-01],\n",
      "        [ 2.2892e-01,  1.8530e-05, -1.8875e-02, -6.0368e-03,  2.0476e-01],\n",
      "        [ 6.9519e-02,  2.7022e-03,  1.1453e-02,  1.3608e-02,  9.7060e-02]],\n",
      "       grad_fn=<IndexPutBackward>)\n",
      "torch.Size([1144, 5])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model(*batch[0]))\n",
    "print(model(*batch[0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics=[rmse, mae], callback_fns=GroupMeanLogMAE, \n",
    "                wd=wd, loss_func=contribs_rmse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-6, end_lr=1.0, num_it=100, stop_div=True)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.00% [9/10 1:48:35<12:03]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>group_mean_log_mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.059328</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.019128</td>\n",
       "      <td>-0.373528</td>\n",
       "      <td>12:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.055478</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>0.022460</td>\n",
       "      <td>0.015055</td>\n",
       "      <td>-0.716924</td>\n",
       "      <td>12:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.045403</td>\n",
       "      <td>0.048092</td>\n",
       "      <td>0.022911</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>-0.802774</td>\n",
       "      <td>12:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036997</td>\n",
       "      <td>0.034989</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>-0.972827</td>\n",
       "      <td>11:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031293</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>-1.231052</td>\n",
       "      <td>11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026620</td>\n",
       "      <td>0.026594</td>\n",
       "      <td>0.012524</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>-1.297033</td>\n",
       "      <td>11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022169</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>-1.537904</td>\n",
       "      <td>11:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>-1.725241</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016237</td>\n",
       "      <td>0.018353</td>\n",
       "      <td>0.008536</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>-1.829478</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1674' class='' max='3187', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      52.53% [1674/3187 05:11<04:41 0.0154]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with group_mean_log_mae value: -0.373527854681015.\n",
      "Better model found at epoch 1 with group_mean_log_mae value: -0.7169235348701477.\n",
      "Better model found at epoch 2 with group_mean_log_mae value: -0.80277419090271.\n",
      "Better model found at epoch 3 with group_mean_log_mae value: -0.9728274941444397.\n",
      "Better model found at epoch 4 with group_mean_log_mae value: -1.2310518026351929.\n",
      "Better model found at epoch 5 with group_mean_log_mae value: -1.2970329523086548.\n",
      "Better model found at epoch 6 with group_mean_log_mae value: -1.5379042625427246.\n",
      "Better model found at epoch 7 with group_mean_log_mae value: -1.7252414226531982.\n",
      "Better model found at epoch 8 with group_mean_log_mae value: -1.8294782638549805.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, max_lr=1e-3, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  name='mpnn1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(skip_start=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, max_lr=3e-4, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  name='mpnn2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, max_lr=1e-4, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  name='mpnn3')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
