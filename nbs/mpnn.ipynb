{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.basic_data import DataBunch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TYPES              = np.array(['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'])\n",
    "TYPES_MAP          = {t: i for i, t in enumerate(TYPES)}\n",
    "SC_EDGE_FEATS      = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', 'dist',\n",
    "                      'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', 'diangle', 'cos_angle', \n",
    "                      'cos_angle0', 'cos_angle1']\n",
    "SC_MOL_FEATS       = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', 'dist',\n",
    "                      'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', 'diangle', 'cos_angle', \n",
    "                      'cos_angle0', 'cos_angle1', 'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                      'num_N_atoms', 'num_O_atoms']\n",
    "N_EDGE_FEATURES    = 10\n",
    "N_SC_EDGE_FEATURES = 18\n",
    "N_SC_MOL_FEATURES  = 31\n",
    "N_ATOM_FEATURES    = 22\n",
    "N_TYPES            = len(TYPES)\n",
    "N_MOLS             = 130775\n",
    "SC_MEAN            = 16\n",
    "SC_STD             = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = '../tmp/'\n",
    "PATH = '../input/champs-processed-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_proc_df.csv', 'test_proc_df.csv', 'atom_df.csv', 'edge_df.csv']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(PATH)\n",
    "files = [f for f in files if f.find('.csv') != -1]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH+'train_proc_df.csv', index_col=0)\n",
    "test_df  = pd.read_csv(PATH+'test_proc_df.csv', index_col=0)\n",
    "atom_df  = pd.read_csv(PATH+'atom_df.csv', index_col=0)\n",
    "edge_df  = pd.read_csv(PATH+'edge_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['scalar_coupling_constant'] = (train_df['scalar_coupling_constant'] - SC_MEAN) / SC_STD\n",
    "train_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10\n",
    "test_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MPNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General dense feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_init(m): pass\n",
    "#     if type(m) == nn.BatchNorm1d: \n",
    "#         nn.init.ones_(m.weight)\n",
    "#         nn.init.zeros_(m.bias)\n",
    "\n",
    "def selu_weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        fan_in = m.weight.size(1)\n",
    "        m.weight.data.normal_(0.0, 1.0 / math.sqrt(fan_in))\n",
    "        m.bias.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def relu_weights_init(m):\n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "#         m.bias.data.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def hidden_layer(n_in, n_out, batch_norm, dropout, act=None):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if act: layers.append(act)\n",
    "    if batch_norm: layers.append(nn.BatchNorm1d(n_out))\n",
    "    if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "    return layers\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], batch_norm=False, \n",
    "                 out_act=None, final_bn=False):\n",
    "        super().__init__()\n",
    "        sizes = [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        layers_ = []\n",
    "        for i, (n_in, n_out, dr) in enumerate(zip(sizes[:-1], sizes[1:], dropout)):\n",
    "            act_ = act if i < len(layers) else out_act\n",
    "            batch_norm_ = batch_norm if i < len(layers) else final_bn\n",
    "            layers_ += hidden_layer(n_in, n_out, batch_norm_, dr, act_)      \n",
    "        self.layers = nn.Sequential(*layers_)\n",
    "        if type(act) == nn.SELU: self.layers.apply(selu_weights_init)\n",
    "        else: self.layers.apply(relu_weights_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM cell as describedi in the set2set paper (https://arxiv.org/pdf/1511.06391.pdf). Doesn't take any inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLSTMCell(nn.Module):\n",
    "    \"\"\"Implements the LSTM cell update described in the sec 4.2 of https://arxiv.org/pdf/1511.06391.pdf.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_h_out):\n",
    "        \"\"\"This LSTM cell takes no external 'x' inputs, but has a hidden state appended with the \n",
    "        readout from a content based attention mechanism. Therefore the hidden state is of a dimension\n",
    "        that is two times the number of nodes in the set.\"\"\"\n",
    "        super().__init__()\n",
    "        self.n_h_out, self.n_h = n_h_out, n_h_out * 2 \n",
    "        self.w_h = nn.Parameter(torch.Tensor(self.n_h, n_h_out * 4))\n",
    "        self.b = nn.Parameter(torch.Tensor(n_h_out * 4))\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "                # nn.init.orthogonal_(p.data)\n",
    "            else: \n",
    "                nn.init.zeros_(p.data)\n",
    "                # initialize the forget gate bias to 1\n",
    "                p.data[self.n_h_out:self.n_h_out*2] = torch.ones(self.n_h_out)\n",
    "        \n",
    "    def forward(self, h_prev, c_prev):\n",
    "        \"\"\"Takes previuos hidden and cell states as arguments and performs a \n",
    "        single LSTM step using no external input.\n",
    "        \"\"\"\n",
    "        n_h_ = self.n_h_out # number of output hidden states\n",
    "        # batch the computations into a single matrix multiplication\n",
    "        gates = h_prev @ self.w_h + self.b\n",
    "        i_g, f_g, g, o_g = (\n",
    "            torch.sigmoid(gates[:, :n_h_]), # input\n",
    "            torch.sigmoid(gates[:, n_h_:n_h_*2]), # forget\n",
    "            torch.tanh(gates[:, n_h_*2:n_h_*3]),\n",
    "            torch.sigmoid(gates[:, n_h_*3:]), # output\n",
    "        )\n",
    "        c = f_g * c_prev + i_g * g\n",
    "        h = o_g * torch.tanh(c)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set2set module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_add(src, idx, num):\n",
    "    sz = num, src.size(1)\n",
    "    exp_idx = idx[:,None].repeat(1, sz[1])\n",
    "    out = torch.zeros(sz, dtype=src.dtype, device=src.device)\n",
    "    return out.scatter_add(0, exp_idx, src)\n",
    "\n",
    "def softmax(x, idx, num=None):\n",
    "    x = x.exp()\n",
    "    x = x / (scatter_add(x, idx, num=num)[idx] + 1e-16)\n",
    "    return x\n",
    "\n",
    "class Set2SetGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric\\\n",
    "        /nn/glob/set2set.html#Set2Set\n",
    "    \"\"\"\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.proc_steps = proc_steps\n",
    "        self.gru = nn.GRUCell(n_set_in, n_set_in)\n",
    "        self.init_q = nn.Parameter(torch.Tensor(1, n_set_in))\n",
    "        self.init_r = nn.Parameter(torch.Tensor(1, n_set_in))\n",
    "        nn.init.zeros_(self.init_q)\n",
    "        nn.init.zeros_(self.init_r)\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        q = self.init_q.expand(batch_size, -1).contiguous()\n",
    "        r = self.init_r.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q = self.gru(r, q)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_add(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "        return torch.cat([q, r], dim=-1) #q_star\n",
    "\n",
    "class Set2SetLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric\\\n",
    "        /nn/glob/set2set.html#Set2Set\n",
    "    \"\"\"\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.n_set_in, n_set_out = n_set_in, 2 * n_set_in\n",
    "        self.proc_steps = proc_steps\n",
    "        self.lstm = HiddenLSTMCell(n_set_in)\n",
    "        self.init_q_star = nn.Parameter(torch.Tensor(1, n_set_out))\n",
    "        self.init_h = nn.Parameter(torch.Tensor(1, n_set_in))\n",
    "        nn.init.zeros_(self.init_q_star)\n",
    "        nn.init.zeros_(self.init_h)\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        h = self.init_h.expand(batch_size, -1).contiguous()\n",
    "        q_star = self.init_q_star.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q, h = self.lstm(q_star, h)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_add(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "            q_star = torch.cat([q, r], dim=-1)\n",
    "            \n",
    "        return q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge network message function as described in the MPNN paper (https://arxiv.org/pdf/1704.01212.pdf). Adds in seperate edge network to allow messages to flow along scalar coupling edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeNetwork(nn.Module):\n",
    "    def __init__(self, n_h, n_e, n_sc_e, net_args={}):\n",
    "        super().__init__()\n",
    "        self.n_h = n_h\n",
    "        self.adj_net = FullyConnectedNet(n_e, n_h**2, **net_args)\n",
    "        self.sc_adj_net = FullyConnectedNet(n_sc_e, n_h**2, **net_args)\n",
    "        self.b = nn.Parameter(torch.Tensor(n_h)) # bias for the message function\n",
    "        nn.init.zeros_(self.b)\n",
    "    \n",
    "    def forward(self, h, e, sc_e, pairs_idx, sc_pairs_idx, t=0):\n",
    "        \"\"\"\n",
    "        Compute message vector m_t given the previuos hidden state\n",
    "        h_t-1 and edge features e.\n",
    "        - h: tensor of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_e: tensor of scalar coupling edge features of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - t: update iteration. \n",
    "        \"\"\"\n",
    "        # compute 'A(e)'\n",
    "        if t==0: \n",
    "            self.a_mat = self.get_a_mat(self.adj_net(e))\n",
    "            self.a_sc_mat = self.get_a_mat(self.sc_adj_net(sc_e))\n",
    "            \n",
    "        # compute 'm_{i} = sum_{j in N(i)}(A_{ij}h_{j})' for all nodes 'i'\n",
    "        m = self.add_message(torch.zeros_like(h), self.a_mat, h, pairs_idx)\n",
    "        m = self.add_message(m, self.a_sc_mat, h, sc_pairs_idx)\n",
    "        return m + self.b # add message bias\n",
    "    \n",
    "    def get_a_mat(self, a_vect):\n",
    "        return a_vect.view(-1, self.n_h, self.n_h) / (self.n_h ** .5)\n",
    "    \n",
    "    def add_message(self, m, a, h, pairs_idx):\n",
    "        # transform 'pairs_idx' and 'a' to make messages go both in to and out of all nodes\n",
    "        in_out_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n",
    "        a_ = torch.cat((a, a)) \n",
    "        \n",
    "        # select the 'h_{j}' feeding into the 'm_{i}'\n",
    "        h_in = h.index_select(0, in_out_idx[:,1])\n",
    "        \n",
    "        # do the matrix multiplication 'A_{ij}h_{j}'\n",
    "        ah = (h_in.unsqueeze(1) @ a_).squeeze(1)\n",
    "        \n",
    "        # Sum up all 'A_{ij}h_{j}' per node 'i'\n",
    "        return m.scatter_add(0, in_out_idx[:,0,None].repeat(1, self.n_h), ah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU update function as described in the MPNN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUUpdate(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(n_h, n_h)\n",
    "        \n",
    "    def forward(self, m, h_prev):\n",
    "        \"\"\"\n",
    "        Update hidden state h.\n",
    "        - h_prev is vector of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - m is vector of messages of shape (batch_size * n_nodes, n_h)\n",
    "        \"\"\"\n",
    "        return self.gru(m, h_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom readout network following th set2set processing stage. Allows some final specialization/fine-tuning for each scalar coupling type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomHead(nn.Module):\n",
    "    def __init__(self, n_input, n_output, pre_layers=[], post_layers=[], act=nn.ReLU(True), dropout=[], \n",
    "                 batch_norm=False):\n",
    "        super().__init__()\n",
    "        n_pre_layers = len(pre_layers)\n",
    "        self.preproc = FullyConnectedNet(n_input, None, pre_layers, act, dropout[:n_pre_layers], batch_norm)\n",
    "        self.postproc = nn.ModuleList([\n",
    "            FullyConnectedNet(pre_layers[-1], n_output, post_layers, act, dropout[n_pre_layers:], batch_norm=False)\n",
    "            for _ in range(N_TYPES)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, sc_types):\n",
    "        x_ = self.preproc(x)\n",
    "        y = torch.zeros(sc_types.size(0), device=x.device)\n",
    "        for i in range(N_TYPES):\n",
    "            if torch.any(sc_types == i): \n",
    "                y[sc_types == i] = self.postproc[i](x_[sc_types == i]).view(-1)\n",
    "        return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines all the the components of the readout function described in the MPNN network using set2set processing and the scalar coupling type customized head. Also adds in some skip connections to final node states and scalar coupling input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Set2SetOutput(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_sc_m, proc_steps, net_args):\n",
    "        super().__init__()\n",
    "        self.R_proj = nn.Linear(n_h + n_x, n_h)\n",
    "        self.R_proc = Set2SetGRU(n_h, proc_steps)\n",
    "        self.R_write = MyCustomHead((4 * n_h) + n_sc_m, 1, **net_args)\n",
    "    \n",
    "    def forward(self, h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types):\n",
    "        \"\"\"\n",
    "        Make prediction.\n",
    "        - h is vector of hidden states of shape (batch_size * n_nodes, n_h).\n",
    "        - x is vector of input features of shape (batch_size * n_nodes, n_x).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        m = self.R_proj(torch.cat([h, x], dim=1))\n",
    "        q = self.R_proc(m, node_idx)\n",
    "        \n",
    "        # introduce skip connection to final node states of scalar coupling atoms\n",
    "        inp = torch.cat([\n",
    "            q.index_select(0, sc_idx),\n",
    "            h.index_select(0, sc_pairs_idx[:,0]),\n",
    "            h.index_select(0, sc_pairs_idx[:,1]),\n",
    "            sc_m\n",
    "        ], dim=-1)\n",
    "        y = self.R_write(inp, sc_types)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines the edge-network message (M), GRU update (U) and set2set readout (R) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps=3, proc_steps=10, \n",
    "                 preproc_net_args={}, enn_args={}, R_net_args={}):\n",
    "        super().__init__()\n",
    "        self.preproc_net = FullyConnectedNet(n_x, n_h, **preproc_net_args)\n",
    "        self.M = EdgeNetwork(n_h, n_e, n_sc_e, enn_args)\n",
    "        self.U = GRUUpdate(n_h)\n",
    "        self.R = Set2SetOutput(n_x, n_h, n_sc_m, proc_steps, R_net_args)\n",
    "        self.update_steps = update_steps\n",
    "        \n",
    "    def forward(self, x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: tensor of node features of shape (batch_size * n_nodes, n_x).\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_e: tensor of scalar coupling edge features of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        h = self.preproc_net(x)\n",
    "        for t in range(self.update_steps):\n",
    "            m = self.M(h, e, sc_e, pairs_idx, sc_pairs_idx, t)\n",
    "            h = self.U(m, h)\n",
    "        y = self.R(h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ids = train_df['molecule_id'].unique()\n",
    "n_obs = len(mol_ids)\n",
    "split = int(n_obs*0.75)\n",
    "set_seed(100)\n",
    "mol_ids_ = np.random.choice(mol_ids, size=n_obs, replace=False)\n",
    "train_mol_ids, val_mol_ids = pd.Series(mol_ids_[:split]), pd.Series(mol_ids_[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mol_sc = train_df.groupby('molecule_id')\n",
    "test_gb_mol_sc = test_df.groupby('molecule_id')\n",
    "gb_mol_atom = atom_df.groupby('molecule_id')\n",
    "gb_mol_edge = edge_df.groupby('molecule_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pytorch dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_np(x):\n",
    "    sz = len(x), len(np.unique(x))\n",
    "    x_one_hot = np.zeros(sz, dtype=np.long)\n",
    "    x_one_hot[np.arange(sz[0]), x] = 1\n",
    "    return x_one_hot\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge):\n",
    "        self.n = len(mol_ids)\n",
    "        self.mol_ids = mol_ids\n",
    "        self.gb_mol_sc = gb_mol_sc\n",
    "        self.gb_mol_atom = gb_mol_atom\n",
    "        self.gb_mol_edge = gb_mol_edge\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gb_mol_sc.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_atom.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_edge.get_group(self.mol_ids[idx]))\n",
    "\n",
    "def np_lst_to_torch(arr_lst, dtype=torch.float):\n",
    "    return torch.from_numpy(np.ascontiguousarray(np.concatenate(arr_lst))).type(dtype)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_size, n_atom_sum = len(batch), 0\n",
    "    x, e, sc_e, sc_m, sc_types, sc_vals = [], [], [], [], [], []\n",
    "    node_idx, pairs_idx, sc_pairs_idx, sc_idx = [], [], [], []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        sc_df, atom_df, edge_df = batch[b]\n",
    "        n_atoms, n_sc = len(atom_df), len(sc_df)\n",
    "        \n",
    "        x.append(atom_df.drop(columns='molecule_id').values)\n",
    "        e.append(edge_df.drop(columns=['idx_0', 'idx_1', 'molecule_id']).values)\n",
    "        sc_e.append(sc_df[SC_EDGE_FEATS].values)\n",
    "        sc_m.append(sc_df[SC_MOL_FEATS].values)\n",
    "        sc_types.append(sc_df['type'].values)\n",
    "        sc_vals.append(sc_df['scalar_coupling_constant'].values)\n",
    "        \n",
    "        node_idx.append(np.repeat(b, n_atoms))\n",
    "        sc_idx.append(np.repeat(b, n_sc))\n",
    "        pairs_idx.append(edge_df[['idx_0', 'idx_1']].values + n_atom_sum)\n",
    "        sc_pairs_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values + n_atom_sum)\n",
    "        \n",
    "        n_atom_sum += n_atoms\n",
    "    \n",
    "    x, e = np_lst_to_torch(x), np_lst_to_torch(e), \n",
    "    sc_e, sc_m = np_lst_to_torch(sc_e), np_lst_to_torch(sc_m)\n",
    "    sc_vals, sc_types = np_lst_to_torch(sc_vals), np_lst_to_torch(sc_types, torch.long)\n",
    "    node_idx = np_lst_to_torch(node_idx, torch.long)\n",
    "    sc_idx = np_lst_to_torch(sc_idx, torch.long)\n",
    "    pairs_idx = np_lst_to_torch(pairs_idx, torch.long)\n",
    "    sc_pairs_idx = np_lst_to_torch(sc_pairs_idx, torch.long)\n",
    "    \n",
    "    return (x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types), sc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MoleculeDataset(train_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge)\n",
    "val_ds   = MoleculeDataset(val_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size, num_workers=8, drop_last=True)\n",
    "db = DataBunch(train_dl, val_dl, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([375, 20])\n",
      "torch.Size([387, 8])\n",
      "torch.Size([1144, 16])\n",
      "torch.Size([1144, 22])\n",
      "torch.Size([375])\n",
      "torch.Size([387, 2])\n",
      "torch.Size([1144])\n",
      "torch.Size([1144, 2])\n",
      "torch.Size([1144])\n",
      "torch.Size([1144])\n"
     ]
    }
   ],
   "source": [
    "for el in batch[0]: print(el.size())\n",
    "print(batch[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[0.0000, 1.0000, 0.0000,  ..., 1.2037, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000,  ..., 1.3965, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000,  ..., 1.2018, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.0000, 0.0000, 0.0000,  ..., 1.0617, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000,  ..., 1.0785, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000,  ..., 1.0802, 0.0000, 0.0000]])\n",
      "e:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  1.5316,  1.3957],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0947, -0.7402],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0948, -0.7397],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.3961,  0.9587],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.3049,  0.2646],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0802, -1.4458]])\n",
      "sc_e:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3127],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.3438,  0.7481, -0.7354],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.4088, -0.8878],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.9860, -0.9776],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4963,  0.8363,  0.8911],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.4963]])\n",
      "sc_m:\n",
      " tensor([[1.0000, 0.0000, 0.0000,  ..., 1.6000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.6000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.6000, 0.0000, 0.1000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3000, 0.1000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3000, 0.1000, 0.1000],\n",
      "        [1.0000, 0.0000, 0.0000,  ..., 0.3000, 0.1000, 0.1000]])\n",
      "node_idx:\n",
      " tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19])\n",
      "pairs_idx:\n",
      " tensor([[  0,   1],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        [  0,  11],\n",
      "        [  1,   2],\n",
      "        [  1,   3],\n",
      "        [  1,  12],\n",
      "        [  2,  13],\n",
      "        [  2,  14],\n",
      "        [  2,  15],\n",
      "        [  3,   4],\n",
      "        [  4,  16],\n",
      "        [  4,   7],\n",
      "        [  4,   5],\n",
      "        [  5,   6],\n",
      "        [  5,  17],\n",
      "        [  5,  18],\n",
      "        [  6,   7],\n",
      "        [  6,  19],\n",
      "        [  6,  20],\n",
      "        [  7,   8],\n",
      "        [  7,  21],\n",
      "        [  8,  22],\n",
      "        [  8,  23],\n",
      "        [  8,  24],\n",
      "        [ 25,  26],\n",
      "        [ 25,  34],\n",
      "        [ 25,  35],\n",
      "        [ 25,  36],\n",
      "        [ 26,  27],\n",
      "        [ 26,  31],\n",
      "        [ 27,  28],\n",
      "        [ 28,  29],\n",
      "        [ 28,  32],\n",
      "        [ 29,  30],\n",
      "        [ 29,  37],\n",
      "        [ 30,  31],\n",
      "        [ 30,  38],\n",
      "        [ 32,  33],\n",
      "        [ 39,  40],\n",
      "        [ 39,  47],\n",
      "        [ 39,  48],\n",
      "        [ 39,  49],\n",
      "        [ 40,  41],\n",
      "        [ 40,  42],\n",
      "        [ 41,  42],\n",
      "        [ 41,  50],\n",
      "        [ 41,  51],\n",
      "        [ 42,  46],\n",
      "        [ 42,  43],\n",
      "        [ 43,  44],\n",
      "        [ 43,  52],\n",
      "        [ 43,  53],\n",
      "        [ 44,  45],\n",
      "        [ 44,  54],\n",
      "        [ 44,  55],\n",
      "        [ 45,  46],\n",
      "        [ 46,  56],\n",
      "        [ 46,  57],\n",
      "        [ 58,  59],\n",
      "        [ 58,  67],\n",
      "        [ 58,  68],\n",
      "        [ 58,  69],\n",
      "        [ 59,  60],\n",
      "        [ 59,  65],\n",
      "        [ 60,  61],\n",
      "        [ 61,  62],\n",
      "        [ 61,  63],\n",
      "        [ 62,  70],\n",
      "        [ 63,  64],\n",
      "        [ 63,  71],\n",
      "        [ 64,  65],\n",
      "        [ 64,  72],\n",
      "        [ 65,  66],\n",
      "        [ 73,  74],\n",
      "        [ 73,  82],\n",
      "        [ 73,  83],\n",
      "        [ 73,  84],\n",
      "        [ 74,  75],\n",
      "        [ 74,  85],\n",
      "        [ 74,  86],\n",
      "        [ 75,  76],\n",
      "        [ 75,  78],\n",
      "        [ 75,  81],\n",
      "        [ 76,  87],\n",
      "        [ 76,  88],\n",
      "        [ 76,  77],\n",
      "        [ 77,  89],\n",
      "        [ 77,  90],\n",
      "        [ 77,  91],\n",
      "        [ 78,  79],\n",
      "        [ 78,  92],\n",
      "        [ 78,  93],\n",
      "        [ 79,  80],\n",
      "        [ 79,  81],\n",
      "        [ 79,  94],\n",
      "        [ 80,  96],\n",
      "        [ 80,  81],\n",
      "        [ 80,  95],\n",
      "        [ 81,  97],\n",
      "        [ 98,  99],\n",
      "        [ 98, 107],\n",
      "        [ 98, 108],\n",
      "        [ 98, 109],\n",
      "        [ 99, 100],\n",
      "        [ 99, 102],\n",
      "        [ 99, 110],\n",
      "        [100, 101],\n",
      "        [100, 102],\n",
      "        [100, 111],\n",
      "        [101, 112],\n",
      "        [102, 103],\n",
      "        [102, 106],\n",
      "        [103, 104],\n",
      "        [103, 113],\n",
      "        [103, 114],\n",
      "        [104, 105],\n",
      "        [104, 106],\n",
      "        [104, 115],\n",
      "        [105, 106],\n",
      "        [106, 116],\n",
      "        [117, 118],\n",
      "        [117, 126],\n",
      "        [117, 127],\n",
      "        [117, 128],\n",
      "        [118, 119],\n",
      "        [118, 124],\n",
      "        [119, 120],\n",
      "        [119, 129],\n",
      "        [120, 121],\n",
      "        [120, 123],\n",
      "        [121, 122],\n",
      "        [121, 130],\n",
      "        [121, 131],\n",
      "        [122, 132],\n",
      "        [123, 124],\n",
      "        [124, 125],\n",
      "        [133, 134],\n",
      "        [133, 142],\n",
      "        [133, 143],\n",
      "        [133, 144],\n",
      "        [134, 135],\n",
      "        [134, 145],\n",
      "        [134, 146],\n",
      "        [135, 136],\n",
      "        [135, 147],\n",
      "        [135, 148],\n",
      "        [136, 137],\n",
      "        [137, 141],\n",
      "        [137, 140],\n",
      "        [137, 138],\n",
      "        [138, 139],\n",
      "        [138, 149],\n",
      "        [138, 150],\n",
      "        [139, 151],\n",
      "        [139, 152],\n",
      "        [139, 153],\n",
      "        [140, 141],\n",
      "        [140, 154],\n",
      "        [140, 155],\n",
      "        [141, 156],\n",
      "        [141, 157],\n",
      "        [158, 159],\n",
      "        [158, 167],\n",
      "        [158, 168],\n",
      "        [158, 169],\n",
      "        [159, 160],\n",
      "        [159, 161],\n",
      "        [159, 170],\n",
      "        [160, 171],\n",
      "        [160, 161],\n",
      "        [161, 162],\n",
      "        [161, 165],\n",
      "        [162, 163],\n",
      "        [162, 172],\n",
      "        [162, 173],\n",
      "        [163, 164],\n",
      "        [164, 174],\n",
      "        [165, 166],\n",
      "        [165, 175],\n",
      "        [176, 177],\n",
      "        [176, 184],\n",
      "        [176, 185],\n",
      "        [176, 186],\n",
      "        [177, 178],\n",
      "        [177, 187],\n",
      "        [177, 188],\n",
      "        [178, 189],\n",
      "        [178, 182],\n",
      "        [178, 179],\n",
      "        [179, 180],\n",
      "        [179, 190],\n",
      "        [179, 191],\n",
      "        [180, 181],\n",
      "        [181, 192],\n",
      "        [181, 193],\n",
      "        [181, 194],\n",
      "        [182, 183],\n",
      "        [195, 196],\n",
      "        [195, 204],\n",
      "        [195, 205],\n",
      "        [195, 206],\n",
      "        [196, 197],\n",
      "        [196, 203],\n",
      "        [197, 198],\n",
      "        [197, 203],\n",
      "        [197, 207],\n",
      "        [198, 208],\n",
      "        [198, 199],\n",
      "        [198, 200],\n",
      "        [199, 200],\n",
      "        [199, 209],\n",
      "        [200, 201],\n",
      "        [200, 203],\n",
      "        [201, 210],\n",
      "        [201, 202],\n",
      "        [203, 211],\n",
      "        [212, 213],\n",
      "        [212, 221],\n",
      "        [212, 222],\n",
      "        [212, 223],\n",
      "        [213, 214],\n",
      "        [213, 215],\n",
      "        [213, 224],\n",
      "        [214, 225],\n",
      "        [214, 226],\n",
      "        [214, 227],\n",
      "        [215, 228],\n",
      "        [215, 219],\n",
      "        [215, 216],\n",
      "        [216, 217],\n",
      "        [216, 229],\n",
      "        [216, 230],\n",
      "        [217, 218],\n",
      "        [217, 219],\n",
      "        [217, 231],\n",
      "        [218, 219],\n",
      "        [219, 220],\n",
      "        [220, 232],\n",
      "        [220, 233],\n",
      "        [220, 234],\n",
      "        [235, 236],\n",
      "        [235, 244],\n",
      "        [235, 245],\n",
      "        [235, 246],\n",
      "        [236, 237],\n",
      "        [236, 240],\n",
      "        [236, 242],\n",
      "        [237, 248],\n",
      "        [237, 247],\n",
      "        [237, 238],\n",
      "        [238, 239],\n",
      "        [238, 249],\n",
      "        [238, 250],\n",
      "        [239, 240],\n",
      "        [240, 241],\n",
      "        [241, 251],\n",
      "        [242, 243],\n",
      "        [243, 252],\n",
      "        [253, 254],\n",
      "        [253, 262],\n",
      "        [254, 255],\n",
      "        [254, 263],\n",
      "        [255, 256],\n",
      "        [256, 257],\n",
      "        [256, 260],\n",
      "        [256, 261],\n",
      "        [257, 264],\n",
      "        [257, 259],\n",
      "        [257, 258],\n",
      "        [258, 259],\n",
      "        [258, 261],\n",
      "        [258, 265],\n",
      "        [259, 260],\n",
      "        [259, 266],\n",
      "        [260, 261],\n",
      "        [260, 267],\n",
      "        [261, 268],\n",
      "        [269, 270],\n",
      "        [269, 278],\n",
      "        [269, 279],\n",
      "        [269, 280],\n",
      "        [270, 271],\n",
      "        [270, 276],\n",
      "        [270, 281],\n",
      "        [271, 272],\n",
      "        [272, 282],\n",
      "        [272, 283],\n",
      "        [272, 273],\n",
      "        [273, 274],\n",
      "        [274, 275],\n",
      "        [274, 277],\n",
      "        [274, 284],\n",
      "        [275, 276],\n",
      "        [275, 285],\n",
      "        [275, 286],\n",
      "        [276, 277],\n",
      "        [276, 287],\n",
      "        [277, 288],\n",
      "        [277, 289],\n",
      "        [290, 291],\n",
      "        [290, 299],\n",
      "        [291, 292],\n",
      "        [291, 295],\n",
      "        [292, 293],\n",
      "        [292, 300],\n",
      "        [292, 301],\n",
      "        [293, 296],\n",
      "        [293, 302],\n",
      "        [293, 294],\n",
      "        [294, 295],\n",
      "        [294, 303],\n",
      "        [294, 304],\n",
      "        [296, 297],\n",
      "        [296, 298],\n",
      "        [296, 305],\n",
      "        [297, 307],\n",
      "        [297, 298],\n",
      "        [297, 306],\n",
      "        [298, 308],\n",
      "        [309, 310],\n",
      "        [309, 318],\n",
      "        [309, 319],\n",
      "        [309, 320],\n",
      "        [310, 311],\n",
      "        [310, 313],\n",
      "        [310, 321],\n",
      "        [311, 315],\n",
      "        [311, 322],\n",
      "        [311, 312],\n",
      "        [312, 313],\n",
      "        [312, 323],\n",
      "        [313, 314],\n",
      "        [315, 316],\n",
      "        [315, 317],\n",
      "        [315, 324],\n",
      "        [316, 326],\n",
      "        [316, 317],\n",
      "        [316, 325],\n",
      "        [317, 327],\n",
      "        [328, 329],\n",
      "        [329, 330],\n",
      "        [329, 337],\n",
      "        [330, 331],\n",
      "        [330, 338],\n",
      "        [331, 332],\n",
      "        [331, 339],\n",
      "        [331, 340],\n",
      "        [332, 333],\n",
      "        [332, 334],\n",
      "        [334, 335],\n",
      "        [334, 341],\n",
      "        [334, 342],\n",
      "        [335, 336],\n",
      "        [336, 343],\n",
      "        [344, 345],\n",
      "        [344, 353],\n",
      "        [345, 346],\n",
      "        [345, 354],\n",
      "        [345, 355],\n",
      "        [346, 347],\n",
      "        [346, 352],\n",
      "        [347, 348],\n",
      "        [347, 356],\n",
      "        [348, 351],\n",
      "        [348, 349],\n",
      "        [348, 350],\n",
      "        [349, 350],\n",
      "        [349, 357],\n",
      "        [349, 358],\n",
      "        [350, 359],\n",
      "        [350, 360],\n",
      "        [351, 352],\n",
      "        [352, 361],\n",
      "        [352, 362],\n",
      "        [363, 364],\n",
      "        [363, 372],\n",
      "        [364, 365],\n",
      "        [365, 366],\n",
      "        [366, 367],\n",
      "        [367, 368],\n",
      "        [367, 371],\n",
      "        [368, 369],\n",
      "        [368, 373],\n",
      "        [369, 370],\n",
      "        [370, 371],\n",
      "        [371, 374]])\n",
      "sc_idx:\n",
      " tensor([ 0,  0,  0,  ..., 19, 19, 19])\n",
      "sc_pairs_idx:\n",
      " tensor([[  9,   0],\n",
      "        [  9,   1],\n",
      "        [  9,   2],\n",
      "        ...,\n",
      "        [374, 368],\n",
      "        [374, 370],\n",
      "        [374, 371]])\n",
      "sc_types:\n",
      " tensor([0, 4, 6,  ..., 6, 3, 0])\n",
      "sc_vals:\n",
      " tensor([ 1.9866, -0.5538, -0.3553,  ..., -0.3705, -0.1089,  3.4257])\n"
     ]
    }
   ],
   "source": [
    "b_dict = dict(x=batch[0][0], \n",
    "              e=batch[0][1], \n",
    "              sc_e=batch[0][2], \n",
    "              sc_m=batch[0][3], \n",
    "              node_idx=batch[0][4], \n",
    "              pairs_idx=batch[0][5], \n",
    "              sc_idx=batch[0][6], \n",
    "              sc_pairs_idx=batch[0][7], \n",
    "              sc_types=batch[0][8], \n",
    "              sc_vals=batch[1])\n",
    "for k,v in b_dict.items(): print(f'{k}:\\n {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the metric used for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types):\n",
    "    proc = lambda x: x.cpu().numpy().ravel() \n",
    "    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n",
    "    y_true = SC_MEAN + y_true * SC_STD\n",
    "    y_pred = SC_MEAN + y_pred * SC_STD\n",
    "    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes).mean()\n",
    "\n",
    "class GroupMeanLogMAE(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['group_mean_log_mae'])\n",
    "    def on_epoch_begin(self, **kwargs): self.input, self.output, self.target = [], [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, last_input, train, **kwargs):\n",
    "        if not train:\n",
    "            self.input.append(last_input[-1])\n",
    "            self.output.append(last_output)\n",
    "            self.target.append(last_target)\n",
    "                \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if (len(self.input) > 0) and (len(self.output) > 0):\n",
    "            inputs = torch.cat(self.input)\n",
    "            preds = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            metric = group_mean_log_mae(preds, target, inputs)\n",
    "            return add_metrics(last_metrics, [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd, batch_norm, act = 0, True, nn.ReLU(True)\n",
    "update_steps, proc_steps = 5, 10\n",
    "n_x, n_h, n_e, n_sc_e, n_sc_m = N_ATOM_FEATURES, 150, N_EDGE_FEATURES, N_SC_EDGE_FEATURES, N_SC_MOL_FEATURES\n",
    "preproc_net_args = dict(layers=[], act=act, dropout=[], batch_norm=batch_norm, out_act=nn.Tanh())\n",
    "enn_args = dict(layers=3*[n_h], act=act, dropout=3*[0.0], batch_norm=batch_norm)\n",
    "R_net_args = dict(pre_layers=[1000], post_layers=[500], act=act, dropout=[0.0, 0.0], batch_norm=batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "model = MPNN(n_x, n_h, n_e, n_sc_e, n_sc_m, stride, update_steps, proc_steps, preproc_net_args, enn_args, R_net_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNN(\n",
      "  (preproc_net): FullyConnectedNet(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=150, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (M): EdgeNetwork(\n",
      "    (adj_net): FullyConnectedNet(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=150, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (4): ReLU(inplace)\n",
      "        (5): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (7): ReLU(inplace)\n",
      "        (8): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): Linear(in_features=150, out_features=22500, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sc_adj_net): FullyConnectedNet(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=150, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (4): ReLU(inplace)\n",
      "        (5): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): Linear(in_features=150, out_features=150, bias=True)\n",
      "        (7): ReLU(inplace)\n",
      "        (8): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): Linear(in_features=150, out_features=22500, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (U): GRUUpdate(\n",
      "    (gru): GRUCell(150, 150)\n",
      "  )\n",
      "  (R): Set2SetOutput(\n",
      "    (R_proj): Linear(in_features=170, out_features=150, bias=True)\n",
      "    (R_proc): Set2Set(\n",
      "      (gru): GRUCell(150, 150)\n",
      "    )\n",
      "    (R_write): MyCustomHead(\n",
      "      (preproc): FullyConnectedNet(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=622, out_features=1000, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (postproc): ModuleList(\n",
      "        (0): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (2): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (3): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (4): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (5): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (6): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (7): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): Linear(in_features=500, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([ 0.0604,  0.1227, -0.0449,  ...,  0.1598, -0.0304,  0.0201],\n",
      "       grad_fn=<IndexPutBackward>)\n",
      "torch.Size([1144])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model(*batch[0]))\n",
    "print(model(*batch[0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics=[mean_absolute_error], callback_fns=GroupMeanLogMAE, \n",
    "                wd=wd, loss_func=root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-6, end_lr=1.0, num_it=100, stop_div=True)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [5/10 42:07<42:07]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>group_mean_log_mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>0.039103</td>\n",
       "      <td>0.023578</td>\n",
       "      <td>-0.154585</td>\n",
       "      <td>08:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.038864</td>\n",
       "      <td>0.033181</td>\n",
       "      <td>0.023252</td>\n",
       "      <td>-0.303911</td>\n",
       "      <td>08:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.025998</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>-0.603389</td>\n",
       "      <td>08:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024595</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.014002</td>\n",
       "      <td>-0.771325</td>\n",
       "      <td>08:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>0.011313</td>\n",
       "      <td>-0.996840</td>\n",
       "      <td>08:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2118' class='' max='3187', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      66.46% [2118/3187 04:22<02:12 0.0166]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with group_mean_log_mae value: -0.1545848250389099.\n",
      "Better model found at epoch 1 with group_mean_log_mae value: -0.3039112389087677.\n",
      "Better model found at epoch 2 with group_mean_log_mae value: -0.6033890247344971.\n",
      "Better model found at epoch 3 with group_mean_log_mae value: -0.7713249921798706.\n",
      "Better model found at epoch 4 with group_mean_log_mae value: -0.9968396425247192.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, max_lr=2e-3, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  name='mpnn1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(skip_start=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, max_lr=3e-4, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  name='mpnn2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, max_lr=1e-4, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  name='mpnn3')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
