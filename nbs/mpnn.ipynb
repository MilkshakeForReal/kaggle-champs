{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.basic_data import DataBunch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __print__ = print\n",
    "# def print(*strings):\n",
    "#     for string in strings:\n",
    "#         os.system(f'echo \\\"{string}\\\"')\n",
    "#         __print__(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID = 1\n",
    "VERSION = 2\n",
    "\n",
    "TYPES              = np.array(['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'])\n",
    "TYPES_MAP          = {t: i for i, t in enumerate(TYPES)}\n",
    "SC_EDGE_FEATS      = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "SC_MOL_FEATS       = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', \n",
    "                      #'inv_dist', 'normed_inv_dist', \n",
    "                      'std_bond_length', 'ave_bond_length', #'total_bond_length',  \n",
    "                      #'ave_inv_bond_length', 'total_inv_bond_length', \n",
    "                      'ave_atom_weight'#, 'total_atom_weight'\n",
    "                     ]\n",
    "ATOM_FEATS         = ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', \n",
    "                      'degree_1', 'degree_2', 'degree_3', 'degree_4', 'degree_5', \n",
    "                      'SP', 'SP2', 'SP3', 'hybridization_unspecified', \n",
    "                      'aromatic', 'formal_charge', 'atomic_num',\n",
    "                      'donor', 'acceptor', \n",
    "                      'ave_bond_length', \n",
    "                      #'ave_inv_bond_length',\n",
    "                      'ave_neighbor_weight']\n",
    "EDGE_FEATS         = ['single', 'double', 'triple', 'aromatic', \n",
    "                      'conjugated', 'in_ring',\n",
    "                      'dist', 'normed_dist', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "TARGET_COL         = 'scalar_coupling_constant'\n",
    "CONTRIB_COLS       = ['fc', 'sd', 'pso', 'dso']\n",
    "N_EDGE_FEATURES    = len(EDGE_FEATS)\n",
    "N_SC_EDGE_FEATURES = len(SC_EDGE_FEATS)\n",
    "N_SC_MOL_FEATURES  = len(SC_MOL_FEATS)\n",
    "N_ATOM_FEATURES    = len(ATOM_FEATS)\n",
    "N_TYPES            = len(TYPES)\n",
    "N_MOLS             = 130775\n",
    "SC_MEAN            = 16\n",
    "SC_STD             = 35\n",
    "\n",
    "SC_FEATS_TO_SCALE   = ['dist', 'dist_min_rad', 'dist_electro_neg_adj', 'num_atoms', 'num_C_atoms', \n",
    "                       'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', 'inv_dist', \n",
    "                       'ave_bond_length', 'std_bond_length', 'total_bond_length',  'ave_inv_bond_length', \n",
    "                       'total_inv_bond_length', 'ave_atom_weight', 'total_atom_weight']\n",
    "ATOM_FEATS_TO_SCALE = ['atomic_num', 'ave_bond_length', 'ave_inv_bond_length', 'ave_neighbor_weight']\n",
    "EDGE_FEATS_TO_SCALE = ['dist', 'inv_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = '../tmp/'\n",
    "CV_IDXS_PATH = PATH\n",
    "# DATA_PATH = '../input/champs-scalar-coupling/'\n",
    "# PATH = '../input/champs-processed-data-3/'\n",
    "# CV_IDXS_PATH = '../input/champs-cv-4-fold-idxs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp/: ['edge_df.csv', 'angle_out_df.csv', 'test_proc_df.csv', 'atom_df.csv', 'train_idxs_8_fold_cv.csv', 'dist_df.csv', 'train_proc_df.csv', 'val_idxs_8_fold_cv.csv', 'angle_in_df.csv']\n",
      "../data/: ['sample_submission.csv', 'scalar_coupling_contributions.csv']\n"
     ]
    }
   ],
   "source": [
    "def show_csv_files(path):\n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if f.find('.csv') != -1]\n",
    "    print(f'{path}:', files)\n",
    "show_csv_files(PATH)\n",
    "show_csv_files(DATA_PATH)\n",
    "# show_csv_files(CV_IDXS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(PATH+'train_proc_df.csv', index_col=0)\n",
    "test_df  = pd.read_csv(PATH+'test_proc_df.csv', index_col=0)\n",
    "atom_df  = pd.read_csv(PATH+'atom_df.csv', index_col=0)\n",
    "edge_df  = pd.read_csv(PATH+'edge_df.csv', index_col=0)\n",
    "dist_df  = pd.read_csv(PATH+'dist_df.csv', index_col=0, dtype=np.float32)\n",
    "angle_in_df  = pd.read_csv(PATH+'angle_in_df.csv', index_col=0)\n",
    "angle_out_df = pd.read_csv(PATH+'angle_out_df.csv', index_col=0)\n",
    "\n",
    "train_mol_ids = pd.read_csv(CV_IDXS_PATH+'train_idxs_8_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "val_mol_ids   = pd.read_csv(CV_IDXS_PATH+'val_idxs_8_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "test_mol_ids  = pd.Series(test_df['molecule_id'].unique())\n",
    "\n",
    "contribs_df = pd.read_csv(DATA_PATH+'scalar_coupling_contributions.csv')\n",
    "train_df = pd.concat((train_df, contribs_df[CONTRIB_COLS]), axis=1)\n",
    "del contribs_df\n",
    "gc.collect()\n",
    "\n",
    "train_df[[TARGET_COL, 'fc']] = (train_df[[TARGET_COL, 'fc']] - SC_MEAN) / SC_STD\n",
    "train_df[CONTRIB_COLS[1:]] = train_df[CONTRIB_COLS[1:]] / SC_STD\n",
    "\n",
    "train_df['num_atoms'] = train_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                  'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "test_df['num_atoms'] = test_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "train_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10\n",
    "test_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>2.593389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914926</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035961</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.333287</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>3.922863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772420</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>-0.098103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.816496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783147</td>\n",
       "      <td>3.922924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772357</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081672</td>\n",
       "      <td>-0.098111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.333347</td>\n",
       "      <td>0.816502</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>3.922945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772340</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>-0.098112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>2.593385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914920</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "0      H      C             1             0   0.000000    0.000000   \n",
       "1      H      H             1             2  -0.333287    0.816483   \n",
       "2      H      H             1             3  -0.333335    0.816498   \n",
       "3      H      H             1             4  -0.333347    0.816502   \n",
       "4      H      C             2             0   0.000000    0.000000   \n",
       "\n",
       "   cos_angle1  diangle      dist  dist_electro_neg_adj  ...  std_bond_length  \\\n",
       "0   -0.333335      0.0  1.091953              2.593389  ...         0.000003   \n",
       "1    0.816482      0.0  1.783120              3.922863  ...         0.000003   \n",
       "2    0.816496      0.0  1.783147              3.922924  ...         0.000003   \n",
       "3    0.816500      0.0  1.783157              3.922945  ...         0.000003   \n",
       "4   -0.333352      0.0  1.091952              2.593385  ...         0.000003   \n",
       "\n",
       "   total_bond_length  ave_inv_bond_length  total_inv_bond_length  \\\n",
       "0           4.367799             0.915793               3.663173   \n",
       "1           4.367799             0.915793               3.663173   \n",
       "2           4.367799             0.915793               3.663173   \n",
       "3           4.367799             0.915793               3.663173   \n",
       "4           4.367799             0.915793               3.663173   \n",
       "\n",
       "   ave_atom_weight  total_atom_weight        fc        sd       pso       dso  \n",
       "0              0.2                1.0  1.914926  0.007274  0.035961  0.007772  \n",
       "1              0.2                1.0 -0.772420  0.010085  0.081668 -0.098103  \n",
       "2              0.2                1.0 -0.772357  0.010084  0.081672 -0.098111  \n",
       "3              0.2                1.0 -0.772340  0.010084  0.081673 -0.098112  \n",
       "4              0.2                1.0  1.914920  0.007274  0.035960  0.007772  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>type_7</th>\n",
       "      <th>inv_dist</th>\n",
       "      <th>normed_inv_dist</th>\n",
       "      <th>ave_bond_length</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>7.311210</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300908</td>\n",
       "      <td>-2.038452</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "4658147      H      C             2             0       -1.0         1.0   \n",
       "4658148      H      C             2             1        0.0         0.0   \n",
       "4658149      H      H             2             3        0.0         1.0   \n",
       "4658150      H      C             3             0        0.0         0.0   \n",
       "4658151      H      C             3             1       -1.0         1.0   \n",
       "\n",
       "         cos_angle1  diangle      dist  dist_electro_neg_adj  ...  type_7  \\\n",
       "4658147        -1.0      0.0  2.261178              5.370298  ...       0   \n",
       "4658148        -1.0      0.0  1.062099              2.522485  ...       0   \n",
       "4658149         1.0      0.0  3.323277              7.311210  ...       0   \n",
       "4658150        -1.0      0.0  1.062099              2.522485  ...       0   \n",
       "4658151        -1.0      0.0  2.261178              5.370298  ...       0   \n",
       "\n",
       "         inv_dist  normed_inv_dist  ave_bond_length  std_bond_length  \\\n",
       "4658147  0.442247        -0.815269         1.107759         0.064573   \n",
       "4658148  0.941532         4.621731         1.107759         0.064573   \n",
       "4658149  0.300908        -2.038452         1.107759         0.064573   \n",
       "4658150  0.941532         4.621731         1.107759         0.064573   \n",
       "4658151  0.442247        -0.815269         1.107759         0.064573   \n",
       "\n",
       "         total_bond_length  ave_inv_bond_length  total_inv_bond_length  \\\n",
       "4658147           3.323277             0.905679               2.717037   \n",
       "4658148           3.323277             0.905679               2.717037   \n",
       "4658149           3.323277             0.905679               2.717037   \n",
       "4658150           3.323277             0.905679               2.717037   \n",
       "4658151           3.323277             0.905679               2.717037   \n",
       "\n",
       "         ave_atom_weight  total_atom_weight  \n",
       "4658147             0.35                1.4  \n",
       "4658148             0.35                1.4  \n",
       "4658149             0.35                1.4  \n",
       "4658150             0.35                1.4  \n",
       "4658151             0.35                1.4  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MPNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General dense feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     5,
     12,
     18,
     27
    ]
   },
   "outputs": [],
   "source": [
    "def bn_init(m): pass\n",
    "#     if type(m) == nn.BatchNorm1d: \n",
    "#         nn.init.ones_(m.weight)\n",
    "#         nn.init.zeros_(m.bias)\n",
    "\n",
    "def selu_weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        fan_in = m.weight.size(1)\n",
    "        m.weight.data.normal_(0.0, 1.0 / math.sqrt(fan_in))\n",
    "        m.bias.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def relu_weights_init(m): \n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "#         m.bias.data.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def hidden_layer(n_in, n_out, batch_norm, dropout, layer_norm=False, act=None):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if act: layers.append(act)\n",
    "    if batch_norm: layers.append(nn.BatchNorm1d(n_out))\n",
    "    if layer_norm: layers.append(nn.LayerNorm(n_out))\n",
    "    if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "    return layers\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], \n",
    "                 batch_norm=False, out_act=None, final_bn=False, layer_norm=False, \n",
    "                 final_ln=False):\n",
    "        super().__init__()\n",
    "        sizes = [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        layers_ = []\n",
    "        for i, (n_in, n_out, dr) in enumerate(zip(sizes[:-1], sizes[1:], dropout)):\n",
    "            act_ = act if i < len(layers) else out_act\n",
    "            batch_norm_ = batch_norm if i < len(layers) else final_bn\n",
    "            layer_norm_ = layer_norm if i < len(layers) else final_ln\n",
    "            layers_ += hidden_layer(n_in, n_out, batch_norm_, dr, layer_norm_, act_)      \n",
    "        self.layers = nn.Sequential(*layers_)\n",
    "        if type(act) == nn.SELU: self.layers.apply(selu_weights_init)\n",
    "        else: self.layers.apply(relu_weights_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class ResFullyConnectedNet(nn.Module):\n",
    "    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], \n",
    "                 batch_norm=False, out_act=None, final_bn=False, layer_norm=False, \n",
    "                 final_ln=False):\n",
    "        super().__init__()\n",
    "        n_layers, sizes = len(layers), [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        assert ((n_layers - 1) % 2) == 0\n",
    "        self.n_blocks, blocks =(n_layers - 1) // 2, [], \n",
    "        self.fc1 = nn.Sequential(*hidden_layer(n_input, layers[0], batch_norm, \n",
    "                                               dropout.pop(0), layer_norm, act))\n",
    "        for i in range(self.n_blocks):\n",
    "            blocks.append(FullyConnectedNet(layers[2*i], layers[2*(i+1)], [layers[(2*i)+1]], act, \n",
    "                                            dropout[2*i:2*(i+1)], batch_norm, act, \n",
    "                                            batch_norm, layer_norm, layer_norm))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.fc_out = nn.Sequential(*hidden_layer(layers[-1], n_output, final_bn, \n",
    "                                                  0.0, final_ln, out_act))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        for i in range(self.n_blocks):\n",
    "            x_ = self.blocks[i](x)\n",
    "            x = x + x_\n",
    "        y = self.fc_out(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM cell as describedi in the set2set paper (https://arxiv.org/pdf/1511.06391.pdf). Doesn't take any inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class HiddenLSTMCell(nn.Module):\n",
    "    \"\"\"Implements the LSTM cell update described in the sec 4.2 of https://arxiv.org/pdf/1511.06391.pdf.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_h_out):\n",
    "        \"\"\"This LSTM cell takes no external 'x' inputs, but has a hidden state appended with the \n",
    "        readout from a content based attention mechanism. Therefore the hidden state is of a dimension\n",
    "        that is two times the number of nodes in the set.\"\"\"\n",
    "        super().__init__()\n",
    "        self.n_h_out, self.n_h = n_h_out, n_h_out * 2 \n",
    "        self.w_h = nn.Parameter(torch.Tensor(self.n_h, n_h_out * 4))\n",
    "        self.b = nn.Parameter(torch.Tensor(n_h_out * 4))\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "                # nn.init.orthogonal_(p.data)\n",
    "            else: \n",
    "                nn.init.zeros_(p.data)\n",
    "                # initialize the forget gate bias to 1\n",
    "                p.data[self.n_h_out:self.n_h_out*2] = torch.ones(self.n_h_out)\n",
    "        \n",
    "    def forward(self, h_prev, c_prev):\n",
    "        \"\"\"Takes previuos hidden and cell states as arguments and performs a \n",
    "        single LSTM step using no external input.\n",
    "        \"\"\"\n",
    "        n_h_ = self.n_h_out # number of output hidden states\n",
    "        # batch the computations into a single matrix multiplication\n",
    "        gates = h_prev @ self.w_h + self.b\n",
    "        i_g, f_g, g, o_g = (\n",
    "            torch.sigmoid(gates[:, :n_h_]), # input\n",
    "            torch.sigmoid(gates[:, n_h_:n_h_*2]), # forget\n",
    "            torch.tanh(gates[:, n_h_*2:n_h_*3]),\n",
    "            torch.sigmoid(gates[:, n_h_*3:]), # output\n",
    "        )\n",
    "        c = f_g * c_prev + i_g * g\n",
    "        h = o_g * torch.tanh(c)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndRNNCell(nn.Module):\n",
    "    def __init__(self, n_in, n_h, act=nn.ReLU(True), layer_norm=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(n_in, n_h)\n",
    "        act_ln_dr = [act]\n",
    "        if layer_norm: act_ln_dr.append(nn.LayerNorm(n_h))\n",
    "        if dropout!=0.0: act_ln_dr.append(nn.Dropout(dropout))\n",
    "        self.act_ln_dr = nn.Sequential(*act_ln_dr)\n",
    "        self.w_h = nn.Parameter(torch.Tensor(n_h))\n",
    "        nn.init.uniform_(self.w_h, a=0, b=1)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        h = self.act_ln_dr(self.lin(x) + self.w_h * h_prev)\n",
    "        return h\n",
    "        \n",
    "class IndRNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_layers, layer_norm=True, dropout=[]):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        if len(dropout)==0: dropout = n_layers * [0.0]\n",
    "        assert len(dropout) == n_layers\n",
    "        layers = []\n",
    "        for i, dr in enumerate(dropout):\n",
    "            n_in = n_x if 1==0 else n_h\n",
    "            layers.append(IndRNNCell(n_in, n_h, layer_norm=layer_norm, dropout=dr))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "            \n",
    "    def forward(self, x, h_prev):\n",
    "        h, hs = x, []\n",
    "        for i in range(self.n_layers):\n",
    "            h = self.layers[i](h, h_prev[i])\n",
    "            hs.append(h)\n",
    "        return h, torch.cat(hs, dim=0)\n",
    "        \n",
    "class ResIndRNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_blocks, layer_norm=True, dropout=[]):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        if len(dropout)==0: dropout = n_blocks * [0.0]\n",
    "        assert len(dropout) == n_blocks\n",
    "        blocks = []\n",
    "        for i, dr in enumerate(dropout):\n",
    "            n_in = n_x if 1==0 else n_h\n",
    "            blocks.append(IndRNN(n_in, n_h, n_layers=2, layer_norm=layer_norm, dropout=2*[dr]))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "            \n",
    "    def forward(self, x, h_prev):\n",
    "        hs = []\n",
    "        for i in range(self.n_blocks):\n",
    "            h, hs_ = self.blocks[i](x, h_prev[(i*2):((i+1)*2)])\n",
    "            x = h + x\n",
    "            hs.append(hs_)\n",
    "        return x, torch.cat(hs, dim=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set2set module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     6,
     11,
     41
    ]
   },
   "outputs": [],
   "source": [
    "def scatter_sum(src, idx, num):\n",
    "    sz = num, src.size(1)\n",
    "    exp_idx = idx[:,None].repeat(1, sz[1])\n",
    "    out = torch.zeros(sz, dtype=src.dtype, device=src.device)\n",
    "    return out.scatter_add(0, exp_idx, src)\n",
    "\n",
    "def scatter_mean(src, idx, num):\n",
    "    return scatter_sum(src, idx, num) / scatter_sum(torch.ones_like(src), idx, num).clamp(1.0)\n",
    "\n",
    "def softmax(x, idx, num=None):\n",
    "    x = x.exp()\n",
    "    x = x / (scatter_sum(x, idx, num=num)[idx] + 1e-16)\n",
    "    return x\n",
    "\n",
    "class SumReadout(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x, node_idx): return scatter_sum(x, node_idx, num=node_idx.max().item()+1)\n",
    "    \n",
    "class MeanReadout(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x, node_idx): return scatter_mean(x, node_idx, num=node_idx.max().item()+1)\n",
    "\n",
    "class Set2SetIndRNN(nn.Module):\n",
    "    def __init__(self, n_set_in, proc_steps, n_blocks=3):\n",
    "        super().__init__()\n",
    "        self.proc_steps = proc_steps\n",
    "        self.gru = ResIndRNN(n_set_in, n_set_in, n_blocks, layer_norm=True, dropout=[])\n",
    "        self.init_q = nn.Parameter(torch.zeros(2 * n_blocks, 1, n_set_in))\n",
    "        self.init_r = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        qs = self.init_q.expand(-1, batch_size, -1).contiguous()\n",
    "        r = self.init_r.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q, qs = self.gru(r, qs)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "        return torch.cat([q, r], dim=-1) #q_star\n",
    "    \n",
    "    \n",
    "class Set2SetGRU(nn.Module):\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.proc_steps = proc_steps\n",
    "        self.gru = nn.GRUCell(n_set_in, n_set_in)\n",
    "        self.init_q = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "        self.init_r = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        q = self.init_q.expand(batch_size, -1).contiguous()\n",
    "        r = self.init_r.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q = self.gru(r, q)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "        return torch.cat([q, r], dim=-1) #q_star\n",
    "\n",
    "class Set2SetLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric\\\n",
    "        /nn/glob/set2set.html#Set2Set\n",
    "    \"\"\"\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.n_set_in, n_set_out = n_set_in, 2 * n_set_in\n",
    "        self.proc_steps = proc_steps\n",
    "        self.lstm = HiddenLSTMCell(n_set_in)\n",
    "        self.init_q_star = nn.Parameter(torch.zeros(1, n_set_out))\n",
    "        self.init_h = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        h = self.init_h.expand(batch_size, -1).contiguous()\n",
    "        q_star = self.init_q_star.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q, h = self.lstm(q_star, h)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "            q_star = torch.cat([q, r], dim=-1)\n",
    "        return q_star\n",
    "    \n",
    "class GaussAtt(nn.Module):\n",
    "    N_TYPES = 8\n",
    "    PAD_VAL = 999\n",
    "    \n",
    "    def __init__(self, dim=1, type_specific=True): \n",
    "        super().__init__()\n",
    "        self.dim, self.type_specific = dim, type_specific\n",
    "        if type_specific: self.log_prec = nn.Parameter(torch.zeros(self.N_TYPES, dim))\n",
    "        else: self.log_prec = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    def forward(self, x, sc_pairs_idx, sc_types, dists, sc_node_idx, sc_idx2):\n",
    "        q0 = self.apply_gauss_att(x, sc_pairs_idx[:int(sc_pairs_idx.size(0)/2),0], dists, sc_node_idx, sc_idx2, sc_types)\n",
    "        q1 = self.apply_gauss_att(x, sc_pairs_idx[:int(sc_pairs_idx.size(0)/2),1], dists, sc_node_idx, sc_idx2, sc_types)\n",
    "        return torch.cat([q0, q1], dim=-1)\n",
    "        \n",
    "    def apply_gauss_att(self, x, sc_pairs_idx, dists, sc_node_idx, sc_idx2, sc_types):\n",
    "        if self.type_specific: log_prec = self.log_prec[sc_types].unsqueeze(1)\n",
    "        else: log_prec = self.log_prec\n",
    "        dists_ = dists.index_select(0, sc_pairs_idx).unsqueeze(-1).expand(-1, -1, self.dim)\n",
    "        pdf = torch.exp(-0.5 * (torch.exp(log_prec) * dists_ ** 2))\n",
    "        att_mask = pdf / pdf.sum(dim=1, keepdim=True)\n",
    "        att_x =  x.index_select(0, sc_node_idx) * att_mask[dists_!=self.PAD_VAL].view(-1, self.dim)\n",
    "        return scatter_sum(att_x, sc_idx2, num=sc_idx2.max().item()+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge network message function as described in the MPNN paper (https://arxiv.org/pdf/1704.01212.pdf). Adds in seperate edge network to allow messages to flow along scalar coupling edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ENNMessage(nn.Module):\n",
    "    def __init__(self, n_h, n_e, n_sc_h, enn_args={}, ann_args={}):\n",
    "        super().__init__()\n",
    "        self.n_h = n_h\n",
    "        self.enn = FullyConnectedNet(n_e, n_h**2, **enn_args)\n",
    "        self.sc_enn = FullyConnectedNet(n_sc_h, n_h**2, **enn_args)\n",
    "        self.ann = FullyConnectedNet(1, n_h, **ann_args)\n",
    "        self.m_bias = nn.Parameter(torch.Tensor(n_h)) # bias for the message function\n",
    "        self.weight_inits()\n",
    "        \n",
    "    def weight_inits(self):\n",
    "        nn.init.zeros_(self.m_bias)\n",
    "    \n",
    "    def forward(self, h, e, sc_h, pairs_idx, sc_pairs_idx, angles, angles_idx, t=0):\n",
    "        \"\"\"\n",
    "        Compute message vector m_t given the previuos hidden state\n",
    "        h_t-1 and edge features e.\n",
    "        - h: tensor of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_h: tensor of scalar coupling edge hidden states of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - t: update iteration. \n",
    "        \"\"\"\n",
    "        # compute 'A(e)' and angle attention masks\n",
    "        if t==0: \n",
    "            self.a_mat = self.get_a_mat(self.enn(e))\n",
    "            self.a_mat = torch.cat((self.a_mat, self.a_mat))\n",
    "            self.a_sc_mat = self.get_a_mat(self.sc_enn(sc_h))\n",
    "            self.att = self.ann(angles.view(-1,1))\n",
    "            \n",
    "        # compute 'm_{i} = sum_{j in N(i)}(A_{ij}h_{j})' for all nodes 'i'\n",
    "        m = self.add_message(torch.zeros_like(h), self.a_mat, h, pairs_idx, True, angles_idx)\n",
    "        m = self.add_message(m, self.a_sc_mat, h, sc_pairs_idx)\n",
    "        \n",
    "        # add message bias\n",
    "        m = m + self.m_bias\n",
    "        return m # apply optional batch norm\n",
    "    \n",
    "    def get_a_mat(self, a_vect):\n",
    "        return a_vect.view(-1, self.n_h, self.n_h) / (self.n_h ** .5)\n",
    "    \n",
    "    def add_message(self, m, a, h, pairs_idx, use_att=False, angles_idx=None):\n",
    "        # transform 'pairs_idx' and 'a' to make messages go both in to and out of all nodes\n",
    "        # in_out_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n",
    "        # a_ = torch.cat((a, a)) \n",
    "        \n",
    "        # select the 'h_{j}' feeding into the 'm_{i}'\n",
    "        h_in = h.index_select(0, pairs_idx[:,1])\n",
    "        \n",
    "        # do the matrix multiplication 'A_{ij}h_{j}'\n",
    "        ah = (h_in.unsqueeze(1) @ a).squeeze(1)\n",
    "        \n",
    "        # apply atttention\n",
    "        if use_att:\n",
    "            ave_att = scatter_mean(self.att, angles_idx, num=pairs_idx.size(0))\n",
    "            ah = ave_att * ah\n",
    "        \n",
    "        # Sum up all 'A_{ij}h_{j}' per node 'i'\n",
    "        return m.scatter_add(0, pairs_idx[:,0,None].repeat(1, self.n_h), ah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU update function as described in the MPNN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GRUUpdate(nn.Module):\n",
    "    def __init__(self, n_x, n_h):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(n_x, n_h)\n",
    "        \n",
    "    def forward(self, m, h_prev):\n",
    "        \"\"\"\n",
    "        Update hidden state h.\n",
    "        - h_prev is vector of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - m is vector of messages of shape (batch_size * n_nodes, n_h)\n",
    "        \"\"\"\n",
    "        return self.gru(m, h_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom readout network following th set2set processing stage. Allows some final specialization/fine-tuning for each scalar coupling type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contrib_net(n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "    layers = hidden_layer(n_in, n_h, False, dropout, layer_norm, act)\n",
    "    layers += hidden_layer(n_h, 1, False, 0.0) # output layer\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ContribsNet(nn.Module):\n",
    "    N_CONTRIBS = 5\n",
    "    CONTIB_SCALES = [1, 250, 45, 35, 500]\n",
    "    \n",
    "    def __init__(self, n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            create_contrib_net(n_in, n_h, act, dropout, layer_norm) \n",
    "            for _ in range(self.N_CONTRIBS)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ys = torch.cat([b(x) / s for b, s in zip(self.blocks, self.CONTIB_SCALES)], dim=-1)\n",
    "        return torch.cat([ys[:,:-1], ys.sum(dim=-1, keepdim=True)], dim=-1)\n",
    "\n",
    "class MyCustomHead(nn.Module):\n",
    "    N_TYPES = 8\n",
    "    \n",
    "    def __init__(self, n_input, n_h_contribs, pre_layers=[], post_layers=[], \n",
    "                 act=nn.ReLU(True), dropout=[], norm=False):\n",
    "        super().__init__()\n",
    "        n_pre_layers = len(pre_layers)\n",
    "        self.preproc = FullyConnectedNet(n_input, None, pre_layers, act, \n",
    "                                         dropout[:n_pre_layers], batch_norm=norm)\n",
    "        self.types_net = nn.ModuleList([\n",
    "            FullyConnectedNet(pre_layers[-1], None, post_layers, act, dropout[n_pre_layers:-1], layer_norm=norm)\n",
    "            for _ in range(self.N_TYPES)\n",
    "        ])\n",
    "        self.contribs_net = ContribsNet(post_layers[-1], n_h_contribs, act, dropout[-1], layer_norm=norm)\n",
    "        \n",
    "    def forward(self, x, sc_types):\n",
    "        x_ = self.preproc(x)\n",
    "        x_types = torch.zeros_like(x)\n",
    "        for i in range(self.N_TYPES):\n",
    "            if torch.any(sc_types==i): \n",
    "                x_types[sc_types==i] = self.types_net[i](x_[sc_types==i])\n",
    "        x = x + x_types \n",
    "        y = self.contribs_net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines all the the components of the readout function described in the MPNN network using set2set processing and the scalar coupling type customized head. Also adds in some skip connections to final node states and scalar coupling input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readout(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_sc_m, proc_steps=10, readout_type='Set2SetGRU', net_args={}):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(n_h + n_x, n_h)\n",
    "        if readout_type=='Sum': self.proc = SumReadout()\n",
    "        if readout_type=='Mean': self.proc = MeanReadout()\n",
    "        if readout_type=='Set2SetGRU': self.proc = Set2SetGRU(n_h, proc_steps)\n",
    "        if readout_type=='Set2SetLSTM': self.proc = Set2SetLSTM(n_h, proc_steps)\n",
    "        if readout_type=='Set2SetIndRNN': self.proc = Set2SetIndRNN(n_h, proc_steps)\n",
    "        if readout_type=='GaussAtt': self.proc = GaussAtt(dim=1, type_specific=True)\n",
    "        n_readout = n_h if ((readout_type=='Sum') or (readout_type=='Mean')) else 2 * n_h \n",
    "        self.write_head = MyCustomHead(n_readout + (4 * n_h) + n_sc_m, **net_args)\n",
    "    \n",
    "    def forward(self, h, x, sc_h, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types, dists, \n",
    "                sc_node_idx, sc_idx2):\n",
    "        \"\"\"\n",
    "        Make prediction.\n",
    "        - h is vector of hidden states of shape (batch_size * n_nodes, n_h).\n",
    "        - x is vector of input features of shape (batch_size * n_nodes, n_x).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        m = self.proj(torch.cat([h, x], dim=1))\n",
    "        if isinstance(self.proc, GaussAtt): \n",
    "            q = self.proc(m, sc_pairs_idx, sc_types, dists, sc_node_idx, sc_idx2)\n",
    "        else: \n",
    "            q = self.proc(m, node_idx).index_select(0, sc_idx)\n",
    "        \n",
    "        # introduce skip connection to final node states of scalar coupling atoms\n",
    "        inp = torch.cat([\n",
    "            q,\n",
    "            h.index_select(0, sc_pairs_idx[:int(sc_pairs_idx.size(0)/2),0]),\n",
    "            h.index_select(0, sc_pairs_idx[:int(sc_pairs_idx.size(0)/2),1]),\n",
    "            sc_h[:int(sc_pairs_idx.size(0)/2)],\n",
    "            sc_h[int(sc_pairs_idx.size(0)/2):],\n",
    "            sc_m\n",
    "        ], dim=-1)\n",
    "        y = self.write_head(inp, sc_types)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines the edge-network message (M), GRU update (U) and set2set readout (R) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps=3, proc_steps=10, \n",
    "                 readout_type='Set2SetGRU', preproc_net_args={}, enn_args={}, ann_args={}, \n",
    "                 R_net_args={}):\n",
    "        super().__init__()\n",
    "        self.preproc_net = FullyConnectedNet(n_x, n_h, **preproc_net_args)\n",
    "        self.sc_preproc_net = FullyConnectedNet(n_sc_e, n_h, **preproc_net_args)\n",
    "        self.M = ENNMessage(n_h, n_e, n_h, enn_args, ann_args)\n",
    "        self.U = GRUUpdate(n_h, n_h)\n",
    "        self.sc_U = GRUUpdate(2*n_h, n_h)\n",
    "        self.R = Readout(n_x, n_h, n_sc_m, proc_steps, readout_type, R_net_args)\n",
    "        self.update_steps = update_steps\n",
    "        \n",
    "    def forward(self, x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, \n",
    "                dists, sc_node_idx, sc_idx2, angles, angles_idx, sc_types):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: tensor of node features of shape (batch_size * n_nodes, n_x).\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_e: tensor of scalar coupling edge features of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        h = self.preproc_net(x)\n",
    "        sc_h = self.sc_preproc_net(sc_e)\n",
    "        for t in range(self.update_steps):\n",
    "            m = self.M(h, e, sc_h, pairs_idx, sc_pairs_idx, angles, angles_idx, t)\n",
    "            h = self.U(m, h)\n",
    "            sc_x = torch.cat([h.index_select(0, sc_pairs_idx[:,0]), \n",
    "                              h.index_select(0, sc_pairs_idx[:,1])], dim=-1)\n",
    "            sc_h = self.sc_U(sc_x, sc_h)\n",
    "        y = self.R(h, x, sc_h, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types, dists,\n",
    "                   sc_node_idx, sc_idx2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "# mol_ids = train_df['molecule_id'].unique()\n",
    "# n_obs = len(mol_ids)\n",
    "# split = int(n_obs*0.75)\n",
    "# mol_ids_ = np.random.choice(mol_ids, size=n_obs, replace=False)\n",
    "# train_mol_ids, val_mol_ids = pd.Series(mol_ids_[:split]), pd.Series(mol_ids_[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def scale_features(df, features, train_mol_ids):\n",
    "    idx = df['molecule_id'].isin(train_mol_ids)\n",
    "    return df.loc[idx, features].mean(), df.loc[idx, features].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "if any(train_df[SC_FEATS_TO_SCALE].mean().abs()>0.1) or any((train_df[SC_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    sc_feat_means, sc_feat_stds = scale_features(train_df, SC_FEATS_TO_SCALE, train_mol_ids)\n",
    "    train_df[SC_FEATS_TO_SCALE] = (train_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "    test_df[SC_FEATS_TO_SCALE] = (test_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "if any(atom_df[ATOM_FEATS_TO_SCALE].mean().abs()>0.1) or any((atom_df[ATOM_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    atom_feat_means, atom_feat_stds = scale_features(atom_df, ATOM_FEATS_TO_SCALE, train_mol_ids)\n",
    "    atom_df[ATOM_FEATS_TO_SCALE] = (atom_df[ATOM_FEATS_TO_SCALE] - atom_feat_means) / atom_feat_stds\n",
    "if any(edge_df[EDGE_FEATS_TO_SCALE].mean().abs()>0.1) or any((edge_df[EDGE_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    edge_feat_means, edge_feat_stds = scale_features(edge_df, EDGE_FEATS_TO_SCALE, train_mol_ids)\n",
    "    edge_df[EDGE_FEATS_TO_SCALE] = (edge_df[EDGE_FEATS_TO_SCALE] - edge_feat_means) / edge_feat_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mol_sc = train_df.groupby('molecule_id')\n",
    "test_gb_mol_sc = test_df.groupby('molecule_id')\n",
    "gb_mol_atom = atom_df.groupby('molecule_id')\n",
    "gb_mol_edge = edge_df.groupby('molecule_id')\n",
    "gb_mol_dist = dist_df.groupby('molecule_id')\n",
    "gb_mol_angle_in = angle_in_df.groupby('molecule_id')\n",
    "gb_mol_angle_out = angle_out_df.groupby('molecule_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pytorch dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     20
    ]
   },
   "outputs": [],
   "source": [
    "def get_existing_group(gb, i):\n",
    "    try: group_df = gb.get_group(i)\n",
    "    except KeyError: group_df = None\n",
    "    return group_df\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_dist, \n",
    "                 gb_mol_angle_in, gb_mol_angle_out):\n",
    "        self.n = len(mol_ids)\n",
    "        self.mol_ids = mol_ids\n",
    "        self.gb_mol_sc = gb_mol_sc\n",
    "        self.gb_mol_atom = gb_mol_atom\n",
    "        self.gb_mol_edge = gb_mol_edge\n",
    "        self.gb_mol_dist = gb_mol_dist\n",
    "        self.gb_mol_angle_in = gb_mol_angle_in\n",
    "        self.gb_mol_angle_out = gb_mol_angle_out\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gb_mol_sc.get_group(self.mol_ids[idx]),\n",
    "                self.gb_mol_atom.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_edge.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_dist.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_angle_in.get_group(self.mol_ids[idx]), \n",
    "                get_existing_group(self.gb_mol_angle_out, self.mol_ids[idx]))\n",
    "\n",
    "def np_lst_to_torch(arr_lst, dtype=torch.float):\n",
    "    return torch.from_numpy(np.ascontiguousarray(np.concatenate(arr_lst))).type(dtype)\n",
    "\n",
    "def collate_fn(batch, test=False):\n",
    "    batch_size, n_atom_sum, n_sc_sum, n_pairs_sum = len(batch), 0, 0, 0\n",
    "    x, e, sc_e, sc_m, dists, angles_in, angles_out = [], [], [], [], [], [], []\n",
    "    sc_types, sc_vals = [], []\n",
    "    node_idx, pairs_idx, sc_pairs_idx, sc_idx = [], [], [], []\n",
    "    sc_node_idx, sc_idx2, angles_in_idx, angles_out_idx = [], [], [], []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        sc_df, atom_df, edge_df, dist_df, angle_in_df, angle_out_df = batch[b]\n",
    "        n_atoms, n_sc, n_pairs = len(atom_df), len(sc_df), len(edge_df)\n",
    "        dists_ = dist_df.iloc[:,:-1].values\n",
    "        dists_[:,n_atoms:] = 999\n",
    "        \n",
    "        x.append(atom_df[ATOM_FEATS].values)\n",
    "        e.append(edge_df[EDGE_FEATS].values)\n",
    "        sc_e.append(sc_df[SC_EDGE_FEATS].values)\n",
    "        sc_m.append(sc_df[SC_MOL_FEATS].values)\n",
    "        sc_types.append(sc_df['type'].values)\n",
    "        if not test: sc_vals.append(sc_df[CONTRIB_COLS+[TARGET_COL]].values)\n",
    "        dists.append(dists_)\n",
    "        angles_in.append(angle_in_df['cos_angle'].values)\n",
    "        if angle_out_df is not None: angles_out.append(angle_out_df['cos_angle'].values)\n",
    "        \n",
    "        node_idx.append(np.repeat(b, n_atoms))\n",
    "        sc_idx.append(np.repeat(b, n_sc))\n",
    "        pairs_idx.append(edge_df[['idx_0', 'idx_1']].values + n_atom_sum)\n",
    "        sc_pairs_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values + n_atom_sum)\n",
    "        angles_in_idx.append(angle_in_df['p_idx'].values + n_pairs_sum)\n",
    "        if angle_out_df is not None: angles_out_idx.append(angle_out_df['p_idx'].values + n_pairs_sum)\n",
    "        sc_node_idx.append(n_sc * list(range(n_atom_sum, n_atom_sum + n_atoms)))\n",
    "        for i in range(n_sc): sc_idx2.append(n_atoms*[i+n_sc_sum])\n",
    "        \n",
    "        n_atom_sum += n_atoms\n",
    "        n_sc_sum += n_sc\n",
    "        n_pairs_sum += n_pairs\n",
    "        \n",
    "    x, e = np_lst_to_torch(x), np_lst_to_torch(e), \n",
    "    sc_e, sc_m = np_lst_to_torch(sc_e), np_lst_to_torch(sc_m)\n",
    "    sc_e = torch.cat((sc_e, sc_e))\n",
    "    if not test: sc_vals = np_lst_to_torch(sc_vals)\n",
    "    else: sc_vals = torch.tensor([0] * len(sc_types))\n",
    "    sc_types = np_lst_to_torch(sc_types, torch.long)\n",
    "    node_idx = np_lst_to_torch(node_idx, torch.long)\n",
    "    sc_idx = np_lst_to_torch(sc_idx, torch.long)\n",
    "    pairs_idx = np_lst_to_torch(pairs_idx, torch.long)\n",
    "    pairs_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n",
    "    sc_pairs_idx = np_lst_to_torch(sc_pairs_idx, torch.long)\n",
    "    sc_pairs_idx = torch.cat((sc_pairs_idx, sc_pairs_idx[:, [1, 0]]))\n",
    "    angles_in_idx = np_lst_to_torch(angles_in_idx, torch.long)\n",
    "    angles_out_idx = np_lst_to_torch(angles_out_idx, torch.long) + n_pairs_sum\n",
    "    angles_idx = torch.cat((angles_in_idx, angles_out_idx))\n",
    "    sc_node_idx = np_lst_to_torch(sc_node_idx, torch.long)\n",
    "    sc_idx2 = np_lst_to_torch(sc_idx2, torch.long)\n",
    "    dists = np_lst_to_torch(dists)\n",
    "    angles = np_lst_to_torch(angles_in + angles_out)\n",
    "    \n",
    "    return (x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, \n",
    "            dists, sc_node_idx, sc_idx2, angles, angles_idx, sc_types), sc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MoleculeDataset(train_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_dist, gb_mol_angle_in, gb_mol_angle_out)\n",
    "val_ds   = MoleculeDataset(val_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_dist, gb_mol_angle_in, gb_mol_angle_out)\n",
    "test_ds  = MoleculeDataset(test_mol_ids, test_gb_mol_sc, gb_mol_atom, gb_mol_edge, gb_mol_dist, gb_mol_angle_in, gb_mol_angle_out)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8)\n",
    "val_dl   = DataLoader(val_ds, batch_size, num_workers=8)\n",
    "test_dl  = DeviceDataLoader.create(test_ds, batch_size, num_workers=8, collate_fn=partial(collate_fn, test=True))\n",
    "db = DataBunch(train_dl, val_dl, collate_fn=collate_fn)\n",
    "db.test_dl = test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([382, 21])\n",
      "torch.Size([394, 8])\n",
      "torch.Size([2420, 16])\n",
      "torch.Size([1210, 25])\n",
      "torch.Size([382])\n",
      "torch.Size([788, 2])\n",
      "torch.Size([1210])\n",
      "torch.Size([2420, 2])\n",
      "torch.Size([382, 29])\n",
      "torch.Size([24194])\n",
      "torch.Size([24194])\n",
      "torch.Size([1496])\n",
      "torch.Size([1496])\n",
      "torch.Size([1210])\n",
      "torch.Size([1210, 5])\n"
     ]
    }
   ],
   "source": [
    "for el in batch[0]: print(el.size())\n",
    "print(batch[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([[ 0.0000,  1.0000,  0.0000,  ...,  0.8505, -2.6217, -0.3045],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ...,  0.7247, -0.1754, -0.3045],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.6925,  0.4362,  3.2838],\n",
      "        ...,\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9125,  0.4362, -0.3045],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9126,  0.4362, -0.3045],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.9170,  0.4362, -0.3045]])\n",
      "e:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  1.2334,  1.1838],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8915, -0.9193],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8735, -0.9015],\n",
      "        ...,\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8706, -0.8137],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8699, -0.8130],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000, -0.8964, -0.8407]])\n",
      "sc_e:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3350],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.3590,  0.7562, -0.7372],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.3811, -0.8274],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.8904, -0.1125],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.3403,  0.7475, -0.0595],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3374]])\n",
      "sc_m:\n",
      " tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.1355,  0.2689, -0.5096],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.1355,  0.2689, -0.5096],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.1355,  0.2689, -0.5096],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4986, -0.6563, -0.5096],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4986, -0.6563, -0.5096],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -0.4986, -0.6563, -0.5096]])\n",
      "node_idx:\n",
      " tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "         4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19])\n",
      "pairs_idx:\n",
      " tensor([[  0,   1],\n",
      "        [  0,   9],\n",
      "        [  0,  10],\n",
      "        ...,\n",
      "        [380, 368],\n",
      "        [379, 368],\n",
      "        [381, 368]])\n",
      "sc_idx:\n",
      " tensor([ 0,  0,  0,  ..., 19, 19, 19])\n",
      "sc_pairs_idx:\n",
      " tensor([[  9,   0],\n",
      "        [  9,   1],\n",
      "        [  9,   8],\n",
      "        ...,\n",
      "        [366, 381],\n",
      "        [367, 381],\n",
      "        [368, 381]])\n",
      "dists:\n",
      " tensor([[  0.0000,   1.5166,   2.4504,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  1.5166,   0.0000,   1.4511,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  2.4504,   1.4511,   0.0000,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        ...,\n",
      "        [  7.3542,   6.6395,   5.8141,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  6.9260,   6.4147,   5.4238,  ..., 999.0000, 999.0000, 999.0000],\n",
      "        [  7.5899,   7.2147,   6.4965,  ..., 999.0000, 999.0000, 999.0000]])\n",
      "sc_node_idx:\n",
      " tensor([  0,   1,   2,  ..., 379, 380, 381])\n",
      "sc_idx2:\n",
      " tensor([   0,    0,    0,  ..., 1209, 1209, 1209])\n",
      "angles:\n",
      " tensor([-0.3590, -0.3203, -0.3624,  ..., -0.3486, -0.3481, -0.3403])\n",
      "angles_idx:\n",
      " tensor([  0,   0,   0,  ..., 783, 783, 783])\n",
      "sc_types:\n",
      " tensor([0, 4, 6,  ..., 6, 4, 0])\n",
      "y:\n",
      " tensor([[ 2.0939e+00,  5.7855e-03,  1.8053e-02,  2.4014e-02,  2.1417e+00],\n",
      "        [-5.8224e-01,  2.7488e-03, -5.1135e-03,  2.4645e-03, -5.8214e-01],\n",
      "        [-4.0959e-01, -6.2886e-04, -2.5231e-03,  3.8351e-03, -4.0890e-01],\n",
      "        ...,\n",
      "        [-3.2269e-01,  1.3919e-03,  7.2254e-03, -1.1281e-02, -3.2535e-01],\n",
      "        [-6.0649e-01,  1.8663e-04, -9.2220e-03, -1.1164e-03, -6.1664e-01],\n",
      "        [ 2.0211e+00,  5.7765e-03,  2.2478e-02,  1.8577e-02,  2.0679e+00]])\n"
     ]
    }
   ],
   "source": [
    "b_dict = dict(x=batch[0][0], \n",
    "              e=batch[0][1], \n",
    "              sc_e=batch[0][2], \n",
    "              sc_m=batch[0][3], \n",
    "              node_idx=batch[0][4], \n",
    "              pairs_idx=batch[0][5], \n",
    "              sc_idx=batch[0][6], \n",
    "              sc_pairs_idx=batch[0][7], \n",
    "              dists=batch[0][8], \n",
    "              sc_node_idx=batch[0][9], \n",
    "              sc_idx2=batch[0][10], \n",
    "              angles=batch[0][11],\n",
    "              angles_idx=batch[0][12],\n",
    "              sc_types=batch[0][13], \n",
    "              y=batch[1])\n",
    "for k,v in b_dict.items(): print(f'{k}:\\n {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the metric used for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10,
     31,
     43,
     46
    ]
   },
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types, epoch):\n",
    "    proc = lambda x: x.cpu().numpy().ravel() \n",
    "    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n",
    "    y_true = SC_MEAN + y_true * SC_STD\n",
    "    y_pred = SC_MEAN + y_pred * SC_STD\n",
    "    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n",
    "    gmlmae = np.log(maes).mean()\n",
    "    # print(f'Epoch: {epoch} - Group Mean Log Mae: {gmlmae}')\n",
    "    return gmlmae\n",
    "\n",
    "class GroupMeanLogMAE(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['group_mean_log_mae'])\n",
    "    def on_epoch_begin(self, **kwargs): self.input, self.output, self.target = [], [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, last_input, train, **kwargs):\n",
    "        if not train:\n",
    "            self.input.append(last_input[-1])\n",
    "            self.output.append(last_output[:,-1])\n",
    "            self.target.append(last_target[:,-1])\n",
    "                \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n",
    "        if (len(self.input) > 0) and (len(self.output) > 0):\n",
    "            inputs = torch.cat(self.input)\n",
    "            preds = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            metric = group_mean_log_mae(preds, target, inputs, epoch)\n",
    "            return add_metrics(last_metrics, [metric])\n",
    "\n",
    "def contribs_rmse_loss(preds, targs):\n",
    "    \"\"\"\n",
    "    Returns the sum of RMSEs for each sc contribution and total sc value.\n",
    "    \n",
    "    Args:\n",
    "        - preds: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            predictions. Last column is the total scalar coupling value.\n",
    "        - targs: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            true values. Last column is the total scalar coupling value.\n",
    "    \"\"\"\n",
    "    return torch.mean((preds - targs) ** 2, dim=0).sqrt().sum()\n",
    "\n",
    "def rmse(preds, targs):\n",
    "    return torch.sqrt(F.mse_loss(preds[:,-1], targs[:,-1]))\n",
    "\n",
    "def mae(preds, targs):\n",
    "    return torch.abs(preds[:,-1] - targs[:,-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd, norm, act = 0, True, nn.ReLU(True)\n",
    "update_steps, proc_steps, readout_type = 5, 6, 'GaussAtt'\n",
    "n_x, n_h, n_e, n_sc_e, n_sc_m = N_ATOM_FEATURES, 300, N_EDGE_FEATURES, N_SC_EDGE_FEATURES, N_SC_MOL_FEATURES\n",
    "preproc_net_args = dict(layers=[], act=act, dropout=[], out_act=nn.Tanh())\n",
    "enn_args = dict(layers=3*[n_h], act=act, dropout=3*[0.0], batch_norm=norm)\n",
    "ann_args = dict(layers=1*[n_h], act=act, dropout=1*[0.0], batch_norm=norm, out_act=nn.Tanh())\n",
    "R_net_args = dict(pre_layers=[n_sc_m+6*n_h], post_layers=[n_sc_m+6*n_h], n_h_contribs=200, act=act, dropout=[0.0, 0.0, 0.0], \n",
    "                  norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "model = MPNN(n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps, proc_steps, readout_type, \n",
    "             preproc_net_args, enn_args, ann_args, R_net_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNN(\n",
      "  (preproc_net): FullyConnectedNet(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=21, out_features=300, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (sc_preproc_net): FullyConnectedNet(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=300, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (M): ENNMessage(\n",
      "    (enn): FullyConnectedNet(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=8, out_features=300, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (4): ReLU(inplace)\n",
      "        (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (7): ReLU(inplace)\n",
      "        (8): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): Linear(in_features=300, out_features=90000, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (sc_enn): FullyConnectedNet(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (4): ReLU(inplace)\n",
      "        (5): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (7): ReLU(inplace)\n",
      "        (8): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): Linear(in_features=300, out_features=90000, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (ann): FullyConnectedNet(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=300, bias=True)\n",
      "        (1): ReLU(inplace)\n",
      "        (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (4): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (U): GRUUpdate(\n",
      "    (gru): GRUCell(300, 300)\n",
      "  )\n",
      "  (sc_U): GRUUpdate(\n",
      "    (gru): GRUCell(600, 300)\n",
      "  )\n",
      "  (R): Readout(\n",
      "    (proj): Linear(in_features=321, out_features=300, bias=True)\n",
      "    (proc): GaussAtt()\n",
      "    (write_head): MyCustomHead(\n",
      "      (preproc): FullyConnectedNet(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): BatchNorm1d(1825, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (types_net): ModuleList(\n",
      "        (0): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (4): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (5): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (6): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (7): FullyConnectedNet(\n",
      "          (layers): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=1825, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([1825]), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (contribs_net): ContribsNet(\n",
      "        (blocks): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=200, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([200]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=200, out_features=1, bias=True)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=200, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([200]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=200, out_features=1, bias=True)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=200, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([200]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=200, out_features=1, bias=True)\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=200, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([200]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=200, out_features=1, bias=True)\n",
      "          )\n",
      "          (4): Sequential(\n",
      "            (0): Linear(in_features=1825, out_features=200, bias=True)\n",
      "            (1): ReLU(inplace)\n",
      "            (2): LayerNorm(torch.Size([200]), eps=1e-05, elementwise_affine=True)\n",
      "            (3): Linear(in_features=200, out_features=1, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[-6.2098e-01,  2.4012e-03,  1.0733e-02, -2.7955e-02, -6.3651e-01],\n",
      "        [-1.0031e-01,  4.5088e-03,  5.6625e-03, -1.1181e-02, -1.0119e-01],\n",
      "        [ 4.4674e-01,  3.4065e-04,  8.0591e-03, -6.1859e-03,  4.4910e-01],\n",
      "        ...,\n",
      "        [-4.4966e-01, -6.1768e-04,  2.5193e-03,  4.7339e-02, -4.0111e-01],\n",
      "        [-1.8475e-01,  5.4551e-03,  4.4445e-03, -3.9219e-03, -1.7920e-01],\n",
      "        [-1.2094e+00,  7.0289e-03, -6.6059e-03, -1.5389e-03, -1.2117e+00]],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([1210, 5])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model(*batch[0]))\n",
    "print(model(*batch[0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(LearnerCallback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, learn:Learner, clip:float = 0., start_it:int = 100):\n",
    "        super().__init__(learn)\n",
    "        self.clip, self.start_it = clip, start_it\n",
    "\n",
    "    def on_backward_end(self, iteration, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip and (iteration > self.start_it): nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics=[rmse, mae], \n",
    "                callback_fns=[partial(GradientClipping, clip=10), GroupMeanLogMAE], \n",
    "                wd=wd, loss_func=contribs_rmse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-6, end_lr=1.0, num_it=100, stop_div=True)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='79' class='' max='80', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      98.75% [79/80 15:37:27<11:51]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>group_mean_log_mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.055838</td>\n",
       "      <td>0.051193</td>\n",
       "      <td>0.023935</td>\n",
       "      <td>0.016604</td>\n",
       "      <td>-0.566951</td>\n",
       "      <td>11:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.042278</td>\n",
       "      <td>0.040899</td>\n",
       "      <td>0.019102</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.844567</td>\n",
       "      <td>11:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.037187</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>0.015956</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>-1.025107</td>\n",
       "      <td>11:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034143</td>\n",
       "      <td>0.032814</td>\n",
       "      <td>0.015277</td>\n",
       "      <td>0.010196</td>\n",
       "      <td>-1.043264</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.032854</td>\n",
       "      <td>0.032731</td>\n",
       "      <td>0.015411</td>\n",
       "      <td>0.009966</td>\n",
       "      <td>-1.097067</td>\n",
       "      <td>11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029757</td>\n",
       "      <td>0.029993</td>\n",
       "      <td>0.014031</td>\n",
       "      <td>0.009458</td>\n",
       "      <td>-1.207651</td>\n",
       "      <td>11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030737</td>\n",
       "      <td>0.029857</td>\n",
       "      <td>0.014016</td>\n",
       "      <td>0.009421</td>\n",
       "      <td>-1.169284</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027328</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>-1.308810</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026467</td>\n",
       "      <td>0.026819</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.008255</td>\n",
       "      <td>-1.352657</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>-1.379290</td>\n",
       "      <td>11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.024079</td>\n",
       "      <td>0.024164</td>\n",
       "      <td>0.011389</td>\n",
       "      <td>0.007518</td>\n",
       "      <td>-1.472877</td>\n",
       "      <td>11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.023604</td>\n",
       "      <td>0.026378</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>-1.471905</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.024666</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.010863</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>-1.459841</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.021959</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>-1.562545</td>\n",
       "      <td>11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020864</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.009879</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>-1.571901</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.020625</td>\n",
       "      <td>0.020429</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>-1.651775</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>-1.705700</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018665</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>-1.703154</td>\n",
       "      <td>11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.017763</td>\n",
       "      <td>0.020679</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>-1.687527</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.017103</td>\n",
       "      <td>0.018680</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>-1.755957</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>0.019838</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>-1.509079</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.017141</td>\n",
       "      <td>0.008027</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>-1.863006</td>\n",
       "      <td>11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>0.017359</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>-1.803111</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.014992</td>\n",
       "      <td>0.016865</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>0.004928</td>\n",
       "      <td>-1.866578</td>\n",
       "      <td>11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.014018</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>-1.924623</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>-1.937282</td>\n",
       "      <td>11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.013294</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>-1.955444</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.012801</td>\n",
       "      <td>0.015103</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>-1.997946</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>0.015487</td>\n",
       "      <td>0.007303</td>\n",
       "      <td>0.004262</td>\n",
       "      <td>-2.023213</td>\n",
       "      <td>11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.015270</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>-1.937669</td>\n",
       "      <td>11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>0.006641</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>-2.042817</td>\n",
       "      <td>11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>-2.119541</td>\n",
       "      <td>11:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.014587</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>-2.058243</td>\n",
       "      <td>11:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>-2.089952</td>\n",
       "      <td>11:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.013755</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>-2.095427</td>\n",
       "      <td>11:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.009623</td>\n",
       "      <td>0.014250</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>-2.133434</td>\n",
       "      <td>11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>0.014604</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.004238</td>\n",
       "      <td>-1.996279</td>\n",
       "      <td>11:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.009087</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>-2.080820</td>\n",
       "      <td>11:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.012236</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>-2.242515</td>\n",
       "      <td>11:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>-2.118969</td>\n",
       "      <td>11:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>-2.179128</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.007934</td>\n",
       "      <td>0.012043</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.003321</td>\n",
       "      <td>-2.248224</td>\n",
       "      <td>11:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.012035</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>-2.259632</td>\n",
       "      <td>11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.007366</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>-2.319148</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>-2.322452</td>\n",
       "      <td>11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.006722</td>\n",
       "      <td>0.013368</td>\n",
       "      <td>0.006302</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>-2.322901</td>\n",
       "      <td>11:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.006514</td>\n",
       "      <td>0.011248</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>-2.362859</td>\n",
       "      <td>11:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.006231</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>-2.223651</td>\n",
       "      <td>11:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.011044</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.002974</td>\n",
       "      <td>-2.351483</td>\n",
       "      <td>11:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>-2.380600</td>\n",
       "      <td>11:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.011613</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>-2.426202</td>\n",
       "      <td>11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>-2.428280</td>\n",
       "      <td>11:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.010687</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>-2.331587</td>\n",
       "      <td>11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>-2.426324</td>\n",
       "      <td>11:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>-2.459283</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.010401</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>-2.510347</td>\n",
       "      <td>11:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.004540</td>\n",
       "      <td>0.010542</td>\n",
       "      <td>0.004973</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>-2.498955</td>\n",
       "      <td>11:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>-2.514616</td>\n",
       "      <td>11:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.010806</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>-2.507350</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.010839</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>-2.514887</td>\n",
       "      <td>11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>-2.515852</td>\n",
       "      <td>11:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-2.547750</td>\n",
       "      <td>11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>-2.561794</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>-2.550322</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.010591</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>-2.576280</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.010056</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>-2.558741</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>-2.589716</td>\n",
       "      <td>11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>-2.579835</td>\n",
       "      <td>11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>-2.575892</td>\n",
       "      <td>11:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.010163</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>-2.573094</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>-2.579907</td>\n",
       "      <td>11:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>-2.615561</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.009903</td>\n",
       "      <td>0.004668</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>-2.604725</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>-2.607676</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>-2.621642</td>\n",
       "      <td>11:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>-2.613095</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>-2.621960</td>\n",
       "      <td>11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.009658</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>-2.623777</td>\n",
       "      <td>11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.010477</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>-2.620282</td>\n",
       "      <td>11:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3562' class='' max='3719', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      95.78% [3562/3719 10:40<00:28 0.0025]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with group_mean_log_mae value: -0.5669506788253784.\n",
      "Better model found at epoch 1 with group_mean_log_mae value: -0.8445674777030945.\n",
      "Better model found at epoch 2 with group_mean_log_mae value: -1.0251069068908691.\n",
      "Better model found at epoch 3 with group_mean_log_mae value: -1.0432639122009277.\n",
      "Better model found at epoch 4 with group_mean_log_mae value: -1.0970665216445923.\n",
      "Better model found at epoch 5 with group_mean_log_mae value: -1.2076513767242432.\n",
      "Better model found at epoch 7 with group_mean_log_mae value: -1.3088102340698242.\n",
      "Better model found at epoch 8 with group_mean_log_mae value: -1.3526568412780762.\n",
      "Better model found at epoch 9 with group_mean_log_mae value: -1.3792901039123535.\n",
      "Better model found at epoch 10 with group_mean_log_mae value: -1.4728772640228271.\n",
      "Better model found at epoch 13 with group_mean_log_mae value: -1.5625452995300293.\n",
      "Better model found at epoch 14 with group_mean_log_mae value: -1.5719006061553955.\n",
      "Better model found at epoch 15 with group_mean_log_mae value: -1.651774525642395.\n",
      "Better model found at epoch 16 with group_mean_log_mae value: -1.7057002782821655.\n",
      "Better model found at epoch 19 with group_mean_log_mae value: -1.7559574842453003.\n",
      "Better model found at epoch 21 with group_mean_log_mae value: -1.8630064725875854.\n",
      "Better model found at epoch 23 with group_mean_log_mae value: -1.8665779829025269.\n",
      "Better model found at epoch 24 with group_mean_log_mae value: -1.9246234893798828.\n",
      "Better model found at epoch 25 with group_mean_log_mae value: -1.9372824430465698.\n",
      "Better model found at epoch 26 with group_mean_log_mae value: -1.9554439783096313.\n",
      "Better model found at epoch 27 with group_mean_log_mae value: -1.9979463815689087.\n",
      "Better model found at epoch 28 with group_mean_log_mae value: -2.0232131481170654.\n",
      "Better model found at epoch 30 with group_mean_log_mae value: -2.042816638946533.\n",
      "Better model found at epoch 31 with group_mean_log_mae value: -2.1195406913757324.\n",
      "Better model found at epoch 35 with group_mean_log_mae value: -2.1334335803985596.\n",
      "Better model found at epoch 38 with group_mean_log_mae value: -2.2425148487091064.\n",
      "Better model found at epoch 41 with group_mean_log_mae value: -2.2482242584228516.\n",
      "Better model found at epoch 42 with group_mean_log_mae value: -2.2596323490142822.\n",
      "Better model found at epoch 43 with group_mean_log_mae value: -2.3191475868225098.\n",
      "Better model found at epoch 44 with group_mean_log_mae value: -2.3224518299102783.\n",
      "Better model found at epoch 45 with group_mean_log_mae value: -2.3229005336761475.\n",
      "Better model found at epoch 46 with group_mean_log_mae value: -2.362858533859253.\n",
      "Better model found at epoch 49 with group_mean_log_mae value: -2.3806002140045166.\n",
      "Better model found at epoch 50 with group_mean_log_mae value: -2.4262022972106934.\n",
      "Better model found at epoch 51 with group_mean_log_mae value: -2.4282796382904053.\n",
      "Better model found at epoch 54 with group_mean_log_mae value: -2.4592833518981934.\n",
      "Better model found at epoch 55 with group_mean_log_mae value: -2.5103468894958496.\n",
      "Better model found at epoch 57 with group_mean_log_mae value: -2.5146162509918213.\n",
      "Better model found at epoch 59 with group_mean_log_mae value: -2.5148873329162598.\n",
      "Better model found at epoch 60 with group_mean_log_mae value: -2.515852451324463.\n",
      "Better model found at epoch 61 with group_mean_log_mae value: -2.547750234603882.\n",
      "Better model found at epoch 62 with group_mean_log_mae value: -2.561793565750122.\n",
      "Better model found at epoch 64 with group_mean_log_mae value: -2.576280117034912.\n",
      "Better model found at epoch 66 with group_mean_log_mae value: -2.5897157192230225.\n",
      "Better model found at epoch 71 with group_mean_log_mae value: -2.61556077003479.\n",
      "Better model found at epoch 74 with group_mean_log_mae value: -2.6216418743133545.\n",
      "Better model found at epoch 76 with group_mean_log_mae value: -2.621959686279297.\n",
      "Better model found at epoch 77 with group_mean_log_mae value: -2.623777389526367.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(80, max_lr=4e-4, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  \n",
    "                                                                  name=f'mpnn_v{VERSION}_fold{FOLD_ID}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(skip_start=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_contrib_preds = learn.get_preds(DatasetType.Valid)\n",
    "test_contrib_preds = learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = val_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN\n",
    "test_preds = test_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_submit(predictions):\n",
    "    submit = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "    print(len(submit), len(predictions))   \n",
    "    submit['scalar_coupling_constant'] = predictions\n",
    "    submit.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-submission.csv', index=False)\n",
    "\n",
    "def store_oof(predictions, val_ids):\n",
    "    oof = pd.DataFrame(predictions, columns=['scalar_coupling_constants'])\n",
    "    print(oof.head())\n",
    "    oof.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-oof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_submit(test_preds)\n",
    "store_oof(val_preds, val_mol_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
