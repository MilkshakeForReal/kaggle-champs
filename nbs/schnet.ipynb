{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "from fastai.basic_data import DataBunch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __print__ = print\n",
    "# def print(*strings):\n",
    "#     for string in strings:\n",
    "#         os.system(f'echo \\\"{string}\\\"')\n",
    "#         __print__(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_ID = 2\n",
    "VERSION = 2\n",
    "\n",
    "TYPES              = np.array(['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'])\n",
    "TYPES_MAP          = {t: i for i, t in enumerate(TYPES)}\n",
    "SC_EDGE_FEATS      = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "SC_MOL_FEATS       = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7', \n",
    "                      'dist', 'dist_min_rad', 'dist_electro_neg_adj', 'normed_dist', \n",
    "                      'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', \n",
    "                      'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', \n",
    "                      #'inv_dist', 'normed_inv_dist', \n",
    "                      'std_bond_length', 'ave_bond_length', #'total_bond_length',  \n",
    "                      #'ave_inv_bond_length', 'total_inv_bond_length', \n",
    "                      'ave_atom_weight'#, 'total_atom_weight'\n",
    "                     ]\n",
    "ATOM_FEATS         = ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', \n",
    "                      'degree_1', 'degree_2', 'degree_3', 'degree_4', 'degree_5', \n",
    "                      'SP', 'SP2', 'SP3', 'hybridization_unspecified', \n",
    "                      'aromatic', 'formal_charge', 'atomic_num',\n",
    "                      'donor', 'acceptor', \n",
    "                      'ave_bond_length', \n",
    "                      #'ave_inv_bond_length',\n",
    "                      'ave_neighbor_weight']\n",
    "EDGE_FEATS         = ['single', 'double', 'triple', 'aromatic', \n",
    "                      'conjugated', 'in_ring',\n",
    "                      'dist', 'normed_dist', \n",
    "                      #'inv_dist', 'normed_inv_dist'\n",
    "                     ]\n",
    "TARGET_COL         = 'scalar_coupling_constant'\n",
    "CONTRIB_COLS       = ['fc', 'sd', 'pso', 'dso']\n",
    "N_EDGE_FEATURES    = len(EDGE_FEATS)\n",
    "N_SC_EDGE_FEATURES = len(SC_EDGE_FEATS)\n",
    "N_SC_MOL_FEATURES  = len(SC_MOL_FEATS)\n",
    "N_ATOM_FEATURES    = len(ATOM_FEATS)\n",
    "N_TYPES            = len(TYPES)\n",
    "N_MOLS             = 130775\n",
    "SC_MEAN            = 16\n",
    "SC_STD             = 35\n",
    "\n",
    "SC_FEATS_TO_SCALE   = ['dist', 'dist_min_rad', 'dist_electro_neg_adj', 'num_atoms', 'num_C_atoms', \n",
    "                       'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', 'inv_dist', \n",
    "                       'ave_bond_length', 'std_bond_length', 'total_bond_length',  'ave_inv_bond_length', \n",
    "                       'total_inv_bond_length', 'ave_atom_weight', 'total_atom_weight']\n",
    "ATOM_FEATS_TO_SCALE = ['atomic_num', 'ave_bond_length', 'ave_inv_bond_length', 'ave_neighbor_weight']\n",
    "EDGE_FEATS_TO_SCALE = ['dist', 'inv_dist']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "PATH = '../tmp/'\n",
    "CV_IDXS_PATH = PATH\n",
    "# DATA_PATH = '../input/champs-scalar-coupling/'\n",
    "# PATH = '../input/champs-processed-data-2/'\n",
    "# CV_IDXS_PATH = '../input/champs-cv-4-fold-idxs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp/: ['atomic_features.csv', 'train_proc_df.csv', 'mask.csv', 'train_idxs_8_fold_cv.csv', 'edge_mask.csv', 'atom_df.csv', 'pairs_idx.csv', 'edge_df.csv', 'train_idxs_4_fold_cv.csv', 'edge_features.csv', 'val_idxs_8_fold_cv.csv', 'val_idxs_4_fold_cv.csv', 'test_proc_df.csv']\n",
      "../data/: ['scalar_coupling_contributions.csv', 'mulliken_charges.csv', 'structures.csv', 'test.csv', 'train.csv', 'magnetic_shielding_tensors.csv', 'dipole_moments.csv', 'sample_submission.csv', 'potential_energy.csv']\n",
      "../tmp/: ['atomic_features.csv', 'train_proc_df.csv', 'mask.csv', 'train_idxs_8_fold_cv.csv', 'edge_mask.csv', 'atom_df.csv', 'pairs_idx.csv', 'edge_df.csv', 'train_idxs_4_fold_cv.csv', 'edge_features.csv', 'val_idxs_8_fold_cv.csv', 'val_idxs_4_fold_cv.csv', 'test_proc_df.csv']\n"
     ]
    }
   ],
   "source": [
    "def show_csv_files(path):\n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if f.find('.csv') != -1]\n",
    "    print(f'{path}:', files)\n",
    "show_csv_files(PATH)\n",
    "show_csv_files(DATA_PATH)\n",
    "show_csv_files(CV_IDXS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/python36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(PATH+'train_proc_df.csv', index_col=0)\n",
    "test_df  = pd.read_csv(PATH+'test_proc_df.csv', index_col=0)\n",
    "atom_df  = pd.read_csv(PATH+'atom_df.csv', index_col=0)\n",
    "edge_df  = pd.read_csv(PATH+'edge_df.csv', index_col=0)\n",
    "\n",
    "train_mol_ids = pd.read_csv(CV_IDXS_PATH+'train_idxs_4_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "val_mol_ids   = pd.read_csv(CV_IDXS_PATH+'val_idxs_4_fold_cv.csv', usecols=[0, FOLD_ID], index_col=0).dropna().astype(int).iloc[:,0]\n",
    "test_mol_ids  = pd.Series(test_df['molecule_id'].unique())\n",
    "\n",
    "contribs_df = pd.read_csv(DATA_PATH+'scalar_coupling_contributions.csv')\n",
    "train_df = pd.concat((train_df, contribs_df[CONTRIB_COLS]), axis=1)\n",
    "del contribs_df\n",
    "gc.collect()\n",
    "\n",
    "train_df[[TARGET_COL, 'fc']] = (train_df[[TARGET_COL, 'fc']] - SC_MEAN) / SC_STD\n",
    "train_df[CONTRIB_COLS[1:]] = train_df[CONTRIB_COLS[1:]] / SC_STD\n",
    "\n",
    "train_df['num_atoms'] = train_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                  'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "test_df['num_atoms'] = test_df[['num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
    "                                'num_N_atoms', 'num_O_atoms']].sum(axis=1)\n",
    "train_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10\n",
    "test_df[['num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms']] /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>2.593389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914926</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035961</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.333287</td>\n",
       "      <td>0.816483</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>3.922863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772420</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.081668</td>\n",
       "      <td>-0.098103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.816496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783147</td>\n",
       "      <td>3.922924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772357</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081672</td>\n",
       "      <td>-0.098111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.333347</td>\n",
       "      <td>0.816502</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>3.922945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.772340</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>-0.098112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>2.593385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.367799</td>\n",
       "      <td>0.915793</td>\n",
       "      <td>3.663173</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.914920</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>0.007772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "0      H      C             1             0   0.000000    0.000000   \n",
       "1      H      H             1             2  -0.333287    0.816483   \n",
       "2      H      H             1             3  -0.333335    0.816498   \n",
       "3      H      H             1             4  -0.333347    0.816502   \n",
       "4      H      C             2             0   0.000000    0.000000   \n",
       "\n",
       "   cos_angle1  diangle      dist  dist_electro_neg_adj    ...     \\\n",
       "0   -0.333335      0.0  1.091953              2.593389    ...      \n",
       "1    0.816482      0.0  1.783120              3.922863    ...      \n",
       "2    0.816496      0.0  1.783147              3.922924    ...      \n",
       "3    0.816500      0.0  1.783157              3.922945    ...      \n",
       "4   -0.333352      0.0  1.091952              2.593385    ...      \n",
       "\n",
       "   std_bond_length  total_bond_length  ave_inv_bond_length  \\\n",
       "0         0.000003           4.367799             0.915793   \n",
       "1         0.000003           4.367799             0.915793   \n",
       "2         0.000003           4.367799             0.915793   \n",
       "3         0.000003           4.367799             0.915793   \n",
       "4         0.000003           4.367799             0.915793   \n",
       "\n",
       "   total_inv_bond_length  ave_atom_weight  total_atom_weight        fc  \\\n",
       "0               3.663173              0.2                1.0  1.914926   \n",
       "1               3.663173              0.2                1.0 -0.772420   \n",
       "2               3.663173              0.2                1.0 -0.772357   \n",
       "3               3.663173              0.2                1.0 -0.772340   \n",
       "4               3.663173              0.2                1.0  1.914920   \n",
       "\n",
       "         sd       pso       dso  \n",
       "0  0.007274  0.035961  0.007772  \n",
       "1  0.010085  0.081668 -0.098103  \n",
       "2  0.010084  0.081672 -0.098111  \n",
       "3  0.010084  0.081673 -0.098112  \n",
       "4  0.007274  0.035960  0.007772  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>cos_angle</th>\n",
       "      <th>cos_angle0</th>\n",
       "      <th>cos_angle1</th>\n",
       "      <th>diangle</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_electro_neg_adj</th>\n",
       "      <th>...</th>\n",
       "      <th>type_7</th>\n",
       "      <th>inv_dist</th>\n",
       "      <th>normed_inv_dist</th>\n",
       "      <th>ave_bond_length</th>\n",
       "      <th>std_bond_length</th>\n",
       "      <th>total_bond_length</th>\n",
       "      <th>ave_inv_bond_length</th>\n",
       "      <th>total_inv_bond_length</th>\n",
       "      <th>ave_atom_weight</th>\n",
       "      <th>total_atom_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>7.311210</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300908</td>\n",
       "      <td>-2.038452</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.062099</td>\n",
       "      <td>2.522485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>4.621731</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.261178</td>\n",
       "      <td>5.370298</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442247</td>\n",
       "      <td>-0.815269</td>\n",
       "      <td>1.107759</td>\n",
       "      <td>0.064573</td>\n",
       "      <td>3.323277</td>\n",
       "      <td>0.905679</td>\n",
       "      <td>2.717037</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        atom_0 atom_1  atom_index_0  atom_index_1  cos_angle  cos_angle0  \\\n",
       "4658147      H      C             2             0       -1.0         1.0   \n",
       "4658148      H      C             2             1        0.0         0.0   \n",
       "4658149      H      H             2             3        0.0         1.0   \n",
       "4658150      H      C             3             0        0.0         0.0   \n",
       "4658151      H      C             3             1       -1.0         1.0   \n",
       "\n",
       "         cos_angle1  diangle      dist  dist_electro_neg_adj  \\\n",
       "4658147        -1.0      0.0  2.261178              5.370298   \n",
       "4658148        -1.0      0.0  1.062099              2.522485   \n",
       "4658149         1.0      0.0  3.323277              7.311210   \n",
       "4658150        -1.0      0.0  1.062099              2.522485   \n",
       "4658151        -1.0      0.0  2.261178              5.370298   \n",
       "\n",
       "               ...          type_7  inv_dist  normed_inv_dist  \\\n",
       "4658147        ...               0  0.442247        -0.815269   \n",
       "4658148        ...               0  0.941532         4.621731   \n",
       "4658149        ...               0  0.300908        -2.038452   \n",
       "4658150        ...               0  0.941532         4.621731   \n",
       "4658151        ...               0  0.442247        -0.815269   \n",
       "\n",
       "         ave_bond_length  std_bond_length  total_bond_length  \\\n",
       "4658147         1.107759         0.064573           3.323277   \n",
       "4658148         1.107759         0.064573           3.323277   \n",
       "4658149         1.107759         0.064573           3.323277   \n",
       "4658150         1.107759         0.064573           3.323277   \n",
       "4658151         1.107759         0.064573           3.323277   \n",
       "\n",
       "         ave_inv_bond_length  total_inv_bond_length  ave_atom_weight  \\\n",
       "4658147             0.905679               2.717037             0.35   \n",
       "4658148             0.905679               2.717037             0.35   \n",
       "4658149             0.905679               2.717037             0.35   \n",
       "4658150             0.905679               2.717037             0.35   \n",
       "4658151             0.905679               2.717037             0.35   \n",
       "\n",
       "         total_atom_weight  \n",
       "4658147                1.4  \n",
       "4658148                1.4  \n",
       "4658149                1.4  \n",
       "4658150                1.4  \n",
       "4658151                1.4  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Softplus2(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.relu(x) + torch.log(0.5 * torch.exp(-x.abs()) + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General dense feedforward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     5,
     12,
     18,
     27,
     50
    ]
   },
   "outputs": [],
   "source": [
    "def bn_init(m): pass\n",
    "#     if type(m) == nn.BatchNorm1d: \n",
    "#         nn.init.ones_(m.weight)\n",
    "#         nn.init.zeros_(m.bias)\n",
    "\n",
    "def selu_weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        fan_in = m.weight.size(1)\n",
    "        m.weight.data.normal_(0.0, 1.0 / math.sqrt(fan_in))\n",
    "        m.bias.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def relu_weights_init(m): \n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "#         m.bias.data.fill_(0.0)\n",
    "    bn_init(m)\n",
    "\n",
    "def hidden_layer(n_in, n_out, batch_norm, dropout, layer_norm=False, act=None):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if act: layers.append(act)\n",
    "    if batch_norm: layers.append(nn.BatchNorm1d(n_out))\n",
    "    if layer_norm: layers.append(nn.LayerNorm(n_out))\n",
    "    if dropout != 0: layers.append(nn.Dropout(dropout))\n",
    "    return layers\n",
    "\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], \n",
    "                 batch_norm=False, out_act=None, final_bn=False, layer_norm=False, \n",
    "                 final_ln=False):\n",
    "        super().__init__()\n",
    "        sizes = [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        layers_ = []\n",
    "        for i, (n_in, n_out, dr) in enumerate(zip(sizes[:-1], sizes[1:], dropout)):\n",
    "            act_ = act if i < len(layers) else out_act\n",
    "            batch_norm_ = batch_norm if i < len(layers) else final_bn\n",
    "            layer_norm_ = layer_norm if i < len(layers) else final_ln\n",
    "            layers_ += hidden_layer(n_in, n_out, batch_norm_, dr, layer_norm_, act_)      \n",
    "        self.layers = nn.Sequential(*layers_)\n",
    "        if type(act) == nn.SELU: self.layers.apply(selu_weights_init)\n",
    "        else: self.layers.apply(relu_weights_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class ResFullyConnectedNet(nn.Module):\n",
    "    def __init__(self, n_input, n_output=None, layers=[], act=nn.ReLU(True), dropout=[], \n",
    "                 batch_norm=False, out_act=None, final_bn=False, layer_norm=False, \n",
    "                 final_ln=False):\n",
    "        super().__init__()\n",
    "        n_layers, sizes = len(layers), [n_input] + layers\n",
    "        if n_output: \n",
    "            sizes += [n_output]\n",
    "            dropout += [0.0]\n",
    "        assert ((n_layers - 1) % 2) == 0\n",
    "        self.n_blocks, blocks =(n_layers - 1) // 2, [], \n",
    "        self.fc1 = nn.Sequential(*hidden_layer(n_input, layers[0], batch_norm, \n",
    "                                               dropout.pop(0), layer_norm, act))\n",
    "        for i in range(self.n_blocks):\n",
    "            blocks.append(FullyConnectedNet(layers[2*i], layers[2*(i+1)], [layers[(2*i)+1]], act, \n",
    "                                            dropout[2*i:2*(i+1)], batch_norm, act, \n",
    "                                            batch_norm, layer_norm, layer_norm))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.fc_out = nn.Sequential(*hidden_layer(layers[-1], n_output, final_bn, \n",
    "                                                  0.0, final_ln, out_act))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        for i in range(self.n_blocks):\n",
    "            x_ = self.blocks[i](x)\n",
    "            x = x + x_\n",
    "        y = self.fc_out(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM cell as describedi in the set2set paper (https://arxiv.org/pdf/1511.06391.pdf). Doesn't take any inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class HiddenLSTMCell(nn.Module):\n",
    "    \"\"\"Implements the LSTM cell update described in the sec 4.2 of https://arxiv.org/pdf/1511.06391.pdf.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_h_out):\n",
    "        \"\"\"This LSTM cell takes no external 'x' inputs, but has a hidden state appended with the \n",
    "        readout from a content based attention mechanism. Therefore the hidden state is of a dimension\n",
    "        that is two times the number of nodes in the set.\"\"\"\n",
    "        super().__init__()\n",
    "        self.n_h_out, self.n_h = n_h_out, n_h_out * 2 \n",
    "        self.w_h = nn.Parameter(torch.Tensor(self.n_h, n_h_out * 4))\n",
    "        self.b = nn.Parameter(torch.Tensor(n_h_out * 4))\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "                # nn.init.orthogonal_(p.data)\n",
    "            else: \n",
    "                nn.init.zeros_(p.data)\n",
    "                # initialize the forget gate bias to 1\n",
    "                p.data[self.n_h_out:self.n_h_out*2] = torch.ones(self.n_h_out)\n",
    "        \n",
    "    def forward(self, h_prev, c_prev):\n",
    "        \"\"\"Takes previuos hidden and cell states as arguments and performs a \n",
    "        single LSTM step using no external input.\n",
    "        \"\"\"\n",
    "        n_h_ = self.n_h_out # number of output hidden states\n",
    "        # batch the computations into a single matrix multiplication\n",
    "        gates = h_prev @ self.w_h + self.b\n",
    "        i_g, f_g, g, o_g = (\n",
    "            torch.sigmoid(gates[:, :n_h_]), # input\n",
    "            torch.sigmoid(gates[:, n_h_:n_h_*2]), # forget\n",
    "            torch.tanh(gates[:, n_h_*2:n_h_*3]),\n",
    "            torch.sigmoid(gates[:, n_h_*3:]), # output\n",
    "        )\n",
    "        c = f_g * c_prev + i_g * g\n",
    "        h = o_g * torch.tanh(c)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0,
     15,
     34
    ]
   },
   "outputs": [],
   "source": [
    "class IndRNNCell(nn.Module):\n",
    "    def __init__(self, n_in, n_h, act=nn.ReLU(True), layer_norm=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(n_in, n_h)\n",
    "        act_ln_dr = [act]\n",
    "        if layer_norm: act_ln_dr.append(nn.LayerNorm(n_h))\n",
    "        if dropout!=0.0: act_ln_dr.append(nn.Dropout(dropout))\n",
    "        self.act_ln_dr = nn.Sequential(*act_ln_dr)\n",
    "        self.w_h = nn.Parameter(torch.Tensor(n_h))\n",
    "        nn.init.uniform_(self.w_h, a=0, b=1)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        h = self.act_ln_dr(self.lin(x) + self.w_h * h_prev)\n",
    "        return h\n",
    "        \n",
    "class IndRNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_layers, layer_norm=True, dropout=[]):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        if len(dropout)==0: dropout = n_layers * [0.0]\n",
    "        assert len(dropout) == n_layers\n",
    "        layers = []\n",
    "        for i, dr in enumerate(dropout):\n",
    "            n_in = n_x if 1==0 else n_h\n",
    "            layers.append(IndRNNCell(n_in, n_h, layer_norm=layer_norm, dropout=dr))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "            \n",
    "    def forward(self, x, h_prev):\n",
    "        h, hs = x, []\n",
    "        for i in range(self.n_layers):\n",
    "            h = self.layers[i](h, h_prev[i])\n",
    "            hs.append(h)\n",
    "        return h, torch.cat(hs, dim=0)\n",
    "        \n",
    "class ResIndRNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_blocks, layer_norm=True, dropout=[]):\n",
    "        super().__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        if len(dropout)==0: dropout = n_blocks * [0.0]\n",
    "        assert len(dropout) == n_blocks\n",
    "        blocks = []\n",
    "        for i, dr in enumerate(dropout):\n",
    "            n_in = n_x if 1==0 else n_h\n",
    "            blocks.append(IndRNN(n_in, n_h, n_layers=2, layer_norm=layer_norm, dropout=2*[dr]))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "            \n",
    "    def forward(self, x, h_prev):\n",
    "        hs = []\n",
    "        for i in range(self.n_blocks):\n",
    "            h, hs_ = self.blocks[i](x, h_prev[(i*2):((i+1)*2)])\n",
    "            x = h + x\n",
    "            hs.append(hs_)\n",
    "        return x, torch.cat(hs, dim=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set2set module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0,
     6,
     14,
     18,
     22,
     46,
     70
    ]
   },
   "outputs": [],
   "source": [
    "def scatter_sum(src, idx, num):\n",
    "    sz = num, src.size(1)\n",
    "    exp_idx = idx[:,None].repeat(1, sz[1])\n",
    "    out = torch.zeros(sz, dtype=src.dtype, device=src.device)\n",
    "    return out.scatter_add(0, exp_idx, src)\n",
    "\n",
    "def scatter_mean(src, idx, num):\n",
    "    return scatter_sum(src, idx, num) / scatter_sum(torch.ones_like(src), idx, num).clamp(1.0)\n",
    "\n",
    "def softmax(x, idx, num=None):\n",
    "    x = x.exp()\n",
    "    x = x / (scatter_sum(x, idx, num=num)[idx] + 1e-16)\n",
    "    return x\n",
    "\n",
    "class SumReadout(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x, node_idx): return scatter_sum(x, node_idx, num=node_idx.max().item()+1)\n",
    "    \n",
    "class MeanReadout(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, x, node_idx): return scatter_mean(x, node_idx, num=node_idx.max().item()+1)\n",
    "\n",
    "class Set2SetIndRNN(nn.Module):\n",
    "    def __init__(self, n_set_in, proc_steps, n_blocks=3):\n",
    "        super().__init__()\n",
    "        self.proc_steps = proc_steps\n",
    "        self.gru = ResIndRNN(n_set_in, n_set_in, n_blocks, layer_norm=True, dropout=[])\n",
    "        self.init_q = nn.Parameter(torch.zeros(2 * n_blocks, 1, n_set_in))\n",
    "        self.init_r = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        qs = self.init_q.expand(-1, batch_size, -1).contiguous()\n",
    "        r = self.init_r.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q, qs = self.gru(r, qs)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "        return torch.cat([q, r], dim=-1) #q_star\n",
    "     \n",
    "class Set2SetGRU(nn.Module):\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.proc_steps = proc_steps\n",
    "        self.gru = nn.GRUCell(n_set_in, n_set_in)\n",
    "        self.init_q = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "        self.init_r = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        q = self.init_q.expand(batch_size, -1).contiguous()\n",
    "        r = self.init_r.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q = self.gru(r, q)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "        return torch.cat([q, r], dim=-1) #q_star\n",
    "\n",
    "class Set2SetLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from: https://rusty1s.github.io/pytorch_geometric/build/html/_modules/torch_geometric\\\n",
    "        /nn/glob/set2set.html#Set2Set\n",
    "    \"\"\"\n",
    "    def __init__(self, n_set_in, proc_steps):\n",
    "        super().__init__()\n",
    "        self.n_set_in, n_set_out = n_set_in, 2 * n_set_in\n",
    "        self.proc_steps = proc_steps\n",
    "        self.lstm = HiddenLSTMCell(n_set_in)\n",
    "        self.init_q_star = nn.Parameter(torch.zeros(1, n_set_out))\n",
    "        self.init_h = nn.Parameter(torch.zeros(1, n_set_in))\n",
    "\n",
    "    def forward(self, x, node_idx):\n",
    "        \"\"\"\n",
    "        x - input tensor of shape (batch_size * n_nodes, in_channels).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        \"\"\"\n",
    "        batch_size = node_idx.max().item() + 1\n",
    "        h = self.init_h.expand(batch_size, -1).contiguous()\n",
    "        q_star = self.init_q_star.expand(batch_size, -1).contiguous()\n",
    "        for i in range(self.proc_steps):\n",
    "            q, h = self.lstm(q_star, h)\n",
    "            e = (x * q[node_idx]).sum(dim=-1, keepdim=True)\n",
    "            a = softmax(e, node_idx, num=batch_size)\n",
    "            r = scatter_sum(a * x, node_idx, num=batch_size) # sum 'a*x' over nodes \n",
    "            q_star = torch.cat([q, r], dim=-1)\n",
    "        return q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge network message function as described in the MPNN paper (https://arxiv.org/pdf/1704.01212.pdf). Adds in seperate edge network to allow messages to flow along scalar coupling edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class EdgeNetwork(nn.Module):\n",
    "    def __init__(self, n_h, n_e, n_sc_e, net_args={}):\n",
    "        super().__init__()\n",
    "        self.n_h = n_h\n",
    "        self.adj_net = FullyConnectedNet(n_e, n_h**2, **net_args)\n",
    "        self.sc_adj_net = FullyConnectedNet(n_sc_e, n_h**2, **net_args)\n",
    "        self.b = nn.Parameter(torch.Tensor(n_h)) # bias for the message function\n",
    "        self.weight_inits()\n",
    "        \n",
    "    def weight_inits(self):\n",
    "        nn.init.zeros_(self.b)\n",
    "    \n",
    "    def forward(self, h, e, sc_e, pairs_idx, sc_pairs_idx, t=0):\n",
    "        \"\"\"\n",
    "        Compute message vector m_t given the previuos hidden state\n",
    "        h_t-1 and edge features e.\n",
    "        - h: tensor of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_e: tensor of scalar coupling edge features of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - t: update iteration. \n",
    "        \"\"\"\n",
    "        # compute 'A(e)'\n",
    "        if t==0: \n",
    "            self.a_mat = self.get_a_mat(self.adj_net(e))\n",
    "            self.a_sc_mat = self.get_a_mat(self.sc_adj_net(sc_e))\n",
    "            \n",
    "        # compute 'm_{i} = sum_{j in N(i)}(A_{ij}h_{j})' for all nodes 'i'\n",
    "        m = self.add_message(torch.zeros_like(h), self.a_mat, h, pairs_idx)\n",
    "        m = self.add_message(m, self.a_sc_mat, h, sc_pairs_idx)\n",
    "        \n",
    "        # add message bias\n",
    "        m = m + self.b\n",
    "        return m # apply optional batch norm\n",
    "    \n",
    "    def get_a_mat(self, a_vect):\n",
    "        return a_vect.view(-1, self.n_h, self.n_h) / (self.n_h ** .5)\n",
    "    \n",
    "    def add_message(self, m, a, h, pairs_idx):\n",
    "        # transform 'pairs_idx' and 'a' to make messages go both in to and out of all nodes\n",
    "        in_out_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n",
    "        a_ = torch.cat((a, a)) \n",
    "        \n",
    "        # select the 'h_{j}' feeding into the 'm_{i}'\n",
    "        h_in = h.index_select(0, in_out_idx[:,1])\n",
    "        \n",
    "        # do the matrix multiplication 'A_{ij}h_{j}'\n",
    "        ah = (h_in.unsqueeze(1) @ a_).squeeze(1)\n",
    "        \n",
    "        # Sum up all 'A_{ij}h_{j}' per node 'i'\n",
    "        return m.scatter_add(0, in_out_idx[:,0,None].repeat(1, self.n_h), ah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU update function as described in the MPNN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GRUUpdate(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(n_h, n_h)\n",
    "        \n",
    "    def forward(self, m, h_prev):\n",
    "        \"\"\"\n",
    "        Update hidden state h.\n",
    "        - h_prev is vector of hidden states of shape (batch_size * n_nodes, n_h)\n",
    "        - m is vector of messages of shape (batch_size * n_nodes, n_h)\n",
    "        \"\"\"\n",
    "        return self.gru(m, h_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom readout network following th set2set processing stage. Allows some final specialization/fine-tuning for each scalar coupling type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0,
     5,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def create_contrib_head(n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "    layers = hidden_layer(n_in, n_h, False, dropout, layer_norm, act)\n",
    "    layers += hidden_layer(n_h, 1, False, 0.0) # output layer\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ContribsHead(nn.Module):\n",
    "    N_CONTRIBS = 5\n",
    "    CONTIB_SCALES = [1, 250, 45, 35, 500]\n",
    "    \n",
    "    def __init__(self, n_in, n_h, act, dropout=0.0, layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            create_contrib_head(n_in, n_h, act, dropout, layer_norm) \n",
    "            for _ in range(self.N_CONTRIBS)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ys = torch.cat([b(x) / s for b, s in zip(self.blocks, self.CONTIB_SCALES)], dim=-1)\n",
    "        return torch.cat([ys[:,:-1], ys.sum(dim=-1, keepdim=True)], dim=-1)\n",
    "\n",
    "class MyCustomHead(nn.Module):\n",
    "    N_OUTPUTS = 5\n",
    "    \n",
    "    def __init__(self, n_input, n_h_contribs, pre_layers=[], post_layers=[], \n",
    "                 act=nn.ReLU(True), dropout=[], norm=False):\n",
    "        super().__init__()\n",
    "        n_pre_layers = len(pre_layers)\n",
    "        self.preproc = FullyConnectedNet(n_input, None, pre_layers, act, \n",
    "                                         dropout[:n_pre_layers], batch_norm=norm)\n",
    "        self.types_proc = nn.ModuleList([\n",
    "            FullyConnectedNet(pre_layers[-1], None, post_layers, act, dropout[n_pre_layers:-1], layer_norm=norm)\n",
    "            for _ in range(N_TYPES)\n",
    "        ])\n",
    "        self.contribs_head = ContribsHead(post_layers[-1], n_h_contribs, act, dropout[-1], layer_norm=norm)\n",
    "        \n",
    "    def forward(self, x, sc_types):\n",
    "        x_ = self.preproc(x)\n",
    "        x_types = torch.zeros(x.size(0), x.size(1), device=x.device)\n",
    "        for i in range(N_TYPES):\n",
    "            if torch.any(sc_types == i): \n",
    "                x_types[sc_types == i] = self.types_proc[i](x_[sc_types == i])\n",
    "        x = x + x_types \n",
    "        y = self.contribs_head(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Set2SetOutput(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_sc_m, proc_steps=10, readout_type='Set2SetGRU', net_args={}):\n",
    "        super().__init__()\n",
    "        self.R_proj = nn.Linear(n_h + n_x, n_h)\n",
    "        if readout_type=='Sum': self.R_proc = SumReadout()\n",
    "        if readout_type=='Mean': self.R_proc = MeanReadout()\n",
    "        if readout_type=='Set2SetGRU': self.R_proc = Set2SetGRU(n_h, proc_steps)\n",
    "        if readout_type=='Set2SetLSTM': self.R_proc = Set2SetLSTM(n_h, proc_steps)\n",
    "        if readout_type=='Set2SetIndRNN': self.R_proc = Set2SetIndRNN(n_h, proc_steps)\n",
    "        n_readout = n_h if readout_type[:7]!='Set2Set' else 2 * n_h \n",
    "        self.R_write = MyCustomHead(n_readout + (2 * n_h) + n_sc_m, **net_args)\n",
    "    \n",
    "    def forward(self, h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types):\n",
    "        \"\"\"\n",
    "        Make prediction.\n",
    "        - h is vector of hidden states of shape (batch_size * n_nodes, n_h).\n",
    "        - x is vector of input features of shape (batch_size * n_nodes, n_x).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        m = self.R_proj(torch.cat([h, x], dim=1))\n",
    "        q = self.R_proc(m, node_idx)\n",
    "        \n",
    "        # introduce skip connection to final node states of scalar coupling atoms\n",
    "        inp = torch.cat([\n",
    "            q.index_select(0, sc_idx),\n",
    "            h.index_select(0, sc_pairs_idx[:,0]),\n",
    "            h.index_select(0, sc_pairs_idx[:,1]),\n",
    "            sc_m\n",
    "        ], dim=-1)\n",
    "        y = self.R_write(inp, sc_types)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchFilterNet(nn.Module):\n",
    "    def __init__(self, n_in, n_h):\n",
    "        super().__init__()\n",
    "        self.n_in, self.n_h = n_in, n_h\n",
    "        self.rbf_proc = FullyConnectedNet(n_in, None, 2*[n_h], act=Softplus2(), dropout=2*[0.0])\n",
    "\n",
    "    def forward(self, rbf_dists):\n",
    "        return self.rbf_proc(rbf_dists)\n",
    "\n",
    "class IBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_h):\n",
    "        super().__init__()\n",
    "        self.n_h = n_h\n",
    "        self.filter_net = SchFilterNet(n_in, n_h)\n",
    "        self.sc_filter_net = SchFilterNet(n_in, n_h)\n",
    "        self.pre_atom_wise = nn.Linear(n_h, n_h)\n",
    "        self.post_atom_wise = FullyConnectedNet(n_h, None, 2*[n_h], act=Softplus2(), dropout=2*[0.0])\n",
    "\n",
    "    def forward(self, h, rbf_dists, sc_rbf_dists, node_idx, pairs_idx, sc_pairs_idx):\n",
    "        w = self.filter_net(rbf_dists)\n",
    "        sc_w = self.filter_net(sc_rbf_dists)\n",
    "        x = self.pre_atom_wise(h)\n",
    "        v = torch.zeros_like(x)\n",
    "        v = self.cf_conv(v, x, w, pairs_idx)\n",
    "        v = self.cf_conv(v, x, sc_w, sc_pairs_idx)\n",
    "        x = self.post_atom_wise(x)\n",
    "        h = h + x\n",
    "        return h\n",
    "    \n",
    "    def cf_conv(self, v, x, w, pairs_idx):\n",
    "        # transform 'pairs_idx' and 'w' to make messages go both in to and out of all nodes\n",
    "        in_out_idx = torch.cat((pairs_idx, pairs_idx[:, [1, 0]]))\n",
    "        a_ = torch.cat((a, a)) \n",
    "        # select the 'x_{j}' feeding into the 'v_{i}'\n",
    "        x_in = x.index_select(0, in_out_idx[:,1])\n",
    "        # Sum up all 'A_{ij}h_{j}' per node 'i'\n",
    "        return v.scatter_add(0, in_out_idx[:,0,None].repeat(1, self.n_h), w * x_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchNet(nn.Module):\n",
    "    def __init__(self, n_h=64, n_i_blocks=6, n_x=None, readout_type='Sum', \n",
    "                 preproc_type='Embdedding', preproc_net_args={}, enn_args={}, R_net_args={}):\n",
    "        super().__init__()\n",
    "        assert int(cutoff / gap) == (cutoff / gap)\n",
    "        n_rbfs = int(cutoff / gap)\n",
    "        self.rbf = RBFLayer(cutoff=20, gap=0.1)\n",
    "        if preproc_type=='Embdedding': self.preproc_net = nn.Embedding(5, n_h)\n",
    "        else self.preproc_net = FullyConnectedNet(n_x, n_h, **preproc_net_args)\n",
    "        self.i_blocks = nn.ModuleList(\n",
    "            [IBlock(self.rbf.fan_out, n_h) for i in range(n_i_blocks)]\n",
    "        )\n",
    "        self.readout = ReadoutNet(n_x, n_h, n_sc_m, readout_type, R_net_args)\n",
    "        self.n_i_blocks = n_i_blocks\n",
    "        \n",
    "    def forward(self, x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: tensor of node features of shape (batch_size * n_nodes, n_x).\n",
    "        - e: tensor of edge features of shape (batch_size * n_edges, n_e).\n",
    "        - sc_e: tensor of scalar coupling edge features of shape \n",
    "            (batch_size * n_sc, n_sc_e).\n",
    "        - sc_m: tensor of scalar coupling molecule level features of shape \n",
    "            (batch_size * n_sc, n_sc_m).\n",
    "        - node_idx: tensor of shape (batch_size * n_nodes) mapping each\n",
    "            node to its corresponding index in the batch.\n",
    "        - pairs_idx: tensor of shape (batch_size * n_edges, 2) mapping atom \n",
    "            indexes (first column) to the other atom indexes they form a \n",
    "            bond with (second column). Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_idx: tensor of shape (batch_size * n_sc) mapping each\n",
    "            scalar coupling constant to its corresponding index in the batch.\n",
    "        - sc_pairs_idx: tensor of shape (batch_size * n_sc, 2) containing atom \n",
    "            indices of the atoms for which the scalar coupling constant\n",
    "            need to be predicted. Atom indices are unique to the entire\n",
    "            batch.\n",
    "        - sc_types: tensor of shape (batch_size * n_sc) containing the scalar \n",
    "            coupling type of each observation. \n",
    "        \"\"\"\n",
    "        rbf_dists = self.rbf(e)\n",
    "        sc_rbf_dists = self.rbf(sc_e)\n",
    "        h = self.preproc_net(x)\n",
    "        for t in range(self.n_i_blocks):\n",
    "            h = self.i_blocks[t](h, rbf_dists, sc_rbf_dists, node_idx, pairs_idx, sc_pairs_idx)\n",
    "        y = self.readout(h, x, sc_m, node_idx, sc_idx, sc_pairs_idx, sc_types)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    # python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # pytorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # numpy RNG\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ids = train_df['molecule_id'].unique()\n",
    "n_obs = len(mol_ids)\n",
    "split = int(n_obs*0.75)\n",
    "set_seed(100)\n",
    "mol_ids_ = np.random.choice(mol_ids, size=n_obs, replace=False)\n",
    "train_mol_ids, val_mol_ids = pd.Series(mol_ids_[:split]), pd.Series(mol_ids_[split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df, features, train_mol_ids):\n",
    "    idx = df['molecule_id'].isin(train_mol_ids)\n",
    "    return df.loc[idx, features].mean(), df.loc[idx, features].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(train_df[SC_FEATS_TO_SCALE].mean().abs()>0.1) or any((train_df[SC_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    sc_feat_means, sc_feat_stds = scale_features(train_df, SC_FEATS_TO_SCALE, train_mol_ids)\n",
    "    train_df[SC_FEATS_TO_SCALE] = (train_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "    test_df[SC_FEATS_TO_SCALE] = (test_df[SC_FEATS_TO_SCALE] - sc_feat_means) / sc_feat_stds\n",
    "if any(atom_df[ATOM_FEATS_TO_SCALE].mean().abs()>0.1) or any((atom_df[ATOM_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    atom_feat_means, atom_feat_stds = scale_features(atom_df, ATOM_FEATS_TO_SCALE, train_mol_ids)\n",
    "    atom_df[ATOM_FEATS_TO_SCALE] = (atom_df[ATOM_FEATS_TO_SCALE] - atom_feat_means) / atom_feat_stds\n",
    "if any(edge_df[EDGE_FEATS_TO_SCALE].mean().abs()>0.1) or any((edge_df[EDGE_FEATS_TO_SCALE].std()-1.0).abs()>0.1):\n",
    "    edge_feat_means, edge_feat_stds = scale_features(edge_df, EDGE_FEATS_TO_SCALE, train_mol_ids)\n",
    "    edge_df[EDGE_FEATS_TO_SCALE] = (edge_df[EDGE_FEATS_TO_SCALE] - edge_feat_means) / edge_feat_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mol_sc = train_df.groupby('molecule_id')\n",
    "test_gb_mol_sc = test_df.groupby('molecule_id')\n",
    "gb_mol_atom = atom_df.groupby('molecule_id')\n",
    "gb_mol_edge = edge_df.groupby('molecule_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pytorch dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge):\n",
    "        self.n = len(mol_ids)\n",
    "        self.mol_ids = mol_ids\n",
    "        self.gb_mol_sc = gb_mol_sc\n",
    "        self.gb_mol_atom = gb_mol_atom\n",
    "        self.gb_mol_edge = gb_mol_edge\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gb_mol_sc.get_group(self.mol_ids[idx]),\n",
    "                self.gb_mol_atom.get_group(self.mol_ids[idx]), \n",
    "                self.gb_mol_edge.get_group(self.mol_ids[idx]))\n",
    "\n",
    "def np_lst_to_torch(arr_lst, dtype=torch.float):\n",
    "    return torch.from_numpy(np.ascontiguousarray(np.concatenate(arr_lst))).type(dtype)\n",
    "\n",
    "def collate_fn(batch, test=False):\n",
    "    batch_size, n_atom_sum = len(batch), 0\n",
    "    x, e, sc_e, sc_m = [], [], [], []\n",
    "    sc_types, sc_vals = [], []\n",
    "    node_idx, pairs_idx, sc_pairs_idx, sc_idx = [], [], [], []\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        sc_df, atom_df, edge_df = batch[b]\n",
    "        n_atoms, n_sc = len(atom_df), len(sc_df)\n",
    "        \n",
    "        x.append(atom_df[ATOM_FEATS].values)\n",
    "        e.append(edge_df[EDGE_FEATS].values)\n",
    "        sc_e.append(sc_df[SC_EDGE_FEATS].values)\n",
    "        sc_m.append(sc_df[SC_MOL_FEATS].values)\n",
    "        sc_types.append(sc_df['type'].values)\n",
    "        if not test: sc_vals.append(sc_df[CONTRIB_COLS+[TARGET_COL]].values)\n",
    "        \n",
    "        node_idx.append(np.repeat(b, n_atoms))\n",
    "        sc_idx.append(np.repeat(b, n_sc))\n",
    "        pairs_idx.append(edge_df[['idx_0', 'idx_1']].values + n_atom_sum)\n",
    "        sc_pairs_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values + n_atom_sum)\n",
    "        \n",
    "        n_atom_sum += n_atoms\n",
    "        \n",
    "    \n",
    "    x, e = np_lst_to_torch(x), np_lst_to_torch(e), \n",
    "    sc_e, sc_m = np_lst_to_torch(sc_e), np_lst_to_torch(sc_m)\n",
    "    if not test: sc_vals = np_lst_to_torch(sc_vals)\n",
    "    else: sc_vals = torch.tensor([0] * len(sc_types))\n",
    "    sc_types = np_lst_to_torch(sc_types, torch.long)\n",
    "    node_idx = np_lst_to_torch(node_idx, torch.long)\n",
    "    sc_idx = np_lst_to_torch(sc_idx, torch.long)\n",
    "    pairs_idx = np_lst_to_torch(pairs_idx, torch.long)\n",
    "    sc_pairs_idx = np_lst_to_torch(sc_pairs_idx, torch.long)\n",
    "    \n",
    "    return (x, e, sc_e, sc_m, node_idx, pairs_idx, sc_idx, sc_pairs_idx, sc_types), sc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MoleculeDataset(train_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge)\n",
    "val_ds   = MoleculeDataset(val_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_edge)\n",
    "test_ds  = MoleculeDataset(test_mol_ids, test_gb_mol_sc, gb_mol_atom, gb_mol_edge)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=8)\n",
    "val_dl   = DataLoader(val_ds, batch_size, num_workers=8)\n",
    "test_dl  = DeviceDataLoader.create(test_ds, batch_size, num_workers=8, collate_fn=partial(collate_fn, test=True))\n",
    "db = DataBunch(train_dl, val_dl, collate_fn=collate_fn)\n",
    "db.test_dl = test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in batch[0]: print(el.size())\n",
    "print(batch[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dict = dict(x=batch[0][0], \n",
    "              e=batch[0][1], \n",
    "              sc_e=batch[0][2], \n",
    "              sc_m=batch[0][3], \n",
    "              node_idx=batch[0][4], \n",
    "              pairs_idx=batch[0][5], \n",
    "              sc_idx=batch[0][6], \n",
    "              sc_pairs_idx=batch[0][7], \n",
    "              sc_types=batch[0][8], \n",
    "              y=batch[1])\n",
    "for k,v in b_dict.items(): print(f'{k}:\\n {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the metric used for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types, epoch):\n",
    "    proc = lambda x: x.cpu().numpy().ravel() \n",
    "    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n",
    "    y_true = SC_MEAN + y_true * SC_STD\n",
    "    y_pred = SC_MEAN + y_pred * SC_STD\n",
    "    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n",
    "    gmlmae = np.log(maes).mean()\n",
    "    print(f'Epoch: {epoch} - Group Mean Log Mae: {gmlmae}')\n",
    "    return gmlmae\n",
    "\n",
    "class GroupMeanLogMAE(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['group_mean_log_mae'])\n",
    "    def on_epoch_begin(self, **kwargs): self.input, self.output, self.target = [], [], []\n",
    "    \n",
    "    def on_batch_end(self, last_target, last_output, last_input, train, **kwargs):\n",
    "        if not train:\n",
    "            self.input.append(last_input[-1])\n",
    "            self.output.append(last_output[:,-1])\n",
    "            self.target.append(last_target[:,-1])\n",
    "                \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n",
    "        if (len(self.input) > 0) and (len(self.output) > 0):\n",
    "            inputs = torch.cat(self.input)\n",
    "            preds = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            metric = group_mean_log_mae(preds, target, inputs, epoch)\n",
    "            return add_metrics(last_metrics, [metric])\n",
    "\n",
    "def contribs_rmse_loss(preds, targs):\n",
    "    \"\"\"\n",
    "    Returns the sum of RMSEs for each sc contribution and total sc value.\n",
    "    \n",
    "    Args:\n",
    "        - preds: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            predictions. Last column is the total scalar coupling value.\n",
    "        - targs: tensor of shape (batch_size * n_sc, 5) containing \n",
    "            true values. Last column is the total scalar coupling value.\n",
    "    \"\"\"\n",
    "    return torch.mean((preds - targs) ** 2, dim=0).sqrt().sum()\n",
    "\n",
    "def rmse(preds, targs):\n",
    "    return torch.sqrt(F.mse_loss(preds[:,-1], targs[:,-1]))\n",
    "\n",
    "def mae(preds, targs):\n",
    "    return torch.abs(preds[:,-1] - targs[:,-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd, norm, act = 0, True, nn.ReLU(True)\n",
    "update_steps, proc_steps, readout_type = 5, 6, 'Set2SetGRU'\n",
    "n_x, n_h, n_e, n_sc_e, n_sc_m = N_ATOM_FEATURES, 300, N_EDGE_FEATURES, N_SC_EDGE_FEATURES, N_SC_MOL_FEATURES\n",
    "preproc_net_args = dict(layers=[], act=act, dropout=[], out_act=nn.Tanh())\n",
    "enn_args = dict(layers=3*[n_h], act=act, dropout=3*[0.0], batch_norm=norm)\n",
    "R_net_args = dict(pre_layers=[1500], post_layers=[n_sc_m+4*n_h], n_h_contribs=200, act=act, dropout=[0.0, 0.0, 0.0], \n",
    "                  norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "model = MPNN(n_x, n_h, n_e, n_sc_e, n_sc_m, update_steps, proc_steps, readout_type, preproc_net_args, enn_args, R_net_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(model(*batch[0]))\n",
    "print(model(*batch[0]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(LearnerCallback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, learn:Learner, clip:float = 0., start_it:int = 100):\n",
    "        super().__init__(learn)\n",
    "        self.clip, self.start_it = clip, start_it\n",
    "\n",
    "    def on_backward_end(self, iteration, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip and (iteration > self.start_it): nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(db, model, metrics=[rmse, mae], \n",
    "                callback_fns=[partial(GradientClipping, clip=10), GroupMeanLogMAE], \n",
    "                wd=wd, loss_func=contribs_rmse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find(start_lr=1e-6, end_lr=1.0, num_it=100, stop_div=True)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, max_lr=1e-3, callbacks=[SaveModelCallback(learn, every='improvement', mode='min',\n",
    "                                                                  monitor='group_mean_log_mae',  name='mpnn1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses(skip_start=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_contrib_preds = learn.get_preds(DatasetType.Valid)\n",
    "test_contrib_preds = learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = val_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN\n",
    "test_preds = test_contrib_preds[0][:,-1].detach().numpy() * SC_STD + SC_MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_submit(predictions):\n",
    "    submit = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "    print(len(submit), len(predictions))   \n",
    "    submit['scalar_coupling_constant'] = predictions\n",
    "    submit.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-submission.csv', index=False)\n",
    "\n",
    "def store_oof(predictions, val_ids):\n",
    "    oof = pd.DataFrame(predictions, columns=['scalar_coupling_constants'])\n",
    "    print(oof.head())\n",
    "    oof.to_csv(f'mpnn-v{VERSION}-idx{FOLD_ID}-oof.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_submit(test_preds)\n",
    "# store_oof(val_preds, val_mol_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
